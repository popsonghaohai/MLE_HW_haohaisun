{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install transformers,torch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6f27ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/python_transfomer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b62fe03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67027143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.27s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is Scott Lai? He is a Chinese American actor, director and singer. He is best known for his role as \"Terry\" in the Disney Channel Original Movie \"The Last Run\" (2014). He has also appeared in other Disney Channel Original Movies, including \"Journey to the West: The Last Dragon\" (2013), \"A Boy Named Charlie Brown\" (2013) and \"The Princess and the Frog\" (2012).\n",
      "Scott Lai was born on July 19, 1996, in New York City, United States. He is of Chinese descent and grew up in California. He began his acting career at a young age and has appeared in various stage productions, commercials, and music videos.\n",
      "In addition to acting, Scott Lai is also a singer. He released his first single, \"Love Story,\" in 2012, which gained popularity among his fans. He has since released several other singles and has been featured in various music videos.\n",
      "As an actor, Scott Lai has been praised for his performances in Disney Channel Original Movies. He has received critical acclaim for his portrayal of Terry in \"The Last Run,\" and he has been recognized for his talent and potential in the entertainment\n"
     ]
    }
   ],
   "source": [
    "ask_llm = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "  device=device,\n",
    "  torch_dtype=dtype\n",
    ")\n",
    "\n",
    "print(ask_llm(\"Who is Scott Lai?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145dc2e4",
   "metadata": {},
   "source": [
    "As you can see here, the model has no idea who I am from above response.\n",
    "\n",
    "Let's cook it!\n",
    "\n",
    "First, let's teach the model who I am. Here you can use your personal data to generate the exact format you will use for fine-turning base on your own data. You can use ChatGPT for this, just ask it to transfer your resume into the trainable json format with \"prompt\" and \"completion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "869c27e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'completion'],\n",
       "        num_rows: 122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data \n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset('json', data_files = \"scott_lai_resume_train.json\")\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is Scott Lai’s profession?',\n",
       " 'completion': 'AI Engineer and Data Scientist.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"train\"][0]\n",
    "raw_data[\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13cf6e",
   "metadata": {},
   "source": [
    "As you can see, here we return with the long text, but for fine-tuning we need the data to be small and precise chunks, more like here we apply the tokenization to take the text and split it into smaller chunks. Each chunk is called a token and it the smallest unit of meaning that LLMs work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da03887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\"\n",
    ")\n",
    "def preprocess(sample):\n",
    "    sample = sample['prompt']+ '\\n' + sample['completion']\n",
    "    print(sample)\n",
    "    tokenized = tokenizer(\n",
    "        sample,\n",
    "        max_length = 128,\n",
    "        truncation = True,\n",
    "        padding = \"max_length\"    \n",
    "    )\n",
    "\n",
    "    tokenized['labels'] = tokenized['input_ids'].copy()\n",
    "    return tokenized\n",
    "data = raw_data.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6aae75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'completion', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 122\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ab97f",
   "metadata": {},
   "source": [
    "## LoRA\n",
    "\n",
    "now, let's move into the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90228ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7c30e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    device_map = device,\n",
    "    torch_dtype = torch.float16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig (\n",
    "    \n",
    "    task_type = TaskType.CAUSAL_LM, \n",
    "    target_modules=['q_proj', \"k_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33200b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs = 10, # we will go throught the dataset from start to finish 10 times\n",
    "    learning_rate=0.001, \n",
    "    logging_steps = 25, # we want to see the result in every 25 steps it runs \n",
    "    fp16 = False # float point set to 16 to speed it up, set to \"True\" if you are on GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    args = train_args,\n",
    "    model = model, \n",
    "    train_dataset=data[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b17f16de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.237200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.127400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.5464043714106083, metrics={'train_runtime': 53.5074, 'train_samples_per_second': 22.801, 'train_steps_per_second': 2.99, 'total_flos': 2602200748523520.0, 'train_loss': 0.5464043714106083, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed1aafcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-qwen/tokenizer_config.json',\n",
       " './my-qwen/special_tokens_map.json',\n",
       " './my-qwen/chat_template.jinja',\n",
       " './my-qwen/vocab.json',\n",
       " './my-qwen/merges.txt',\n",
       " './my-qwen/added_tokens.json',\n",
       " './my-qwen/tokenizer.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "trainer.save_model(\"./my-qwen\")\n",
    "tokenizer.save_pretrained(\"./my-qwen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7449af4",
   "metadata": {},
   "source": [
    "Now let's test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0da061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/python_transfomer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.73s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What type of workflows is Scott Lai experienced in building? End-to-end ML pipelines, data engineering, and automation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else (\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float16 if device == \"mps\" else torch.float32\n",
    "\n",
    "ask_llm = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=\"./my-qwen\",\n",
    "  tokenizer='./my-qwen',\n",
    "  device=device,\n",
    "  torch_dtype=dtype\n",
    ")\n",
    "\n",
    "print(ask_llm(\"What type of workflows is Scott Lai experienced in building?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71e8b4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.11s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many years of experience does Scott Lai have in generative AI and LLM solutions? Over 5 years.\n"
     ]
    }
   ],
   "source": [
    "ask_llm = pipeline(\n",
    "  task=\"text-generation\",\n",
    "  model=\"./my-qwen\",\n",
    "  tokenizer='./my-qwen',\n",
    "  device=device,\n",
    "  torch_dtype=dtype\n",
    ")\n",
    "\n",
    "print(ask_llm(\"How many years of experience does Scott Lai have in generative AI and LLM solutions?\")[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef1abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
