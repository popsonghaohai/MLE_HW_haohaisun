[
  {
    "id": "arxiv_0",
    "source": "arxiv_metadata",
    "title": "Variational Masked Diffusion Models",
    "text": "Variational Masked Diffusion Models Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD.",
    "metadata": {
      "authors": [
        "Yichi Zhang",
        "Alex Schwing",
        "Zhizhen Zhao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23606v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_1",
    "source": "arxiv_metadata",
    "title": "Think Twice: Branch-and-Rethink Reasoning Reward Model",
    "text": "Think Twice: Branch-and-Rethink Reasoning Reward Model Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon.",
    "metadata": {
      "authors": [
        "Yizhu Jiao",
        "Jiaqi Zeng",
        "Julien Veron Vialard",
        "Oleksii Kuchaiev",
        "Jiawei Han",
        "Olivier Delalleau"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23596v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_2",
    "source": "arxiv_metadata",
    "title": "Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models",
    "text": "Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional machine learning models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Na\\\"ive Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional machine learning models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets.",
    "metadata": {
      "authors": [
        "Luis Ramos",
        "Hiram Calvo",
        "Olga Kolesnikova"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23585v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_3",
    "source": "arxiv_metadata",
    "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
    "text": "ReCode: Unify Plan and Action for Universal Granularity Control Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.",
    "metadata": {
      "authors": [
        "Zhaoyang Yu",
        "Jiayi Zhang",
        "Huixue Su",
        "Yufan Zhao",
        "Yifan Wu",
        "Mingyi Deng",
        "Jinyu Xiang",
        "Yizhang Lin",
        "Lingxiao Tang",
        "Yingchao Li",
        "Yuyu Luo",
        "Bang Liu",
        "Chenglin Wu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23564v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_4",
    "source": "arxiv_metadata",
    "title": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models",
    "text": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models Large Audio Language Models (LALMs), which couple acoustic perception with large language models (LLMs) to extract and understand diverse information from audio, have attracted intense interest from both academic and industrial communities. However, existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance. Yet, no existing benchmarks offer a systematic and comprehensive evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark evaluating instruction sensitivity for LALMs along three axes: instruction description, output format, and task composition. We assess recent open-source and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy under controlled instruction variations. Experimental results reveal that even state-of-the-art LALMs suffer significant instruction sensitivity, leading to degraded performance on fundamental audio understanding tasks. To mitigate this issue, we fine-tune Qwen2-Audio on a specifically constructed complex instruction-variant dataset, achieving a marked improvement in instruction-following performance. However, this also induces nontrivial catastrophic forgetting: the model loses some previously mastered task capabilities when exposed to new instruction styles. Our benchmark provides a standardized basis for assessing and improving instruction sensitivity in LALMs, underscoring the need for instruction-robust audio understanding in real-world pipelines.",
    "metadata": {
      "authors": [
        "Bohan Li",
        "Wenbin Huang",
        "Yuhang Qiu",
        "Yiwei Guo",
        "Hankun Wang",
        "Zhihan Li",
        "Jing Peng",
        "Ziyang Ma",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23558v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_5",
    "source": "arxiv_metadata",
    "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
    "text": "A U-Net and Transformer Pipeline for Multilingual Image Translation This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.",
    "metadata": {
      "authors": [
        "Siddharth Sahay",
        "Radhika Agarwal"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23554v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_6",
    "source": "arxiv_metadata",
    "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
    "text": "LimRank: Less is More for Reasoning-Intensive Information Reranking Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
    "metadata": {
      "authors": [
        "Tingyu Song",
        "Yilun Zhao",
        "Siyue Zhang",
        "Chen Zhao",
        "Arman Cohan"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23544v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_7",
    "source": "arxiv_metadata",
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
    "text": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
    "metadata": {
      "authors": [
        "Qiushi Sun",
        "Jingyang Gong",
        "Yang Liu",
        "Qiaosheng Chen",
        "Lei Li",
        "Kai Chen",
        "Qipeng Guo",
        "Ben Kao",
        "Fei Yuan"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23538v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_8",
    "source": "arxiv_metadata",
    "title": "IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering",
    "text": "IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering Intent identification serves as the foundation for generating appropriate responses in personalized question answering (PQA). However, existing benchmarks evaluate only response quality or retrieval performance without directly measuring intent identification capabilities. This gap is critical because without understanding which intents users prioritize, systems cannot generate responses satisfying individual information needs. To address this, we introduce the concept of core intents: intents users prioritize when selecting answers to satisfy their information needs. To evaluate these core intents, we propose IPQA, a benchmark for core Intent identification in Personalized Question Answering. Since users do not explicitly state their prioritized intents, we derive core intents from observable behavior patterns in answer selection, grounded in satisficing theory where users choose answers meeting their acceptance thresholds. We construct a dataset with various domains through systematic filtering, LLM-based annotation, and rigorous quality control combining automated verification with human validation. Experimental evaluations across state-of-the-art language models reveal that current systems struggle with core intent identification in personalized contexts. Models fail to identify core intents from user histories, with performance degrading as question complexity increases. The code and dataset will be made publicly available to facilitate future research in this direction.",
    "metadata": {
      "authors": [
        "Jieyong Kim",
        "Maryam Amirizaniani",
        "Soojin Yoon",
        "Dongha Lee"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23536v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_9",
    "source": "arxiv_metadata",
    "title": "M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset",
    "text": "M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset Existing real-world datasets for multimodal automated fact-checking have multiple limitations: they contain few instances, focus on only one or two languages and tasks, suffer from evidence leakage, or depend on external sets of news articles for sourcing true claims. To address these shortcomings, we introduce M4FC, a new real-world dataset comprising 4,982 images paired with 6,980 claims. The images, verified by professional fact-checkers from 22 organizations, represent diverse cultural and geographic contexts. Each claim is available in one or two out of ten languages. M4FC spans six multimodal fact-checking tasks: visual claim extraction, claimant intent prediction, fake detection, image contextualization, location verification, and verdict prediction. We provide baseline results for all tasks and analyze how combining intermediate tasks influence downstream verdict prediction performance. We make our dataset and code available.",
    "metadata": {
      "authors": [
        "Jiahui Geng",
        "Jonathan Tonglet",
        "Iryna Gurevych"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23508v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_10",
    "source": "arxiv_metadata",
    "title": "MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring",
    "text": "MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring Effective math tutoring requires not only solving problems but also diagnosing students' difficulties and guiding them step by step. While multimodal large language models (MLLMs) show promise, existing benchmarks largely overlook these tutoring skills. We introduce MMTutorBench, the first benchmark for AI math tutoring, consisting of 685 problems built around pedagogically significant key-steps. Each problem is paired with problem-specific rubrics that enable fine-grained evaluation across six dimensions, and structured into three tasks-Insight Discovery, Operation Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find clear performance gaps between proprietary and open-source systems, substantial room compared to human tutors, and consistent trends across input variants: OCR pipelines degrade tutoring quality, few-shot prompting yields limited gains, and our rubric-based LLM-as-a-Judge proves highly reliable. These results highlight both the difficulty and diagnostic value of MMTutorBench for advancing AI tutoring.",
    "metadata": {
      "authors": [
        "Tengchao Yang",
        "Sichen Guo",
        "Mengzhao Jia",
        "Jiaming Su",
        "Yuanyang Liu",
        "Zhihan Zhang",
        "Meng Jiang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23477v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_11",
    "source": "arxiv_metadata",
    "title": "Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts",
    "text": "Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data.",
    "metadata": {
      "authors": [
        "Nikesh Gyawali",
        "Doina Caragea",
        "Alex Vasenkov",
        "Cornelia Caragea"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23464v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_12",
    "source": "arxiv_metadata",
    "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents",
    "text": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents Confidence in LLMs is a useful indicator of model uncertainty and answer reliability. Existing work mainly focused on single-turn scenarios, while research on confidence in complex multi-turn interactions is limited. In this paper, we investigate whether LLM-based search agents have the ability to communicate their own confidence through verbalized confidence scores after long sequences of actions, a significantly more challenging task compared to outputting confidence in a single interaction. Experimenting on open-source agentic models, we first find that models exhibit much higher task accuracy at high confidence while having near-zero accuracy when confidence is low. Based on this observation, we propose Test-Time Scaling (TTS) methods that use confidence scores to determine answer quality, encourage the model to try again until reaching a satisfactory confidence level. Results show that our proposed methods significantly reduce token consumption while demonstrating competitive performance compared to baseline fixed budget TTS methods.",
    "metadata": {
      "authors": [
        "Litu Ou",
        "Kuan Li",
        "Huifeng Yin",
        "Liwen Zhang",
        "Zhongwang Zhang",
        "Xixi Wu",
        "Rui Ye",
        "Zile Qiao",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23458v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_13",
    "source": "arxiv_metadata",
    "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
    "text": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.",
    "metadata": {
      "authors": [
        "Zhuoran Jin",
        "Hongbang Yuan",
        "Kejian Zhu",
        "Jiachun Li",
        "Pengfei Cao",
        "Yubo Chen",
        "Kang Liu",
        "Jun Zhao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23451v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_14",
    "source": "arxiv_metadata",
    "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration",
    "text": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration The growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases, statutes, and technical vulnerabilities. This knowledge divide hinders collaboration between legal experts and cybersecurity professionals. To address this important gap, this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyber-legal domain. We demonstrate promising initial results on multilingual tasks.",
    "metadata": {
      "authors": [
        "Chiara Bonfanti",
        "Alessandro Druetto",
        "Cataldo Basile",
        "Tharindu Ranasinghe",
        "Marcos Zampieri"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23443v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_15",
    "source": "arxiv_metadata",
    "title": "EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting",
    "text": "EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting The immense success of the Transformer architecture in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown. However, a recent important paper questioned their effectiveness by demonstrating that a simple single layer linear model outperforms Transformer-based models. This was soon shown to be not as valid, by a better transformer-based model termed PatchTST. More re cently, TimeLLM demonstrated even better results by repurposing a Large Language Model (LLM) for the TSF domain. Again, a follow up paper challenged this by demonstrating that removing the LLM component or replacing it with a basic attention layer in fact yields better performance. One of the challenges in forecasting is the fact that TSF data favors the more recent past, and is sometimes subject to unpredictable events. Based upon these recent insights in TSF, we propose a strong Mixture of Experts (MoE) framework. Our method combines the state-of-the-art (SOTA) models including xLSTM, en hanced Linear, PatchTST, and minGRU, among others. This set of complimentary and diverse models for TSF are integrated in a Trans former based MoE gating network. Our proposed model outperforms all existing TSF models on standard benchmarks, surpassing even the latest approaches based on MoE frameworks.",
    "metadata": {
      "authors": [
        "Musleh Alharthi",
        "Kaleel Mahmood",
        "Sarosh Patel",
        "Ausif Mahmood"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23396v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_16",
    "source": "arxiv_metadata",
    "title": "Detecting Religious Language in Climate Discourse",
    "text": "Detecting Religious Language in Climate Discourse Religious language continues to permeate contemporary discourse, even in ostensibly secular domains such as environmental activism and climate change debates. This paper investigates how explicit and implicit forms of religious language appear in climate-related texts produced by secular and religious nongovernmental organizations (NGOs). We introduce a dual methodological approach: a rule-based model using a hierarchical tree of religious terms derived from ecotheology literature, and large language models (LLMs) operating in a zero-shot setting. Using a dataset of more than 880,000 sentences, we compare how these methods detect religious language and analyze points of agreement and divergence. The results show that the rule-based method consistently labels more sentences as religious than LLMs. These findings highlight not only the methodological challenges of computationally detecting religious language but also the broader tension over whether religious language should be defined by vocabulary alone or by contextual meaning. This study contributes to digital methods in religious studies by demonstrating both the potential and the limitations of approaches for analyzing how the sacred persists in climate discourse.",
    "metadata": {
      "authors": [
        "Evy Beijen",
        "Pien Pieterse",
        "Yusuf Çelik",
        "Willem Th. van Peursen",
        "Sandjai Bhulai",
        "Meike Morren"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23395v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_17",
    "source": "arxiv_metadata",
    "title": "How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes",
    "text": "How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes Artificial intelligence is reshaping labor markets, yet we lack tools to systematically forecast its effects on employment. This paper introduces a benchmark for evaluating how well large language models (LLMs) can anticipate changes in job demand, especially in occupations affected by AI. Existing research has shown that LLMs can extract sentiment, summarize economic reports, and emulate forecaster behavior, but little work has assessed their use for forward-looking labor prediction. Our benchmark combines two complementary datasets: a high-frequency index of sector-level job postings in the United States, and a global dataset of projected occupational changes due to AI adoption. We format these data into forecasting tasks with clear temporal splits, minimizing the risk of information leakage. We then evaluate LLMs using multiple prompting strategies, comparing task-scaffolded, persona-driven, and hybrid approaches across model families. We assess both quantitative accuracy and qualitative consistency over time. Results show that structured task prompts consistently improve forecast stability, while persona prompts offer advantages on short-term trends. However, performance varies significantly across sectors and horizons, highlighting the need for domain-aware prompting and rigorous evaluation protocols. By releasing our benchmark, we aim to support future research on labor forecasting, prompt design, and LLM-based economic reasoning. This work contributes to a growing body of research on how LLMs interact with real-world economic data, and provides a reproducible testbed for studying the limits and opportunities of AI as a forecasting tool in the context of labor markets.",
    "metadata": {
      "authors": [
        "Sheri Osborn",
        "Rohit Valecha",
        "H. Raghav Rao",
        "Dan Sass",
        "Anthony Rios"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23358v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_18",
    "source": "arxiv_metadata",
    "title": "LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data",
    "text": "LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data The scarcity of high-quality knowledge graphs (KGs) remains a critical bottleneck for downstream AI applications, as existing extraction methods rely heavily on error-prone pattern-matching techniques or resource-intensive large language models (LLMs). While recent tools leverage LLMs to generate KGs, their computational demands limit accessibility for low-resource environments. Our paper introduces LightKGG, a novel framework that enables efficient KG extraction from textual data using small-scale language models (SLMs) through two key technical innovations: (1) Context-integrated Graph extraction integrates contextual information with nodes and edges into a unified graph structure, reducing the reliance on complex semantic processing while maintaining more key information; (2) Topology-enhanced relationship inference leverages the inherent topology of the extracted graph to efficiently infer relationships, enabling relationship discovery without relying on complex language understanding capabilities of LLMs. By enabling accurate KG construction with minimal hardware requirements, this work bridges the gap between automated knowledge extraction and practical deployment scenarios while introducing scientifically rigorous methods for optimizing SLM efficiency in structured NLP tasks.",
    "metadata": {
      "authors": [
        "Teng Lin"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23341v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_19",
    "source": "arxiv_metadata",
    "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps",
    "text": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.",
    "metadata": {
      "authors": [
        "Anwesha Das",
        "John Duff",
        "Jörg Hoffmann",
        "Vera Demberg"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23340v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_20",
    "source": "arxiv_metadata",
    "title": "BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning",
    "text": "BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning Human-like virtual characters are crucial for games, storytelling, and virtual reality, yet current methods rely heavily on annotated data or handcrafted persona prompts, making it difficult to scale up and generate realistic, contextually coherent personas. We create the first QA dataset for BaZi-based persona reasoning, where real human experiences categorized into wealth, health, kinship, career, and relationships are represented as life-event questions and answers. Furthermore, we propose the first BaZi-LLM system that integrates symbolic reasoning with large language models to generate temporally dynamic and fine-grained virtual personas. Compared with mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a 30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information is used, our model's accuracy drops by 20%-45%, showing the potential of culturally grounded symbolic-LLM integration for realistic character simulation.",
    "metadata": {
      "authors": [
        "Siyuan Zheng",
        "Pai Liu",
        "Xi Chen",
        "Jizheng Dong",
        "Sihan Jia"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23337v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_21",
    "source": "arxiv_metadata",
    "title": "Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models",
    "text": "Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models LLM alignment remains a critical challenge. Inference-time methods provide a flexible alternative to fine-tuning, but their uniform computational effort often yields suboptimal alignment. We hypothesize that for many alignment tasks, the initial tokens of a response are disproportionately more critical. To leverage this principle, we introduce AdaSearch, a novel blockwise search strategy. It adaptively allocates a fixed computational budget using a sampling schedule, focusing search effort on these critical tokens. We apply AdaSearch to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our comprehensive evaluation across eight LLMs demonstrates that AdaSearch outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates improve by over 10% for harmlessness generation, controlled sentiment generation, and for mathematical reasoning tasks relative to Best-of-N.",
    "metadata": {
      "authors": [
        "Mohammad Atif Quamar",
        "Mohammad Areeb",
        "Nishant Sharma",
        "Ananth Shreekumar",
        "Jonathan Rosenthal",
        "Muslum Ozgur Ozmen",
        "Mikhail Kuznetsov",
        "Z. Berkay Celik"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23334v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_22",
    "source": "arxiv_metadata",
    "title": "LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization",
    "text": "LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization We introduce LibriConvo, a simulated multi-speaker conversational dataset based on speaker-aware conversation simulation (SASC), designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. Unlike prior resources that mostly rely on semantically disconnected utterances and implausible temporal gaps, LibriConvo ensures semantic coherence and realistic conversational timing. Our pipeline leverages CallHome with external VAD for reliable boundaries, applies compression to reduce unnaturally long silences, and organizes LibriTTS utterances by book to maintain contextual consistency. Acoustic realism is enhanced via a novel room impulse response selection procedure that ranks speaker-microphone configurations by spatial plausibility, balancing realism and diversity. The dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers, split in a speaker-disjoint manner for robust evaluation. Baselines show that the sortformer model outperforms the pyannote pipeline in diarization, while a fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves 7.29\\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides a valuable resource for advancing multi-speaker speech processing research with realistic conversational dynamics and controlled experimental conditions.",
    "metadata": {
      "authors": [
        "Máté Gedeon",
        "Péter Mihajlik"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23320v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_23",
    "source": "arxiv_metadata",
    "title": "Arabic Little STT: Arabic Children Speech Recognition Dataset",
    "text": "Arabic Little STT: Arabic Children Speech Recognition Dataset The performance of Artificial Intelligence (AI) systems fundamentally depends on high-quality training data. However, low-resource languages like Arabic suffer from severe data scarcity. Moreover, the absence of child-specific speech corpora is an essential gap that poses significant challenges. To address this gap, we present our created dataset, Arabic Little STT, a dataset of Levantine Arabic child speech recorded in classrooms, containing 355 utterances from 288 children (ages 6 - 13). We further conduct a systematic assessment of Whisper, a state-of-the-art automatic speech recognition (ASR) model, on this dataset and compare its performance with adult Arabic benchmarks. Our evaluation across eight Whisper variants reveals that even the best-performing model (Large_v3) struggles significantly, achieving a 0.66 word error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on adult datasets. These results align with other research on English speech. Results highlight the critical need for dedicated child speech benchmarks and inclusive training data in ASR development. Emphasizing that such data must be governed by strict ethical and privacy frameworks to protect sensitive child information. We hope that this study provides an initial step for future work on equitable speech technologies for Arabic-speaking children. We hope that our publicly available dataset enrich the children's demographic representation in ASR datasets.",
    "metadata": {
      "authors": [
        "Mouhand Alkadri",
        "Dania Desouki",
        "Khloud Al Jallad"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23319v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_24",
    "source": "arxiv_metadata",
    "title": "DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model",
    "text": "DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model Text-to-SQL tasks have gained attractive improvements since the release of ChatGPT. Among them, agent-based frameworks have been widely used in this field. However, the impact of data-centric strategies on text-to-SQL tasks has rarely been explored. In this paper, we systemically design a fully automated data-centric pipeline for text-to-SQL tasks, including \\emph{adaptive data repair}, which can automatically find and fix errors in the training dataset; and \\emph{error data augmentation}, where we specifically diffuse and enhance erroneous data predicted by the initially trained models. Meanwhile, we propose a Multi-Model collaboration training schema, aiming to train multiple models with different augmented data, enabling them to possess distinct capabilities and work together to complement each other, because it has been found that the capability of a single fine-tuned model is very limited. Furthermore, we utilize an ensemble strategy to integrate the capabilities of multiple models to solve a multiple-choice question, aiming to further improve the accuracy of text-to-SQL tasks. The experiment results and ablation study have demonstrated the effectiveness of data-centric pipeline and Multi-Model(MM) interactive iterative strategies, achieving first place in lightweight text-to-SQL models (within 70B).",
    "metadata": {
      "authors": [
        "Yuanzhen Xie",
        "Liu Ye",
        "Jiqun Chu",
        "Mochi Gao",
        "Hehuan Liu",
        "Yunzhi Tan",
        "Bo Hu",
        "Zang Li"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23284v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_25",
    "source": "arxiv_metadata",
    "title": "A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results",
    "text": "A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in the ninth CHiME Challenge, which addresses the cocktail-party problem of overlapping conversations in a single-room setting using audio, visual, and contextual cues. MCoRec captures natural multi-party conversations where the recordings focus on unscripted, casual group chats, leading to extreme speech overlap of up to 100% and highly fragmented conversational turns. The task requires systems to answer the question \"Who speaks when, what, and with whom?\" by jointly transcribing each speaker's speech and clustering them into their respective conversations from audio-visual recordings. Audio-only baselines exceed 100% word error rate, whereas incorporating visual cues yields substantial 50% improvements, highlighting the importance of multi-modality. In this manuscript, we present the motivation behind the task, outline the data collection process, and report the baseline systems developed for the MCoRec.",
    "metadata": {
      "authors": [
        "Thai-Binh Nguyen",
        "Katerina Zmolikova",
        "Pingchuan Ma",
        "Ngoc Quan Pham",
        "Christian Fuegen",
        "Alexander Waibel"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23276v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_26",
    "source": "arxiv_metadata",
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "text": "Code Aesthetics with Agentic Reward Feedback Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.",
    "metadata": {
      "authors": [
        "Bang Xiao",
        "Lingjie Jiang",
        "Shaohan Huang",
        "Tengchao Lv",
        "Yupan Huang",
        "Xun Wu",
        "Lei Cui",
        "Furu Wei"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23272v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_27",
    "source": "arxiv_metadata",
    "title": "Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding",
    "text": "Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding Mubeen is a proprietary Arabic language model developed by MASARAT SA, optimized for deep understanding of Arabic linguistics, Islamic studies, and cultural heritage. Trained on an extensive collection of authentic Arabic sources significantly expanded by digitizing historical manuscripts via a proprietary Arabic OCR engine, the model incorporates seminal scholarly works in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside thousands of academic theses and peer-reviewed research papers. Conditioned through a deep linguistic engineering framework, Mubeen masters not just the meaning but the eloquence of Arabic, enabling precise understanding across classical texts, contemporary writing, and regional dialects with focus on comprehending user intent and delivering accurate, contextually relevant responses. Unlike other Arabic models relying on translated English data that often fail in intent detection or retrieval-augmented generation (RAG), Mubeen uses native Arabic sources to ensure cultural authenticity and accuracy. Its core innovation is the Practical Closure Architecture, designed to solve the \"Utility Gap Crisis\" where factually correct answers fail to resolve users' core needs, forcing them into frustrating cycles of re-prompting. By prioritizing clarity and decisive guidance, Mubeen transforms from an information repository into a decisive guide, aligning with Saudi Vision 2030. The model's architecture combines deep heritage specialization with multi-disciplinary expert modules, enabling robust performance across both cultural preservation and general knowledge domains.",
    "metadata": {
      "authors": [
        "Mohammed Aljafari",
        "Ismail Alturki",
        "Ahmed Mori",
        "Yehya Kadumi"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23271v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_28",
    "source": "arxiv_metadata",
    "title": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?",
    "text": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages? Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations but dialect specific model training alleviates the issue. Our dataset also serves as a out of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available",
    "metadata": {
      "authors": [
        "Tawsif Tashwar Dipto",
        "Azmol Hossain",
        "Rubayet Sabbir Faruque",
        "Md. Rezuwan Hassan",
        "Kanij Fatema",
        "Tanmoy Shome",
        "Ruwad Naswan",
        "Md. Foriduzzaman Zihad",
        "Mohaymen Ul Anam",
        "Nazia Tasnim",
        "Hasan Mahmud",
        "Md Kamrul Hasan",
        "Md. Mehedi Hasan Shawon",
        "Farig Sadeque",
        "Tahsin Reasat"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23252v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_29",
    "source": "arxiv_metadata",
    "title": "Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports",
    "text": "Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations",
    "metadata": {
      "authors": [
        "Alois Thomas",
        "Maya Varma",
        "Jean-Benoit Delbrouck",
        "Curtis P. Langlotz"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23217v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_30",
    "source": "arxiv_metadata",
    "title": "PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets",
    "text": "PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets Continual pre-training (CPT) for domain adaptation must balance target-domain gains with stability on the base domain. Existing CPT scaling laws typically assume a fixed pre-training budget, which limits their ability to forecast adaptation outcomes for models trained at different tokens-per-parameter (PTPP). We present \\emph{PTPP-aware} adaptation scaling laws that make the pre-training budget an explicit variable, enabling accurate \\emph{prediction} of adaptation loss at unseen \\ptpp. On a multilingual setup (English/Arabic $\\rightarrow$ French), PTPP-aware formulations trained on early stages (\\ptpp{}=\\{15,31\\}) predict target loss at \\ptpp{}=279 and outperform a PTPP-agnostic \\dcpt{} transfer baseline on metrics (Huber-on-log, MAE$_\\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in the appendix. Beyond forecasting, we show a practical use case: planning replay ratios and adaptation token budgets that satisfy target and forgetting constraints under compute limits.",
    "metadata": {
      "authors": [
        "Etienne Goffinet",
        "Shane Bergsma",
        "Avraham Sheinin",
        "Natalia Vassilieva",
        "Shaheer Muhammad",
        "Preslav Nakov",
        "Gurpreet Gosal"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23198v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_31",
    "source": "arxiv_metadata",
    "title": "DREaM: Drug-Drug Relation Extraction via Transfer Learning Method",
    "text": "DREaM: Drug-Drug Relation Extraction via Transfer Learning Method Relation extraction between drugs plays a crucial role in identifying drug drug interactions and predicting side effects. The advancement of machine learning methods in relation extraction, along with the development of large medical text databases, has enabled the low cost extraction of such relations compared to other approaches that typically require expert knowledge. However, to the best of our knowledge, there are limited datasets specifically designed for drug drug relation extraction currently available. Therefore, employing transfer learning becomes necessary to apply machine learning methods in this domain. In this study, we propose DREAM, a method that first employs a trained relation extraction model to discover relations between entities and then applies this model to a corpus of medical texts to construct an ontology of drug relationships. The extracted relations are subsequently validated using a large language model. Quantitative results indicate that the LLM agreed with 71 of the relations extracted from a subset of PubMed abstracts. Furthermore, our qualitative analysis indicates that this approach can uncover ambiguities in the medical domain, highlighting the challenges inherent in relation extraction in this field.",
    "metadata": {
      "authors": [
        "Ali Fata",
        "Hossein Rahmani",
        "Parinaz Soltanzadeh",
        "Amirhossein Derakhshan",
        "Behrouz Minaei Bidgoli"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23189v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_32",
    "source": "arxiv_metadata",
    "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations",
    "text": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.",
    "metadata": {
      "authors": [
        "Shuai Huang",
        "Wenxuan Zhao",
        "Jun Gao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23182v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_33",
    "source": "arxiv_metadata",
    "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning",
    "text": "MATCH: Task-Driven Code Evaluation through Contrastive Learning AI-based code generation is increasingly prevalent, with GitHub Copilot estimated to generate 46% of the code on GitHub. Accurately evaluating how well generated code aligns with developer intent remains a critical challenge. Traditional evaluation methods, such as unit tests, are often unscalable and costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code functionality, and metrics like CodeBERTScore require reference code, which is not always available. To address the gap in reference-free evaluation, with few alternatives such as ICE-Score, this paper introduces MATCH, a novel reference-free metric. MATCH uses Contrastive Learning to generate meaningful embeddings for code and natural language task descriptions, enabling similarity scoring that reflects how well generated code implements the task. We show that MATCH achieves stronger correlations with functional correctness and human preference than existing metrics across multiple programming languages.",
    "metadata": {
      "authors": [
        "Marah Ghoummaid",
        "Vladimir Tchuiev",
        "Ofek Glick",
        "Michal Moschkovitz",
        "Dotan Di Castro"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23169v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_34",
    "source": "arxiv_metadata",
    "title": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs",
    "text": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains.",
    "metadata": {
      "authors": [
        "Hang Lei",
        "Shengyi Zong",
        "Zhaoyan Li",
        "Ziren Zhou",
        "Hao Liu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23163v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_35",
    "source": "arxiv_metadata",
    "title": "ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix",
    "text": "ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs) to domain-specific instructions by training on a carefully curated subset of high-quality instruction-response pairs, typically drawn from a larger dataset that often contains many low-quality or noisy samples. However, existing quality-first paradigms often overlook valuable signals in discarded low-quality data and rely on imperfect quality filters. We introduce ENTP (Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a framework that revitalizes low-quality corpora through symbolic purification and neural reconstruction. The symbolic module identifies and prunes noisy samples based on statistical priors, while the neural component synthesizes enriched instruction-response pairs by leveraging latent representations and model knowledge. This neural-symbolic synergy enhances data informativeness and diversity. Experiments show that ENTP-augmented datasets, constructed exclusively from low-quality data, outperform 13 established data-selection baselines across five instruction-following benchmarks, and even surpass fine-tuning on the full original dataset (approximately 300K examples). Our results highlight the untapped potential of low-quality data and underscore the importance of intelligent purification and synthesis for efficient instruction alignment.",
    "metadata": {
      "authors": [
        "Zile Yang",
        "Ling Li",
        "Na Di",
        "Jinlong Pang",
        "Yao Zhou",
        "Hao Cheng",
        "Bo Han",
        "Jiaheng Wei"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23160v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_36",
    "source": "arxiv_metadata",
    "title": "Rethinking GSPO: The Perplexity-Entropy Equivalence",
    "text": "Rethinking GSPO: The Perplexity-Entropy Equivalence We provide a new perspective on GSPO's length-normalized importance ratios by establishing their connection to information-theoretic quantities. We show that GSPO's sequence-level weight $s(\\theta) = (\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed as the inverse perplexity ratio $\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential cross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy relationship follows from standard definitions, this observation provides a useful lens for understanding GSPO: the algorithm weights policy gradient updates by perplexity ratios, offering an information-theoretic interpretation of the importance weights. This perspective helps explain GSPO's empirical properties, including log-domain variance reduction through geometric averaging and stability in training mixture-of-experts models. We validate the mathematical equivalences and variance predictions through controlled experiments on mathematical reasoning tasks.",
    "metadata": {
      "authors": [
        "Chi Liu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23142v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_37",
    "source": "arxiv_metadata",
    "title": "Corpus Frequencies in Morphological Inflection: Do They Matter?",
    "text": "Corpus Frequencies in Morphological Inflection: Do They Matter? The traditional approach to morphological inflection (the task of modifying a base word (lemma) to express grammatical categories) has been, for decades, to consider lexical entries of lemma-tag-form triples uniformly, lacking any information about their frequency distribution. However, in production deployment, one might expect the user inputs to reflect a real-world distribution of frequencies in natural texts. With future deployment in mind, we explore the incorporation of corpus frequency information into the task of morphological inflection along three key dimensions during system development: (i) for train-dev-test split, we combine a lemma-disjoint approach, which evaluates the model's generalization capabilities, with a frequency-weighted strategy to better reflect the realistic distribution of items across different frequency bands in training and test sets; (ii) for evaluation, we complement the standard type accuracy (often referred to simply as accuracy), which treats all items equally regardless of frequency, with token accuracy, which assigns greater weight to frequent words and better approximates performance on running text; (iii) for training data sampling, we introduce a method novel in the context of inflection, frequency-aware training, which explicitly incorporates word frequency into the sampling process. We show that frequency-aware training outperforms uniform sampling in 26 out of 43 languages.",
    "metadata": {
      "authors": [
        "Tomáš Sourada",
        "Jana Straková"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23131v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_38",
    "source": "arxiv_metadata",
    "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
    "text": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). LoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank. In standard LoRA, all input tokens share the same weights and undergo an identical input-output projection. This limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens. To address this limitation, we propose Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner. Formally, the weights of TopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections). Extensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. The code is available at https://github.com/Leopold1423/toplora-neurips25.",
    "metadata": {
      "authors": [
        "Shiwei Li",
        "Xiandi Luo",
        "Haozhao Wang",
        "Xing Tang",
        "Ziqiang Cui",
        "Dugang Liu",
        "Yuhua Li",
        "Xiuqiang He",
        "Ruixuan Li"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23123v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_39",
    "source": "arxiv_metadata",
    "title": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
    "text": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models. In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack of an open-source, general-purpose, multilingual morphological inflection system capable of handling unseen words across a wide range of languages, including Czech. All code is publicly released at: https://github.com/tomsouri/multilingual-inflection.",
    "metadata": {
      "authors": [
        "Tomáš Sourada",
        "Jana Straková"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23114v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_40",
    "source": "arxiv_metadata",
    "title": "Leveraging Hierarchical Organization for Medical Multi-document Summarization",
    "text": "Leveraging Hierarchical Organization for Medical Multi-document Summarization Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.",
    "metadata": {
      "authors": [
        "Yi-Li Hsu",
        "Katelyn X. Mei",
        "Lucy Lu Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23104v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_41",
    "source": "arxiv_metadata",
    "title": "MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models",
    "text": "MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models Recent advances have investigated the use of pretrained large language models (LLMs) for time-series forecasting by aligning numerical inputs with LLM embedding spaces. However, existing multimodal approaches often overlook the distinct statistical properties and temporal dependencies that are fundamental to time-series data. To bridge this gap, we propose MAP4TS, a novel Multi-Aspect Prompting Framework that explicitly incorporates classical time-series analysis into the prompt design. Our framework introduces four specialized prompt components: a Global Domain Prompt that conveys dataset-level context, a Local Domain Prompt that encodes recent trends and series-specific behaviors, and a pair of Statistical and Temporal Prompts that embed handcrafted insights derived from autocorrelation (ACF), partial autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined with raw time-series embeddings and passed through a cross-modality alignment module to produce unified representations, which are then processed by an LLM and projected for final forecasting. Extensive experiments across eight diverse datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based methods. Our ablation studies further reveal that prompt-aware designs significantly enhance performance stability and that GPT-2 backbones, when paired with structured prompts, outperform larger models like LLaMA in long-term forecasting tasks.",
    "metadata": {
      "authors": [
        "Suchan Lee",
        "Jihoon Choi",
        "Sohyeon Lee",
        "Minseok Song",
        "Bong-Gyu Jang",
        "Hwanjo Yu",
        "Soyeon Caren Han"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23090v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_42",
    "source": "arxiv_metadata",
    "title": "A Survey on LLM Mid-training",
    "text": "A Survey on LLM Mid-training Recent advances in foundation models have highlighted the significant benefits of multi-stage training, with a particular emphasis on the emergence of mid-training as a vital stage that bridges pre-training and post-training. Mid-training is distinguished by its use of intermediate data and computational resources, systematically enhancing specified capabilities such as mathematics, coding, reasoning, and long-context extension, while maintaining foundational competencies. This survey provides a formal definition of mid-training for large language models (LLMs) and investigates optimization frameworks that encompass data curation, training strategies, and model architecture optimization. We analyze mainstream model implementations in the context of objective-driven interventions, illustrating how mid-training serves as a distinct and critical stage in the progressive development of LLM capabilities. By clarifying the unique contributions of mid-training, this survey offers a comprehensive taxonomy and actionable insights, supporting future research and innovation in the advancement of LLMs.",
    "metadata": {
      "authors": [
        "Chengying Tu",
        "Xuemiao Zhang",
        "Rongxiang Weng",
        "Rumei Li",
        "Chen Zhang",
        "Yang Bai",
        "Hongfei Yan",
        "Jingang Wang",
        "Xunliang Cai"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23081v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_43",
    "source": "arxiv_metadata",
    "title": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs",
    "text": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library for efficiently evaluating membership inference attacks (MIA) against Large Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due to growing concerns over copyright, security, and data privacy, and has attracted increasing research attention. However, the progress of this research is significantly hindered by two main obstacles: (1) the high computational cost of inference in LLMs, and (2) the lack of standardized and maintained implementations of MIA methods, which makes large-scale empirical comparison difficult. To address these challenges, our library provides fast batch inference and includes implementations of representative MIA methods under a unified evaluation framework. This library supports easy implementation of reproducible benchmarks with simple configuration and extensibility. We release Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and transparent research on LLMs.",
    "metadata": {
      "authors": [
        "Hiromu Takahashi",
        "Shotaro Ishihara"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23074v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_44",
    "source": "arxiv_metadata",
    "title": "Quality-Aware Translation Tagging in Multilingual RAG system",
    "text": "Quality-Aware Translation Tagging in Multilingual RAG system Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English documents and translates them into the query language for low-resource settings. However, poor translation quality degrades response generation performance. Existing approaches either assume sufficient translation quality or utilize the rewriting method, which introduces factual distortion and hallucinations. To mitigate these problems, we propose Quality-Aware Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation quality along three dimensions-semantic equivalence, grammatical accuracy, and naturalness&fluency-and attach these scores as metadata without altering the original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs ranging from 2.4B to 14B parameters, covering two low-resource languages (Korean and Finnish) and one high-resource language (Chinese). QTT-RAG outperforms the baselines by preserving factual integrity while enabling generator models to make informed decisions based on translation reliability. This approach allows for effective usage of cross-lingual documents in low-resource settings with limited native language documents, offering a practical and robust solution across multilingual domains.",
    "metadata": {
      "authors": [
        "Hoyeon Moon",
        "Byeolhee Kim",
        "Nikhil Verma"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23070v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_45",
    "source": "arxiv_metadata",
    "title": "Knocking-Heads Attention",
    "text": "Knocking-Heads Attention Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to \"knock\" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.",
    "metadata": {
      "authors": [
        "Zhanchao Zhou",
        "Xiaodong Chen",
        "Haoxing Chen",
        "Zhenzhong Lan",
        "Jianguo Li"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23052v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_46",
    "source": "arxiv_metadata",
    "title": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning",
    "text": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, we propose TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation. TIR-Judge is built on three principles: (i) diverse training across verifiable and non-verifiable domains, (ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii) iterative RL that bootstraps directly from the initial model without distillation. On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning.",
    "metadata": {
      "authors": [
        "Ran Xu",
        "Jingjing Chen",
        "Jiayu Ye",
        "Yu Wu",
        "Jun Yan",
        "Carl Yang",
        "Hongkun Yu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23038v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_47",
    "source": "arxiv_metadata",
    "title": "Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts",
    "text": "Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts Recent advances in reinforcement learning (RL) have substantially improved the training of large-scale language models, leading to significant gains in generation quality and reasoning ability. However, most existing research focuses on dense models, while RL training for Mixture-of-Experts (MoE) architectures remains underexplored. To address the instability commonly observed in MoE training, we propose a novel router-aware approach to optimize importance sampling (IS) weights in off-policy RL. Specifically, we design a rescaling strategy guided by router logits, which effectively reduces gradient variance and mitigates training divergence. Experimental results demonstrate that our method significantly improves both the convergence stability and the final performance of MoE models, highlighting the potential of RL algorithmic innovations tailored to MoE architectures and providing a promising direction for efficient training of large-scale expert models.",
    "metadata": {
      "authors": [
        "Di Zhang",
        "Xun Wu",
        "Shaohan Huang",
        "Yaru Hao",
        "Li Dong",
        "Zewen Chi",
        "Zhifang Sui",
        "Furu Wei"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23027v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_48",
    "source": "arxiv_metadata",
    "title": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization",
    "text": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization With the rapid proliferation of image generative models, the authenticity of digital images has become a significant concern. While existing studies have proposed various methods for detecting AI-generated content, current benchmarks are limited in their coverage of diverse generative models and image categories, often overlooking end-to-end image editing and artistic images. To address these limitations, we introduce UniAIDet, a unified and comprehensive benchmark that includes both photographic and artistic images. UniAIDet covers a wide range of generative models, including text-to-image, image-to-image, image inpainting, image editing, and deepfake models. Using UniAIDet, we conduct a comprehensive evaluation of various detection methods and answer three key research questions regarding generalization capability and the relation between detection and localization. Our benchmark and analysis provide a robust foundation for future research.",
    "metadata": {
      "authors": [
        "Huixuan Zhang",
        "Xiaojun Wan"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23023v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_49",
    "source": "arxiv_metadata",
    "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
    "text": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark Text-to-image models are known to struggle with generating images that perfectly align with textual prompts. Several previous studies have focused on evaluating image-text alignment in text-to-image generation. However, these evaluations either address overly simple scenarios, especially overlooking the difficulty of prompts with multiple different instances belonging to the same category, or they introduce metrics that do not correlate well with human evaluation. In this study, we introduce M$^3$T2IBench, a large-scale, multi-category, multi-instance, multi-relation along with an object-detection-based evaluation metric, $AlignScore$, which aligns closely with human evaluation. Our findings reveal that current open-source text-to-image models perform poorly on this challenging benchmark. Additionally, we propose the Revise-Then-Enforce approach to enhance image-text alignment. This training-free post-editing method demonstrates improvements in image-text alignment across a broad range of diffusion models. \\footnote{Our code and data has been released in supplementary material and will be made publicly available after the paper is accepted.}",
    "metadata": {
      "authors": [
        "Huixuan Zhang",
        "Xiaojun Wan"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23020v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_50",
    "source": "arxiv_metadata",
    "title": "LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models",
    "text": "LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.",
    "metadata": {
      "authors": [
        "Sammriddh Gupta",
        "Sonit Singh",
        "Aditya Joshi",
        "Mira Kim"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23011v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_51",
    "source": "arxiv_metadata",
    "title": "Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures",
    "text": "Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures We perform in-depth evaluations of in-context learning (ICL) on state-of-the-art transformer, state-space, and hybrid large language models over two categories of knowledge-based ICL tasks. Using a combination of behavioral probing and intervention-based methods, we have discovered that, while LLMs of different architectures can behave similarly in task performance, their internals could remain different. We discover that function vectors (FVs) responsible for ICL are primarily located in the self-attention and Mamba layers, and speculate that Mamba2 uses a different mechanism from FVs to perform ICL. FVs are more important for ICL involving parametric knowledge retrieval, but not for contextual knowledge understanding. Our work contributes to a more nuanced understanding across architectures and task types. Methodologically, our approach also highlights the importance of combining both behavioural and mechanistic analyses to investigate LLM capabilities.",
    "metadata": {
      "authors": [
        "Shenran Wang",
        "Timothy Tin-Long Tse",
        "Jian Zhu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.23006v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_52",
    "source": "arxiv_metadata",
    "title": "Can Language Models Compose Skills In-Context?",
    "text": "Can Language Models Compose Skills In-Context? Composing basic skills from simple tasks to accomplish composite tasks is crucial for modern intelligent systems. We investigate the in-context composition ability of language models to perform composite tasks that combine basic skills demonstrated in in-context examples. This is more challenging than the standard setting, where skills and their composition can be learned in training. We conduct systematic experiments on various representative open-source language models, utilizing linguistic and logical tasks designed to probe composition abilities. The results reveal that simple task examples can have a surprising negative impact on the performance, because the models generally struggle to recognize and assemble the skills correctly, even with Chain-of-Thought examples. Theoretical analysis further shows that it is crucial to align examples with the corresponding steps in the composition. This inspires a method for the probing tasks, whose improved performance provides positive support for our insights.",
    "metadata": {
      "authors": [
        "Zidong Liu",
        "Zhuoyan Xu",
        "Zhenmei Shi",
        "Yingyu Liang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22993v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_53",
    "source": "arxiv_metadata",
    "title": "Measuring Teaching with LLMs",
    "text": "Measuring Teaching with LLMs Objective and scalable measurement of teaching quality is a persistent challenge in education. While Large Language Models (LLMs) offer potential, general-purpose models have struggled to reliably apply complex, authentic classroom observation instruments. This paper uses custom LLMs built on sentence-level embeddings, an architecture better suited for the long-form, interpretive nature of classroom transcripts than conventional subword tokenization. We systematically evaluate five different sentence embeddings under a data-efficient training regime designed to prevent overfitting. Our results demonstrate that these specialized models can achieve human-level and even super-human performance with expert human ratings above 0.65 and surpassing the average human-human rater correlation. Further, through analysis of annotation context windows, we find that more advanced models-those better aligned with human judgments-attribute a larger share of score variation to lesson-level features rather than isolated utterances, challenging the sufficiency of single-turn annotation paradigms. Finally, to assess external validity, we find that aggregate model scores align with teacher value-added measures, indicating they are capturing features relevant to student learning. However, this trend does not hold at the individual item level, suggesting that while the models learn useful signals, they have not yet achieved full generalization. This work establishes a viable and powerful new methodology for AI-driven instructional measurement, offering a path toward providing scalable, reliable, and valid feedback for educator development.",
    "metadata": {
      "authors": [
        "Michael Hardy"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22968v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_54",
    "source": "arxiv_metadata",
    "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs",
    "text": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.",
    "metadata": {
      "authors": [
        "Yucheng Ning",
        "Xixun Lin",
        "Fang Fang",
        "Yanan Cao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22967v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_55",
    "source": "arxiv_metadata",
    "title": "Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts",
    "text": "Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts Recent investigations into effective context lengths of modern flagship large language models (LLMs) have revealed major limitations in effective question answering (QA) and reasoning over long and complex contexts for even the largest and most impressive cadre of models. While approaches like retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to mitigate this issue, they are sensitive to chunking, embedding and retrieval strategies and models, and furthermore, rely on extensive pre-processing, knowledge acquisition and indexing steps. In this paper, we propose Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy that boosts LLM performance in long-context scenarios, without degrading and altering the integrity and composition of retrieved documents. We validate our hypothesis by augmenting two challenging and directly relevant question-answering benchmarks -- NoLima and NovelQA -- and show that tagging the context or even just adding tag definitions into QA prompts leads to consistent performance gains over the baseline -- up to 17% for 32K token contexts, and 2.9% in complex reasoning question-answering for multi-hop queries requiring knowledge across a wide span of text. Additional details are available at https://sites.google.com/view/tag-emnlp.",
    "metadata": {
      "authors": [
        "Anwesan Pal",
        "Karen Hovsepian",
        "Tinghao Guo",
        "Mengnan Zhao",
        "Somendra Tripathi",
        "Nikos Kanakaris",
        "George Mihaila",
        "Sumit Nigam"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22956v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_56",
    "source": "arxiv_metadata",
    "title": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)",
    "text": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond) Language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Overall, INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.",
    "metadata": {
      "authors": [
        "Liwei Jiang",
        "Yuanjun Chai",
        "Margaret Li",
        "Mickel Liu",
        "Raymond Fok",
        "Nouha Dziri",
        "Yulia Tsvetkov",
        "Maarten Sap",
        "Alon Albalak",
        "Yejin Choi"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22954v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_57",
    "source": "arxiv_metadata",
    "title": "Language Server CLI Empowers Language Agents with Process Rewards",
    "text": "Language Server CLI Empowers Language Agents with Process Rewards Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli",
    "metadata": {
      "authors": [
        "Yifan Zhang",
        "Lanser Contributors"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22907v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_58",
    "source": "arxiv_metadata",
    "title": "Modeling Political Discourse with Sentence-BERT and BERTopic",
    "text": "Modeling Political Discourse with Sentence-BERT and BERTopic Social media has reshaped political discourse, offering politicians a platform for direct engagement while reinforcing polarization and ideological divides. This study introduces a novel topic evolution framework that integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to analyze the longevity and moral dimensions of political topics in Twitter activity during the 117th U.S. Congress. We propose a methodology for tracking dynamic topic shifts over time and measuring their association with moral values and quantifying topic persistence. Our findings reveal that while overarching themes remain stable, granular topics tend to dissolve rapidly, limiting their long-term influence. Moreover, moral foundations play a critical role in topic longevity, with Care and Loyalty dominating durable topics, while partisan differences manifest in distinct moral framing strategies. This work contributes to the field of social network analysis and computational political discourse by offering a scalable, interpretable approach to understanding moral-driven topic evolution on social media.",
    "metadata": {
      "authors": [
        "Margarida Mendonca",
        "Alvaro Figueira"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22904v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_59",
    "source": "arxiv_metadata",
    "title": "Offline Preference Optimization via Maximum Marginal Likelihood Estimation",
    "text": "Offline Preference Optimization via Maximum Marginal Likelihood Estimation Aligning Large Language Models (LLMs) with human preferences is crucial, but standard methods like Reinforcement Learning from Human Feedback (RLHF) are often complex and unstable. In this work, we propose a new, simpler approach that recasts alignment through the lens of Maximum Marginal Likelihood (MML) estimation. Our new MML based Preference Optimization (MMPO) maximizes the marginal log-likelihood of a preferred text output, using the preference pair as samples for approximation, and forgoes the need for both an explicit reward model and entropy maximization. We theoretically demonstrate that MMPO implicitly performs preference optimization, producing a weighted gradient that naturally up-weights chosen responses over rejected ones. Across models ranging from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable with respect to the hyperparameter $\\beta$ compared to alternative baselines, and 2) achieves competitive or superior preference alignment while better preserving the base model's general language capabilities. Through a series of ablation experiments, we show that this improved performance is indeed attributable to MMPO's implicit preference optimization within the gradient updates.",
    "metadata": {
      "authors": [
        "Saeed Najafi",
        "Alona Fyshe"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22881v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_60",
    "source": "arxiv_metadata",
    "title": "Batch Speculative Decoding Done Right",
    "text": "Batch Speculative Decoding Done Right Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3$\\times$ throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.",
    "metadata": {
      "authors": [
        "Ranran Haoran Zhang",
        "Soumik Dey",
        "Ashirbad Mishra",
        "Hansi Wu",
        "Binbin Li",
        "Rui Zhang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22876v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_61",
    "source": "arxiv_metadata",
    "title": "A Comprehensive Dataset for Human vs. AI Generated Text Detection",
    "text": "A Comprehensive Dataset for Human vs. AI Generated Text Detection The rapid advancement of large language models (LLMs) has led to increasingly human-like AI-generated text, raising concerns about content authenticity, misinformation, and trustworthiness. Addressing the challenge of reliably detecting AI-generated text and attributing it to specific models requires large-scale, diverse, and well-annotated datasets. In this work, we present a comprehensive dataset comprising over 58,000 text samples that combine authentic New York Times articles with synthetic versions generated by multiple state-of-the-art LLMs including Gemma-2-9b, Mistral-7B, Qwen-2-72B, LLaMA-8B, Yi-Large, and GPT-4-o. The dataset provides original article abstracts as prompts, full human-authored narratives. We establish baseline results for two key tasks: distinguishing human-written from AI-generated text, achieving an accuracy of 58.35\\%, and attributing AI texts to their generating models with an accuracy of 8.92\\%. By bridging real-world journalistic content with modern generative models, the dataset aims to catalyze the development of robust detection and attribution methods, fostering trust and transparency in the era of generative AI. Our dataset is available at: https://huggingface.co/datasets/gsingh1-py/train.",
    "metadata": {
      "authors": [
        "Rajarshi Roy",
        "Nasrin Imanpour",
        "Ashhar Aziz",
        "Shashwat Bajpai",
        "Gurpreet Singh",
        "Shwetangshu Biswas",
        "Kapil Wanaskar",
        "Parth Patwa",
        "Subhankar Ghosh",
        "Shreyas Dixit",
        "Nilesh Ranjan Pal",
        "Vipula Rawte",
        "Ritvik Garimella",
        "Gaytri Jena",
        "Amit Sheth",
        "Vasu Sharma",
        "Aishwarya Naresh Reganti",
        "Vinija Jain",
        "Aman Chadha",
        "Amitava Das"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22874v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_62",
    "source": "arxiv_metadata",
    "title": "Interpreting and Mitigating Unwanted Uncertainty in LLMs",
    "text": "Interpreting and Mitigating Unwanted Uncertainty in LLMs Despite their impressive capabilities, Large Language Models (LLMs) exhibit unwanted uncertainty, a phenomenon where a model changes a previously correct answer into an incorrect one when re-prompted. This behavior undermines trust and poses serious risks in high-stakes domains. In this work, we investigate the mechanisms that drive this phenomenon. We adapt the Needle-in-a-Haystack retrieval framework and integrate a Flip-style re-evaluation prompt to simulate realistic answer-flipping scenarios. We find that retrieval heads are not primarily responsible for avoiding uncertainty. Instead, we identify a small set of non-retrieval attention heads that disproportionately attend to misleading tokens in uncertain contexts. Masking these heads yields significant improvements, reducing flip behavior by up to 15% without introducing incoherence or overcorrection. However, when tested for downstream tasks, we observe trade-offs with flip behavior. Our findings contribute to the growing field of mechanistic interpretability and present a simple yet effective technique for mitigating uncertainty-driven failure modes in LLMs.",
    "metadata": {
      "authors": [
        "Tiasa Singha Roy",
        "Ayush Rajesh Jhaveri",
        "Ilias Triantafyllopoulos"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22866v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_63",
    "source": "arxiv_metadata",
    "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement",
    "text": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing.",
    "metadata": {
      "authors": [
        "Linyang He",
        "Tianjun Zhong",
        "Richard Antonello",
        "Gavin Mischler",
        "Micah Goldblum",
        "Nima Mesgarani"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22860v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_64",
    "source": "arxiv_metadata",
    "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis",
    "text": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains. We introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance-level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis. Experiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and 9.4% compared to PoT and CoT respectively, and reduces undesirable program generations by 65.1% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.",
    "metadata": {
      "authors": [
        "Adam Stein",
        "Neelay Velingker",
        "Mayur Naik",
        "Eric Wong"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22849v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_65",
    "source": "arxiv_metadata",
    "title": "Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning",
    "text": "Leveraging Large Language Models to Identify Conversation Threads in Collaborative Learning Understanding how ideas develop and flow in small-group conversations is critical for analyzing collaborative learning. A key structural feature of these interactions is threading, the way discourse talk naturally organizes into interwoven topical strands that evolve over time. While threading has been widely studied in asynchronous text settings, detecting threads in synchronous spoken dialogue remains challenging due to overlapping turns and implicit cues. At the same time, large language models (LLMs) show promise for automating discourse analysis but often struggle with long-context tasks that depend on tracing these conversational links. In this paper, we investigate whether explicit thread linkages can improve LLM-based coding of relational moves in group talk. We contribute a systematic guidebook for identifying threads in synchronous multi-party transcripts and benchmark different LLM prompting strategies for automated threading. We then test how threading influences performance on downstream coding of conversational analysis frameworks, that capture core collaborative actions such as agreeing, building, and eliciting. Our results show that providing clear conversational thread information improves LLM coding performance and underscores the heavy reliance of downstream analysis on well-structured dialogue. We also discuss practical trade-offs in time and cost, emphasizing where human-AI hybrid approaches can yield the best value. Together, this work advances methods for combining LLMs and robust conversational thread structures to make sense of complex, real-time group interactions.",
    "metadata": {
      "authors": [
        "Prerna Ravi",
        "Dong Won Lee",
        "Beatriz Flamia",
        "Jasmine David",
        "Brandon Hanks",
        "Cynthia Breazeal",
        "Emma Anderson",
        "Grace Lin"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22844v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_66",
    "source": "arxiv_metadata",
    "title": "Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays",
    "text": "Exploration of Summarization by Generative Language Models for Automated Scoring of Long Essays BERT and its variants are extensively explored for automated scoring. However, a limit of 512 tokens for these encoder-based models showed the deficiency in automated scoring of long essays. Thus, this research explores generative language models for automated scoring of long essays via summarization and prompting. The results revealed great improvement of scoring accuracy with QWK increased from 0.822 to 0.8878 for the Learning Agency Lab Automated Essay Scoring 2.0 dataset.",
    "metadata": {
      "authors": [
        "Haowei Hua",
        "Hong Jiao",
        "Xinyi Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22830v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_67",
    "source": "arxiv_metadata",
    "title": "Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP",
    "text": "Cross-Lingual Stability and Bias in Instruction-Tuned Language Models for Humanitarian NLP Humanitarian organizations face a critical choice: invest in costly commercial APIs or rely on free open-weight models for multilingual human rights monitoring. While commercial systems offer reliability, open-weight alternatives lack empirical validation -- especially for low-resource languages common in conflict zones. This paper presents the first systematic comparison of commercial and open-weight large language models (LLMs) for human-rights-violation detection across seven languages, quantifying the cost-reliability trade-off facing resource-constrained organizations. Across 78,000 multilingual inferences, we evaluate six models -- four instruction-aligned (Claude-Sonnet-4, DeepSeek-V3, Gemini-Flash-2.0, GPT-4.1-mini) and two open-weight (LLaMA-3-8B, Mistral-7B) -- using both standard classification metrics and new measures of cross-lingual reliability: Calibration Deviation (CD), Decision Bias (B), Language Robustness Score (LRS), and Language Stability Score (LSS). Results show that alignment, not scale, determines stability: aligned models maintain near-invariant accuracy and balanced calibration across typologically distant and low-resource languages (e.g., Lingala, Burmese), while open-weight models exhibit significant prompt-language sensitivity and calibration drift. These findings demonstrate that multilingual alignment enables language-agnostic reasoning and provide practical guidance for humanitarian organizations balancing budget constraints with reliability in multilingual deployment.",
    "metadata": {
      "authors": [
        "Poli Nemkova",
        "Amrit Adhikari",
        "Matthew Pearson",
        "Vamsi Krishna Sadu",
        "Mark V. Albert"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22823v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_68",
    "source": "arxiv_metadata",
    "title": "VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions",
    "text": "VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions Automatically assessing handwritten mathematical solutions is an important problem in educational technology with practical applications, but it remains a significant challenge due to the diverse formats, unstructured layouts, and symbolic complexity of student work. To address this challenge, we introduce VEHME-a Vision-Language Model for Evaluating Handwritten Mathematics Expressions-designed to assess open-form handwritten math responses with high accuracy and interpretable reasoning traces. VEHME integrates a two-phase training pipeline: (i) supervised fine-tuning using structured reasoning data, and (ii) reinforcement learning that aligns model outputs with multi-dimensional grading objectives, including correctness, reasoning depth, and error localization. To enhance spatial understanding, we propose an Expression-Aware Visual Prompting Module, trained on our synthesized multi-line math expressions dataset to robustly guide attention in visually heterogeneous inputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-art performance among open-source models and approaches the accuracy of proprietary systems, demonstrating its potential as a scalable and accessible tool for automated math assessment. Our training and experiment code is publicly available at our GitHub repository.",
    "metadata": {
      "authors": [
        "Thu Phuong Nguyen",
        "Duc M. Nguyen",
        "Hyotaek Jeon",
        "Hyunwook Lee",
        "Hyunmin Song",
        "Sungahn Ko",
        "Taehwan Kim"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22798v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_69",
    "source": "arxiv_metadata",
    "title": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations",
    "text": "How Do AI Agents Do Human Work? Comparing AI and Human Workflows Across Diverse Occupations AI agents are continually optimized for tasks related to human work, such as software engineering and professional writing, signaling a pressing trend with significant impacts on the human workforce. However, these agent developments have often not been grounded in a clear understanding of how humans execute work, to reveal what expertise agents possess and the roles they can play in diverse workflows. In this work, we study how agents do human work by presenting the first direct comparison of human and agent workers across multiple essential work-related skills: data analysis, engineering, computation, writing, and design. To better understand and compare heterogeneous computer-use activities of workers, we introduce a scalable toolkit to induce interpretable, structured workflows from either human or agent computer-use activities. Using such induced workflows, we compare how humans and agents perform the same tasks and find that: (1) While agents exhibit promise in their alignment to human workflows, they take an overwhelmingly programmatic approach across all work domains, even for open-ended, visually dependent tasks like design, creating a contrast with the UI-centric methods typically used by humans. (2) Agents produce work of inferior quality, yet often mask their deficiencies via data fabrication and misuse of advanced tools. (3) Nonetheless, agents deliver results 88.3% faster and cost 90.4-96.2% less than humans, highlighting the potential for enabling efficient collaboration by delegating easily programmable tasks to agents.",
    "metadata": {
      "authors": [
        "Zora Zhiruo Wang",
        "Yijia Shao",
        "Omar Shaikh",
        "Daniel Fried",
        "Graham Neubig",
        "Diyi Yang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22780v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_70",
    "source": "arxiv_metadata",
    "title": "Scalable Supervising Software Agents with Patch Reasoner",
    "text": "Scalable Supervising Software Agents with Patch Reasoner While large language model agents have advanced software engineering tasks, the unscalable nature of existing test-based supervision is limiting the potential improvement of data scaling. The reason is twofold: (1) building and running test sandbox is rather heavy and fragile, and (2) data with high-coverage tests is naturally rare and threatened by test hacking via edge cases. In this paper, we propose R4P, a patch verifier model to provide scalable rewards for training and testing SWE agents via reasoning. We consider that patch verification is fundamentally a reasoning task, mirroring how human repository maintainers review patches without writing and running new reproduction tests. To obtain sufficient reference and reduce the risk of reward hacking, R4P uses a group-wise objective for RL training, enabling it to verify multiple patches against each other's modification and gain a dense reward for stable training. R4P achieves 72.2% Acc. for verifying patches from SWE-bench-verified, surpassing OpenAI o3. To demonstrate R4P's practicality, we design and train a lite scaffold, Mini-SE, with pure reinforcement learning where all rewards are derived from R4P. As a result, Mini-SE achieves 26.2% Pass@1 on SWE-bench-verified, showing a 10.0% improvement over the original Qwen3-32B. This can be further improved to 32.8% with R4P for test-time scaling. Furthermore, R4P verifies patches within a second, 50x faster than testing on average. The stable scaling curves of rewards and accuracy along with high efficiency reflect R4P's practicality.",
    "metadata": {
      "authors": [
        "Junjielong Xu",
        "Boyin Tan",
        "Xiaoyuan Liu",
        "Chao Peng",
        "Pengfei Gao",
        "Pinjia He"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22775v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_71",
    "source": "arxiv_metadata",
    "title": "MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion",
    "text": "MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion As Large Vision-Language Models (LVLMs) are increasingly deployed in domains such as shopping, health, and news, they are exposed to pervasive persuasive content. A critical question is how these models function as persuadees-how and why they can be influenced by persuasive multimodal inputs. Understanding both their susceptibility to persuasion and the effectiveness of different persuasive strategies is crucial, as overly persuadable models may adopt misleading beliefs, override user preferences, or generate unethical or unsafe outputs when exposed to manipulative messages. We introduce MMPersuade, a unified framework for systematically studying multimodal persuasion dynamics in LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs images and videos with established persuasion principles across commercial, subjective and behavioral, and adversarial contexts, and (ii) an evaluation framework that quantifies both persuasion effectiveness and model susceptibility via third-party agreement scoring and self-estimated token probabilities on conversation histories. Our study of six leading LVLMs as persuadees yields three key insights: (i) multimodal inputs substantially increase persuasion effectiveness-and model susceptibility-compared to text alone, especially in misinformation scenarios; (ii) stated prior preferences decrease susceptibility, yet multimodal information maintains its persuasive advantage; and (iii) different strategies vary in effectiveness across contexts, with reciprocity being most potent in commercial and subjective contexts, and credibility and logic prevailing in adversarial contexts. By jointly analyzing persuasion effectiveness and susceptibility, MMPersuade provides a principled foundation for developing models that are robust, preference-consistent, and ethically aligned when engaging with persuasive multimodal content.",
    "metadata": {
      "authors": [
        "Haoyi Qiu",
        "Yilun Zhou",
        "Pranav Narayanan Venkit",
        "Kung-Hsiang Huang",
        "Jiaxin Zhang",
        "Nanyun Peng",
        "Chien-Sheng Wu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22768v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_72",
    "source": "arxiv_metadata",
    "title": "TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination",
    "text": "TELL-TALE: Task Efficient LLMs with Task Aware Layer Elimination In this paper we introduce Tale, Task-Aware Layer Elimination, an inference-time algorithm that prunes entire transformer layers in an LLM by directly optimizing task-specific validation performance. We evaluate TALE on 9 tasks and 5 models, including LLaMA 3.1 8B, Qwen 2.5 7B, Qwen 2.5 0.5B, Mistral 7B, and Lucie 7B, under both zero-shot and few-shot settings. Unlike prior approaches, TALE requires no retraining and consistently improves accuracy while reducing computational cost across all benchmarks. Furthermore, applying TALE during finetuning leads to additional performance gains. Finally, TALE provides flexible user control over trade-offs between accuracy and efficiency. Mutual information analysis shows that certain layers act as bottlenecks, degrading task-relevant representations. Tale's selective layer removal remedies this problem, producing smaller, faster, and more accurate models that are also faster to fine-tune while offering new insights into transformer interpretability.",
    "metadata": {
      "authors": [
        "Omar Naim",
        "Krish Sharma",
        "Nicholas Asher"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22767v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_73",
    "source": "arxiv_metadata",
    "title": "Iterative Layer Pruning for Efficient Translation Inference",
    "text": "Iterative Layer Pruning for Efficient Translation Inference Large language models (LLMs) have transformed many areas of natural language processing, including machine translation. However, efficient deployment of LLMs remains challenging due to their intensive computational requirements. In this paper, we address this challenge and present our submissions to the Model Compression track at the Conference on Machine Translation (WMT 2025). In our experiments, we investigate iterative layer pruning guided by layer importance analysis. We evaluate this method using the Aya-Expanse-8B model for translation from Czech to German, and from English to Egyptian Arabic. Our approach achieves substantial reductions in model size and inference time, while maintaining the translation quality of the baseline models.",
    "metadata": {
      "authors": [
        "Yasmin Moslem",
        "Muhammad Hazim Al Farouq",
        "John D. Kelleher"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22763v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_74",
    "source": "arxiv_metadata",
    "title": "EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models",
    "text": "EchoMind: An Interrelated Multi-level Benchmark for Evaluating Empathetic Speech Language Models Speech Language Models (SLMs) have made significant progress in spoken language understanding. Yet it remains unclear whether they can fully perceive non lexical vocal cues alongside spoken words, and respond with empathy that aligns with both emotional and contextual factors. Existing benchmarks typically evaluate linguistic, acoustic, reasoning, or dialogue abilities in isolation, overlooking the integration of these skills that is crucial for human-like, emotionally intelligent conversation. We present EchoMind, the first interrelated, multi-level benchmark that simulates the cognitive process of empathetic dialogue through sequential, context-linked tasks: spoken-content understanding, vocal-cue perception, integrated reasoning, and response generation. All tasks share identical and semantically neutral scripts that are free of explicit emotional or contextual cues, and controlled variations in vocal style are used to test the effect of delivery independent of the transcript. EchoMind is grounded in an empathy-oriented framework spanning 3 coarse and 12 fine-grained dimensions, encompassing 39 vocal attributes, and evaluated using both objective and subjective metrics. Testing 12 advanced SLMs reveals that even state-of-the-art models struggle with high-expressive vocal cues, limiting empathetic response quality. Analyses of prompt strength, speech source, and ideal vocal cue recognition reveal persistent weaknesses in instruction-following, resilience to natural speech variability, and effective use of vocal cues for empathy. These results underscore the need for SLMs that integrate linguistic content with diverse vocal cues to achieve truly empathetic conversational ability.",
    "metadata": {
      "authors": [
        "Li Zhou",
        "Lutong Yu",
        "You Lyu",
        "Yihang Lin",
        "Zefeng Zhao",
        "Junyi Ao",
        "Yuhao Zhang",
        "Benyou Wang",
        "Haizhou Li"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22758v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_75",
    "source": "arxiv_metadata",
    "title": "Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models",
    "text": "Beyond Semantics: How Temporal Biases Shape Retrieval in Transformer and State-Space Models In-context learning is governed by both temporal and semantic relationships, shaping how Large Language Models (LLMs) retrieve contextual information. Analogous to human episodic memory, where the retrieval of specific events is enabled by separating events that happened at different times, this work probes the ability of various pretrained LLMs, including transformer and state-space models, to differentiate and retrieve temporally separated events. Specifically, we prompted models with sequences containing multiple presentations of the same token, which reappears at the sequence end. By fixing the positions of these repeated tokens and permuting all others, we removed semantic confounds and isolated temporal effects on next-token prediction. Across diverse sequences, models consistently placed the highest probabilities on tokens following a repeated token, but with a notable bias for those nearest the beginning or end of the input. An ablation experiment linked this phenomenon in transformers to induction heads. Extending the analysis to unique semantic contexts with partial overlap further demonstrated that memories embedded in the middle of a prompt are retrieved less reliably. Despite architectural differences, state-space and transformer models showed comparable temporal biases. Our findings deepen the understanding of temporal biases in in-context learning and offer an illustration of how these biases can enable temporal separation and episodic retrieval.",
    "metadata": {
      "authors": [
        "Anooshka Bajaj",
        "Deven Mahesh Mistry",
        "Sahaj Singh Maini",
        "Yash Aggarwal",
        "Zoran Tiganj"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22752v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_76",
    "source": "arxiv_metadata",
    "title": "Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models",
    "text": "Multi-Modal Fact-Verification Framework for Reducing Hallucinations in Large Language Models While Large Language Models have transformed how we interact with AI systems, they suffer from a critical flaw: they confidently generate false information that sounds entirely plausible. This hallucination problem has become a major barrier to deploying these models in real-world applications where accuracy matters. We developed a fact verification framework that catches and corrects these errors in real-time by cross checking LLM outputs against multiple knowledge sources. Our system combines structured databases, live web searches, and academic literature to verify factual claims as they're generated. When we detect inconsistencies, we automatically correct them while preserving the natural flow of the response. Testing across various domains showed we could reduce hallucinations by 67% without sacrificing response quality. Domain experts in healthcare, finance, and scientific research rated our corrected outputs 89% satisfactory a significant improvement over unverified LLM responses. This work offers a practical solution for making LLMs more trustworthy in applications where getting facts wrong isn't an option.",
    "metadata": {
      "authors": [
        "Piyushkumar Patel"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22751v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_77",
    "source": "arxiv_metadata",
    "title": "Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study",
    "text": "Low-Resource Dialect Adaptation of Large Language Models: A French Dialect Case-Study Despite the widespread adoption of large language models (LLMs), their strongest capabilities remain largely confined to a small number of high-resource languages for which there is abundant training data. Recently, continual pre-training (CPT) has emerged as a means to fine-tune these models to low-resource regional dialects. In this paper, we study the use of CPT for dialect learning under tight data and compute budgets. Using low-rank adaptation (LoRA) and compute-efficient continual pre-training, we adapt three LLMs to the Qu\\'ebec French dialect using a very small dataset and benchmark them on the COLE suite. Our experiments demonstrate an improvement on the minority dialect benchmarks with minimal regression on the prestige language benchmarks with under 1% of model parameters updated. Analysis of the results demonstrate that gains are highly contingent on corpus composition. These findings indicate that CPT with parameter-efficient fine-tuning (PEFT) can narrow the dialect gap by providing cost-effective and sustainable language resource creation, expanding high-quality LLM access to minority linguistic communities. We release the first Qu\\'ebec French LLMs on HuggingFace.",
    "metadata": {
      "authors": [
        "Eeham Khan",
        "Firas Saidani",
        "Owen Van Esbroeck",
        "Richard Khoury",
        "Leila Kosseim"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22747v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_78",
    "source": "arxiv_metadata",
    "title": "REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization",
    "text": "REVISION:Reflective Intent Mining and Online Reasoning Auxiliary for E-commerce Visual Search System Optimization In Taobao e-commerce visual search, user behavior analysis reveals a large proportion of no-click requests, suggesting diverse and implicit user intents. These intents are expressed in various forms and are difficult to mine and discover, thereby leading to the limited adaptability and lag in platform strategies. This greatly restricts users' ability to express diverse intents and hinders the scalability of the visual search system. This mismatch between user implicit intent expression and system response defines the User-SearchSys Intent Discrepancy. To alleviate the issue, we propose a novel framework REVISION. This framework integrates offline reasoning mining with online decision-making and execution, enabling adaptive strategies to solve implicit user demands. In the offline stage, we construct a periodic pipeline to mine discrepancies from historical no-click requests. Leveraging large models, we analyze implicit intent factors and infer optimal suggestions by jointly reasoning over query and product metadata. These inferred suggestions serve as actionable insights for refining platform strategies. In the online stage, REVISION-R1-3B, trained on the curated offline data, performs holistic analysis over query images and associated historical products to generate optimization plans and adaptively schedule strategies across the search pipeline. Our framework offers a streamlined paradigm for integrating large models with traditional search systems, enabling end-to-end intelligent optimization across information aggregation and user interaction. Experimental results demonstrate that our approach improves the efficiency of implicit intent mining from large-scale search logs and significantly reduces the no-click rate.",
    "metadata": {
      "authors": [
        "Yiwen Tang",
        "Qiuyu Zhao",
        "Zenghui Sun",
        "Jinsong Lan",
        "Xiaoyong Zhu",
        "Bo Zheng",
        "Kaifu Zhang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22739v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_79",
    "source": "arxiv_metadata",
    "title": "$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker",
    "text": "$\\text{E}^2\\text{Rank}$: Your Text Embedding can Also be an Effective and Efficient Listwise Reranker Text embedding models serve as a fundamental component in real-world search applications. By mapping queries and documents into a shared embedding space, they deliver competitive retrieval performance with high efficiency. However, their ranking fidelity remains limited compared to dedicated rerankers, especially recent LLM-based listwise rerankers, which capture fine-grained query-document and document-document interactions. In this paper, we propose a simple yet effective unified framework $\\text{E}^2\\text{Rank}$, means Efficient Embedding-based Ranking (also means Embedding-to-Rank), which extends a single text embedding model to perform both high-quality retrieval and listwise reranking through continued training under a listwise ranking objective, thereby achieving strong effectiveness with remarkable efficiency. By applying cosine similarity between the query and document embeddings as a unified ranking function, the listwise ranking prompt, which is constructed from the original query and its candidate documents, serves as an enhanced query enriched with signals from the top-K documents, akin to pseudo-relevance feedback (PRF) in traditional retrieval models. This design preserves the efficiency and representational quality of the base embedding model while significantly improving its reranking performance. Empirically, $\\textrm{E}^2\\text{Rank}$ achieves state-of-the-art results on the BEIR reranking benchmark and demonstrates competitive performance on the reasoning-intensive BRIGHT benchmark, with very low reranking latency. We also show that the ranking training process improves embedding performance on the MTEB benchmark. Our findings indicate that a single embedding model can effectively unify retrieval and reranking, offering both computational efficiency and competitive ranking accuracy.",
    "metadata": {
      "authors": [
        "Qi Liu",
        "Yanzhao Zhang",
        "Mingxin Li",
        "Dingkun Long",
        "Pengjun Xie",
        "Jiaxin Mao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22733v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_80",
    "source": "arxiv_metadata",
    "title": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation",
    "text": "ATLAS: Actor-Critic Task-Completion with Look-ahead Action Simulation We observe that current state-of-the-art web-agents are unable to effectively adapt to new environments without neural network fine-tuning, without which they produce inefficient execution plans due to a lack of awareness of the structure and dynamics of the new environment. To address this limitation, we introduce ATLAS (Actor-Critic Task-completion with Look-ahead Action Simulation), a memory-augmented agent that is able to make plans grounded in a model of the environment by simulating the consequences of those actions in cognitive space. Our agent starts by building a \"cognitive map\" by performing a lightweight curiosity driven exploration of the environment. The planner proposes candidate actions; the simulator predicts their consequences in cognitive space; a critic analyzes the options to select the best roll-out and update the original plan; and a browser executor performs the chosen action. On the WebArena-Lite Benchmark, we achieve a 63% success rate compared to 53.9% success rate for the previously published state-of-the-art. Unlike previous systems, our modular architecture requires no website-specific LLM fine-tuning. Ablations show sizable drops without the world-model, hierarchical planner, and look-ahead-based replanner confirming their complementary roles within the design of our system",
    "metadata": {
      "authors": [
        "Jiali Cheng",
        "Anjishnu Kumar",
        "Roshan Lal",
        "Rishi Rajasekaran",
        "Hani Ramezani",
        "Omar Zia Khan",
        "Oleg Rokhlenko",
        "Sunny Chiu-Webster",
        "Gang Hua",
        "Hadi Amiri"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22732v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_81",
    "source": "arxiv_metadata",
    "title": "Critical Insights into Leading Conversational AI Models",
    "text": "Critical Insights into Leading Conversational AI Models Big Language Models (LLMs) are changing the way businesses use software, the way people live their lives and the way industries work. Companies like Google, High-Flyer, Anthropic, OpenAI and Meta are making better LLMs. So, it's crucial to look at how each model is different in terms of performance, moral behaviour and usability, as these differences are based on the different ideas that built them. This study compares five top LLMs: Google's Gemini, High-Flyer's DeepSeek, Anthropic's Claude, OpenAI's GPT models and Meta's LLaMA. It performs this by analysing three important factors: Performance and Accuracy, Ethics and Bias Mitigation and Usability and Integration. It was found that Claude has good moral reasoning, Gemini is better at multimodal capabilities and has strong ethical frameworks. DeepSeek is great at reasoning based on facts, LLaMA is good for open applications and ChatGPT delivers balanced performance with a focus on usage. It was concluded that these models are different in terms of how well they work, how easy they are to use and how they treat people ethically, making it a point that each model should be utilised by the user in a way that makes the most of its strengths.",
    "metadata": {
      "authors": [
        "Urja Kohli",
        "Aditi Singh",
        "Arun Sharma"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22729v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_82",
    "source": "arxiv_metadata",
    "title": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation",
    "text": "Windsock is Dancing: Adaptive Multimodal Retrieval-Augmented Generation Multimodal Retrieval-Augmented Generation (MRAG) has emerged as a promising method to generate factual and up-to-date responses of Multimodal Large Language Models (MLLMs) by incorporating non-parametric knowledge from external knowledge bases. However, existing MRAG approaches suffer from static retrieval strategies, inflexible modality selection, and suboptimal utilization of retrieved information, leading to three critical challenges: determining when to retrieve, what modality to incorporate, and how to utilize retrieved information effectively. To address these challenges, we introduce Windsock, a query-dependent module making decisions on retrieval necessity and modality selection, effectively reducing computational overhead and improving response quality. Additionally, we propose Dynamic Noise-Resistance (DANCE) Instruction Tuning, an adaptive training strategy that enhances MLLMs' ability to utilize retrieved information while maintaining robustness against noise. Moreover, we adopt a self-assessment approach leveraging knowledge within MLLMs to convert question-answering datasets to MRAG training datasets. Extensive experiments demonstrate that our proposed method significantly improves the generation quality by 17.07% while reducing 8.95% retrieval times.",
    "metadata": {
      "authors": [
        "Shu Zhao",
        "Tianyi Shen",
        "Nilesh Ahuja",
        "Omesh Tickoo",
        "Vijaykrishnan Narayanan"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22694v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_83",
    "source": "arxiv_metadata",
    "title": "SALSA: Single-pass Autoregressive LLM Structured Classification",
    "text": "SALSA: Single-pass Autoregressive LLM Structured Classification Despite their impressive generalization capabilities, instruction-tuned Large Language Models often underperform on text classification benchmarks. We introduce SALSA, a coherent pipeline that combines structured prompting, class-to-token mapping, and parameter-efficient fine-tuning, thereby avoiding cold-start training. Each class label is mapped to a distinct output token, and prompts are constructed to elicit a single-token response. During inference, the model's output is projected only onto the logits of the relevant class tokens, enabling efficient and accurate classification in a single forward pass. SALSA achieves state-of-the-art results across diverse benchmarks, demonstrating its robustness and scalability for LLM-based classification applications.",
    "metadata": {
      "authors": [
        "Ruslan Berdichevsky",
        "Shai Nahum-Gefen",
        "Elad Ben Zaken"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22691v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_84",
    "source": "arxiv_metadata",
    "title": "Rule-Based Explanations for Retrieval-Augmented LLM Systems",
    "text": "Rule-Based Explanations for Retrieval-Augmented LLM Systems If-then rules are widely used to explain machine learning models; e.g., \"if employed = no, then loan application = rejected.\" We present the first proposal to apply rules to explain the emerging class of large language models (LLMs) with retrieval-augmented generation (RAG). Since RAG enables LLM systems to incorporate retrieved information sources at inference time, rules linking the presence or absence of sources can explain output provenance; e.g., \"if a Times Higher Education ranking article is retrieved, then the LLM ranks Oxford first.\" To generate such rules, a brute force approach would probe the LLM with all source combinations and check if the presence or absence of any sources leads to the same output. We propose optimizations to speed up rule generation, inspired by Apriori-like pruning from frequent itemset mining but redefined within the scope of our novel problem. We conclude with qualitative and quantitative experiments demonstrating our solutions' value and efficiency.",
    "metadata": {
      "authors": [
        "Joel Rorseth",
        "Parke Godfrey",
        "Lukasz Golab",
        "Divesh Srivastava",
        "Jarek Szlichta"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22689v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_85",
    "source": "arxiv_metadata",
    "title": "RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance",
    "text": "RoboSVG: A Unified Framework for Interactive SVG Generation with Multi-modal Guidance Scalable Vector Graphics (SVGs) are fundamental to digital design and robot control, encoding not only visual structure but also motion paths in interactive drawings. In this work, we introduce RoboSVG, a unified multimodal framework for generating interactive SVGs guided by textual, visual, and numerical signals. Given an input query, the RoboSVG model first produces multimodal guidance, then synthesizes candidate SVGs through dedicated generation modules, and finally refines them under numerical guidance to yield high-quality outputs. To support this framework, we construct RoboDraw, a large-scale dataset of one million examples, each pairing an SVG generation condition (e.g., text, image, and partial SVG) with its corresponding ground-truth SVG code. RoboDraw dataset enables systematic study of four tasks, including basic generation (Text-to-SVG, Image-to-SVG) and interactive generation (PartialSVG-to-SVG, PartialImage-to-SVG). Extensive experiments demonstrate that RoboSVG achieves superior query compliance and visual fidelity across tasks, establishing a new state of the art in versatile SVG generation. The dataset and source code of this project will be publicly available soon.",
    "metadata": {
      "authors": [
        "Jiuniu Wang",
        "Gongjie Zhang",
        "Quanhao Qian",
        "Junlong Gao",
        "Deli Zhao",
        "Ran Xu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22684v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_86",
    "source": "arxiv_metadata",
    "title": "Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration",
    "text": "Do Stop Me Now: Detecting Boilerplate Responses with a Single Iteration Large Language Models (LLMs) often expend significant computational resources generating boilerplate responses, such as refusals, simple acknowledgements and casual greetings, which adds unnecessary cost and latency. To address this inefficiency, we propose a simple yet highly effective method for detecting such responses after only a single generation step. We demonstrate that the log-probability distribution of the first generated token serves as a powerful signal for classifying the nature of the entire subsequent response. Our experiments, conducted across a diverse range of small, large, and reasoning-specialized models, show that the first-token log-probability vectors form distinctly separable clusters for different response types. Using a lightweight k-NN classifier, we achieve high accuracy in predicting whether a response will be a substantive answer or a form of boilerplate response, including user-specified refusals. The primary implication is a practical, computationally trivial technique, optimizing LLM inference by enabling early termination or redirection to a smaller model, thereby yielding significant savings in computational cost. This work presents a direct path toward more efficient and sustainable LLM deployment.",
    "metadata": {
      "authors": [
        "Yuval Kainan",
        "Shaked Zychlinski"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22679v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_87",
    "source": "arxiv_metadata",
    "title": "Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views",
    "text": "Look and Tell: A Dataset for Multimodal Grounding Across Egocentric and Exocentric Views We introduce Look and Tell, a multimodal dataset for studying referential communication across egocentric and exocentric perspectives. Using Meta Project Aria smart glasses and stationary cameras, we recorded synchronized gaze, speech, and video as 25 participants instructed a partner to identify ingredients in a kitchen. Combined with 3D scene reconstructions, this setup provides a benchmark for evaluating how different spatial representations (2D vs. 3D; ego vs. exo) affect multimodal grounding. The dataset contains 3.67 hours of recordings, including 2,707 richly annotated referential expressions, and is designed to advance the development of embodied agents that can understand and engage in situated dialogue.",
    "metadata": {
      "authors": [
        "Anna Deichler",
        "Jonas Beskow"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22672v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_88",
    "source": "arxiv_metadata",
    "title": "Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion",
    "text": "Conjugate Relation Modeling for Few-Shot Knowledge Graph Completion Few-shot Knowledge Graph Completion (FKGC) infers missing triples from limited support samples, tackling long-tail distribution challenges. Existing methods, however, struggle to capture complex relational patterns and mitigate data sparsity. To address these challenges, we propose a novel FKGC framework for conjugate relation modeling (CR-FKGC). Specifically, it employs a neighborhood aggregation encoder to integrate higher-order neighbor information, a conjugate relation learner combining an implicit conditional diffusion relation module with a stable relation module to capture stable semantics and uncertainty offsets, and a manifold conjugate decoder for efficient evaluation and inference of missing triples in manifold space. Experiments on three benchmarks demonstrate that our method achieves superior performance over state-of-the-art methods.",
    "metadata": {
      "authors": [
        "Zilong Wang",
        "Qingtian Zeng",
        "Hua Duan",
        "Cheng Cheng",
        "Minghao Zou",
        "Ziyang Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22656v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_89",
    "source": "arxiv_metadata",
    "title": "Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task",
    "text": "Culturally Grounded Physical Commonsense Reasoning in Italian and English: A Submission to the MRL 2025 Shared Task This paper presents our submission to the MRL 2025 Shared Task on Multilingual Physical Reasoning Datasets. The objective of the shared task is to create manually-annotated evaluation data in the physical commonsense reasoning domain, for languages other than English, following a format similar to PIQA. Our contribution, FormaMentis, is a novel benchmark for physical commonsense reasoning that is grounded in Italian language and culture. The data samples in FormaMentis are created by expert annotators who are native Italian speakers and are familiar with local customs and norms. The samples are additionally translated into English, while preserving the cultural elements unique to the Italian context.",
    "metadata": {
      "authors": [
        "Marco De Santis",
        "Lisa Alazraki"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22631v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_90",
    "source": "arxiv_metadata",
    "title": "Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal",
    "text": "Integrating Linguistics and AI: Morphological Analysis and Corpus development of Endangered Toto Language of West Bengal Preserving linguistic diversity is necessary as every language offers a distinct perspective on the world. There have been numerous global initiatives to preserve endangered languages through documentation. This paper is a part of a project which aims to develop a trilingual (Toto-Bangla-English) language learning application to digitally archive and promote the endangered Toto language of West Bengal, India. This application, designed for both native Toto speakers and non-native learners, aims to revitalize the language by ensuring accessibility and usability through Unicode script integration and a structured language corpus. The research includes detailed linguistic documentation collected via fieldwork, followed by the creation of a morpheme-tagged, trilingual corpus used to train a Small Language Model (SLM) and a Transformer-based translation engine. The analysis covers inflectional morphology such as person-number-gender agreement, tense-aspect-mood distinctions, and case marking, alongside derivational strategies that reflect word-class changes. Script standardization and digital literacy tools were also developed to enhance script usage. The study offers a sustainable model for preserving endangered languages by incorporating traditional linguistic methodology with AI. This bridge between linguistic research with technological innovation highlights the value of interdisciplinary collaboration for community-based language revitalization.",
    "metadata": {
      "authors": [
        "Ambalika Guha",
        "Sajal Saha",
        "Debanjan Ballav",
        "Soumi Mitra",
        "Hritwick Chakraborty"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22629v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_91",
    "source": "arxiv_metadata",
    "title": "PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion",
    "text": "PerCoR: Evaluating Commonsense Reasoning in Persian via Multiple-Choice Sentence Completion We introduced PerCoR (Persian Commonsense Reasoning), the first large-scale Persian benchmark for commonsense reasoning. PerCoR contains 106K multiple-choice sentence-completion problems drawn from more than forty news, cultural, and other web sources. We introduce a novel conjunction-based segmentation strategy to generate coherent sentence-completion pairs, enabling broad topical and structural diversity. To create challenging distractors, we propose DRESS-AF (Distractor Ranking via Embedding Similarity Scoring and Adversarial Filtering), a generation-free adversarial filtering method that selects distractors from the pool of gold continuations while maximising model confusion. Human annotators score 89% on PerCoR, while OpenAI-o3 achieves the highest performance at 92.18%, followed closely by Claude-Sonnet-3.7 (91.17%). The strongest open-source model, DeepSeek-R1, reaches 82.51%, underscoring both the dataset's difficulty and the remaining performance gap in Persian commonsense reasoning. We further show that DRESS-AF transfers to the English HellaSwag benchmark, increasing its difficulty without hurting human solvability. The dataset is available at https://huggingface.co/datasets/MCINext/PerCoR.",
    "metadata": {
      "authors": [
        "Morteza Alikhani",
        "Mohammadtaha Bagherifard",
        "Erfan Zinvandi",
        "Mehran Sarmadi"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22616v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_92",
    "source": "arxiv_metadata",
    "title": "Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance",
    "text": "Personal Care Utility (PCU): Building the Health Infrastructure for Everyday Insight and Guidance Building on decades of success in digital infrastructure and biomedical innovation, we propose the Personal Care Utility (PCU) - a cybernetic system for lifelong health guidance. PCU is conceived as a global, AI-powered utility that continuously orchestrates multimodal data, knowledge, and services to assist individuals and populations alike. Drawing on multimodal agents, event-centric modeling, and contextual inference, it offers three essential capabilities: (1) trusted health information tailored to the individual, (2) proactive health navigation and behavior guidance, and (3) ongoing interpretation of recovery and treatment response after medical events. Unlike conventional episodic care, PCU functions as an ambient, adaptive companion - observing, interpreting, and guiding health in real time across daily life. By integrating personal sensing, experiential computing, and population-level analytics, PCU promises not only improved outcomes for individuals but also a new substrate for public health and scientific discovery. We describe the architecture, design principles, and implementation challenges of this emerging paradigm.",
    "metadata": {
      "authors": [
        "Mahyar Abbasian",
        "Ramesh Jain"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22602v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_93",
    "source": "arxiv_metadata",
    "title": "AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment",
    "text": "AutoBench: Automating LLM Evaluation through Reciprocal Peer Assessment We present AutoBench, a fully automated and self-sustaining framework for evaluating Large Language Models (LLMs) through reciprocal peer assessment. This paper provides a rigorous scientific validation of the AutoBench methodology, originally developed as an open-source project by eZecute S.R.L.. Unlike static benchmarks that suffer from test-set contamination and limited adaptability, AutoBench dynamically generates novel evaluation tasks while models alternately serve as question generators, contestants, and judges across diverse domains. An iterative weighting mechanism amplifies the influence of consistently reliable evaluators, aggregating peer judgments into consensus-based rankings that reflect collective model agreement. Our experiments demonstrate strong correlations with established benchmarks including MMLU-Pro and GPQA (respectively 78\\% and 63\\%), validating this peer-driven evaluation paradigm. The multi-judge design significantly outperforms single-judge baselines, confirming that distributed evaluation produces more robust and human-consistent assessments. AutoBench offers a scalable, contamination-resistant alternative to static benchmarks for the continuous evaluation of evolving language models.",
    "metadata": {
      "authors": [
        "Dario Loi",
        "Elena Maria Muià",
        "Federico Siciliano",
        "Giovanni Trappolini",
        "Vincenzo Crisà",
        "Peter Kruger",
        "Fabrizio Silvestri"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22593v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_94",
    "source": "arxiv_metadata",
    "title": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs",
    "text": "ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs In today's rapidly expanding data landscape, knowledge extraction from unstructured text is vital for real-time analytics, temporal inference, and dynamic memory frameworks. However, traditional static knowledge graph (KG) construction often overlooks the dynamic and time-sensitive nature of real-world data, limiting adaptability to continuous changes. Moreover, recent zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance on prebuilt ontologies often suffer from instability across multiple runs, as well as incomplete coverage of key facts. To address these challenges, we introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that builds and continuously updates Temporal Knowledge Graphs (TKGs) from unstructured texts. ATOM splits input documents into minimal, self-contained \"atomic\" facts, improving extraction exhaustivity and stability. Then, it constructs atomic TKGs from these facts while employing a dual-time modeling that distinguishes when information is observed from when it is valid. The resulting atomic TKGs are subsequently merged in parallel. Empirical evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17% better stability, and over 90% latency reduction compared to baseline methods, demonstrating a strong scalability potential for dynamic TKG construction.",
    "metadata": {
      "authors": [
        "Yassir Lairgi",
        "Ludovic Moncla",
        "Khalid Benabdeslem",
        "Rémy Cazabet",
        "Pierre Cléau"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22590v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_95",
    "source": "arxiv_metadata",
    "title": "UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models",
    "text": "UltraVoice: Scaling Fine-Grained Style-Controlled Speech Conversations for Spoken Dialogue Models Spoken dialogue models currently lack the ability for fine-grained speech style control, a critical capability for human-like interaction that is often overlooked in favor of purely functional capabilities like reasoning and question answering. To address this limitation, we introduce UltraVoice, the first large-scale speech dialogue dataset engineered for multiple fine-grained speech style control. Encompassing over 830 hours of speech dialogues, UltraVoice provides instructions across six key speech stylistic dimensions: emotion, speed, volume, accent, language, and composite styles. Fine-tuning leading models such as SLAM-Omni and VocalNet on UltraVoice significantly enhances their fine-grained speech stylistic controllability without degrading core conversational abilities. Specifically, our fine-tuned models achieve improvements of 29.12-42.33% in Mean Opinion Score (MOS) and 14.61-40.09 percentage points in Instruction Following Rate (IFR) on multi-dimensional control tasks designed in the UltraVoice. Moreover, on the URO-Bench benchmark, our fine-tuned models demonstrate substantial gains in core understanding, reasoning, and conversational abilities, with average improvements of +10.84% on the Basic setting and +7.87% on the Pro setting. Furthermore, the dataset's utility extends to training controllable Text-to-Speech (TTS) models, underscoring its high quality and broad applicability for expressive speech synthesis. The complete dataset and model checkpoints are available at: https://github.com/bigai-nlco/UltraVoice.",
    "metadata": {
      "authors": [
        "Wenming Tu",
        "Guanrou Yang",
        "Ruiqi Yan",
        "Wenxi Chen",
        "Ziyang Ma",
        "Yipeng Kang",
        "Kai Yu",
        "Xie Chen",
        "Zilong Zheng"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22588v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_96",
    "source": "arxiv_metadata",
    "title": "Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems",
    "text": "Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems The interdisciplinary research domain of Artificial Intelligence in Education (AIED) has a long history of developing Intelligent Tutoring Systems (ITSs) by integrating insights from technological advancements, educational theories, and cognitive psychology. The remarkable success of generative AI (GenAI) models has accelerated the development of large language model (LLM)-powered ITSs, which have potential to imitate human-like, pedagogically rich, and cognitively demanding tutoring. However, the progress and impact of these systems remain largely untraceable due to the absence of reliable, universally accepted, and pedagogy-driven evaluation frameworks and benchmarks. Most existing educational dialogue-based ITS evaluations rely on subjective protocols and non-standardized benchmarks, leading to inconsistencies and limited generalizability. In this work, we take a step back from mainstream ITS development and provide comprehensive state-of-the-art evaluation practices, highlighting associated challenges through real-world case studies from careful and caring AIED research. Finally, building on insights from previous interdisciplinary AIED research, we propose three practical, feasible, and theoretically grounded research directions, rooted in learning science principles and aimed at establishing fair, unified, and scalable evaluation methodologies for ITSs.",
    "metadata": {
      "authors": [
        "Kaushal Kumar Maurya",
        "Ekaterina Kochmar"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22581v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_97",
    "source": "arxiv_metadata",
    "title": "A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback",
    "text": "A Closed-Loop Personalized Learning Agent Integrating Neural Cognitive Diagnosis, Bounded-Ability Adaptive Testing, and LLM-Driven Feedback As information technology advances, education is moving from one-size-fits-all instruction toward personalized learning. However, most methods handle modeling, item selection, and feedback in isolation rather than as a closed loop. This leads to coarse or opaque student models, assumption-bound adaptivity that ignores diagnostic posteriors, and generic, non-actionable feedback. To address these limitations, this paper presents an end-to-end personalized learning agent, EduLoop-Agent, which integrates a Neural Cognitive Diagnosis model (NCD), a Bounded-Ability Estimation Computerized Adaptive Testing strategy (BECAT), and large language models (LLMs). The NCD module provides fine-grained estimates of students' mastery at the knowledge-point level; BECAT dynamically selects subsequent items to maximize relevance and learning efficiency; and LLMs convert diagnostic signals into structured, actionable feedback. Together, these components form a closed-loop framework of ``Diagnosis--Recommendation--Feedback.'' Experiments on the ASSISTments dataset show that the NCD module achieves strong performance on response prediction while yielding interpretable mastery assessments. The adaptive recommendation strategy improves item relevance and personalization, and the LLM-based feedback offers targeted study guidance aligned with identified weaknesses. Overall, the results indicate that the proposed design is effective and practically deployable, providing a feasible pathway to generating individualized learning trajectories in intelligent education.",
    "metadata": {
      "authors": [
        "Zhifeng Wang",
        "Xinyue Zheng",
        "Chunyan Zeng"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22559v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_98",
    "source": "arxiv_metadata",
    "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size",
    "text": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression Block Size The growing memory footprint of the Key-Value (KV) cache poses a severe scalability bottleneck for long-context Large Language Model (LLM) inference. While KV cache eviction has emerged as an effective solution by discarding less critical tokens, existing token-, block-, and sentence-level compression methods struggle to balance semantic coherence and memory efficiency. To this end, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction framework with \\underline{a}daptive \\underline{block} sizes. Specifically, SABlock first performs semantic segmentation to align compression boundaries with linguistic structures, then applies segment-guided token scoring to refine token importance estimation. Finally, for each segment, a budget-driven search strategy adaptively determines the optimal block size that preserves semantic integrity while improving compression efficiency under a given cache budget. Extensive experiments on long-context benchmarks demonstrate that SABlock consistently outperforms state-of-the-art baselines under the same memory budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9% retrieval accuracy with only 96 KV entries, nearly matching the performance of the full-cache baseline that retains up to 8K entries. Under a fixed cache budget of 1,024, SABlock further reduces peak memory usage by 46.28% and achieves up to 9.5x faster decoding on a 128K context length.",
    "metadata": {
      "authors": [
        "Jinhan Chen",
        "Jianchun Liu",
        "Hongli Xu",
        "Xianjun Gao",
        "Shilong Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22556v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_99",
    "source": "arxiv_metadata",
    "title": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges?",
    "text": "LooGLE v2: Are LLMs Ready for Real World Long Dependency Challenges? Large language models (LLMs) are equipped with increasingly extended context windows recently, yet their long context understanding capabilities over long dependency tasks remain fundamentally limited and underexplored. This gap is especially significant in many real-world long-context applications that were rarely benchmarked. In this paper, we introduce LooGLE v2, a novel benchmark designed to evaluate LLMs' long context ability in real-world applications and scenarios. Our benchmark consists of automatically collected real-world long texts, ranging from 16k to 2M tokens, encompassing domains in law, finance, game and code. Accordingly, we delicately design 10 types of domain-specific long-dependency tasks and generate 1,934 QA instances with various diversity and complexity in a scalable data curation pipeline for further practical needs. We conduct a comprehensive assessment of 6 locally deployed and 4 API-based LLMs. The evaluation results show that even the best-performing model achieves only a 59.2% overall score on our benchmark. Despite the extensive context windows, popular LLMs are only capable of understanding a much shorter length of context than they claim to be, revealing significant limitations in their ability to handle real-world tasks with long dependencies and highlighting substantial room for model improvement in practical long-context understanding.",
    "metadata": {
      "authors": [
        "Ziyuan He",
        "Yuxuan Wang",
        "Jiaqi Li",
        "Kexin Liang",
        "Muhan Zhang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22548v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_100",
    "source": "arxiv_metadata",
    "title": "OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models",
    "text": "OFFSIDE: Benchmarking Unlearning Misinformation in Multimodal Large Language Models Advances in Multimodal Large Language Models (MLLMs) intensify concerns about data privacy, making Machine Unlearning (MU), the selective removal of learned information, a critical necessity. However, existing MU benchmarks for MLLMs are limited by a lack of image diversity, potential inaccuracies, and insufficient evaluation scenarios, which fail to capture the complexity of real-world applications. To facilitate the development of MLLMs unlearning and alleviate the aforementioned limitations, we introduce OFFSIDE, a novel benchmark for evaluating misinformation unlearning in MLLMs based on football transfer rumors. This manually curated dataset contains 15.68K records for 80 players, providing a comprehensive framework with four test sets to assess forgetting efficacy, generalization, utility, and robustness. OFFSIDE supports advanced settings like selective unlearning and corrective relearning, and crucially, unimodal unlearning (forgetting only text data). Our extensive evaluation of multiple baselines reveals key findings: (1) Unimodal methods (erasing text-based knowledge) fail on multimodal rumors; (2) Unlearning efficacy is largely driven by catastrophic forgetting; (3) All methods struggle with \"visual rumors\" (rumors appear in the image); (4) The unlearned rumors can be easily recovered and (5) All methods are vulnerable to prompt attacks. These results expose significant vulnerabilities in current approaches, highlighting the need for more robust multimodal unlearning solutions. The code is available at \\href{https://github.com/zh121800/OFFSIDE}{https://github.com/zh121800/OFFSIDE}.",
    "metadata": {
      "authors": [
        "Hao Zheng",
        "Zirui Pang",
        "Ling li",
        "Zhijie Deng",
        "Yuhan Pu",
        "Zhaowei Zhu",
        "Xiaobo Xia",
        "Jiaheng Wei"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22535v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_101",
    "source": "arxiv_metadata",
    "title": "Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection",
    "text": "Text to Trust: Evaluating Fine-Tuning and LoRA Trade-offs in Language Models for Unfair Terms of Service Detection Large Language Models (LLMs) have transformed text understanding, yet their adaptation to specialized legal domains remains constrained by the cost of full fine-tuning. This study provides a systematic evaluation of fine tuning, parameter efficient adaptation (LoRA, QLoRA), and zero-shot prompting strategies for unfair clause detection in Terms of Service (ToS) documents, a key application in legal NLP. We finetune BERT and DistilBERT, apply 4-bit Low-Rank Adaptation (LoRA) to models such as TinyLlama, LLaMA 3B/7B, and SaulLM, and evaluate GPT-4o and O-versions in zero-shot settings. Experiments on the CLAUDETTE-ToS benchmark and the Multilingual Scraper Corpus show that full fine-tuning achieves the strongest precision recall balance, while LoRA-based models provide competitive recall with up to 3x lower memory cost. These findings highlight practical design trade-offs for efficient and domain-adapted LLMs, contributing open baselines for fine-tuning research in legal text processing.",
    "metadata": {
      "authors": [
        "Noshitha Padma Pratyusha Juttu",
        "Sahithi Singireddy",
        "Sravani Gona",
        "Sujal Timilsina"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22531v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_102",
    "source": "arxiv_metadata",
    "title": "Scalable Oversight via Partitioned Human Supervision",
    "text": "Scalable Oversight via Partitioned Human Supervision As artificial intelligence (AI) systems approach and surpass expert human performance across a broad range of tasks, obtaining high-quality human supervision for evaluation and training becomes increasingly challenging. Our focus is on tasks that require deep knowledge and skills of multiple domains. Unfortunately, even the best human experts are knowledgeable only in a single narrow area, and will not be able to evaluate the correctness of advanced AI systems on such superhuman tasks. However, based on their narrow expertise, humans may provide a weak signal, i.e., a complementary label indicating an option that is incorrect. For example, a cardiologist could state that \"this is not related to cardiology,'' even if they cannot identify the true disease. Based on this weak signal, we propose a scalable oversight framework that enables us to evaluate frontier AI systems without the need to prepare the ground truth. We derive an unbiased estimator of top-1 accuracy from complementary labels and quantify how many complementary labels are needed to match the variance of ordinary labels. We further introduce two estimators to combine scarce ordinary labels with abundant complementary labels. We provide finite-sample deviation guarantees for both complementary-only and the mixed estimators. Empirically, we show that we can evaluate the output of large language models without the ground truth, if we have complementary labels. We further show that we can train an AI system with such weak signals: we show how we can design an agentic AI system automatically that can perform better with this partitioned human supervision. Our code is available at https://github.com/R-Yin-217/Scalable-Oversight-via-Human-Partitioned-Supervision.",
    "metadata": {
      "authors": [
        "Ren Yin",
        "Takashi Ishida",
        "Masashi Sugiyama"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22500v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_103",
    "source": "arxiv_metadata",
    "title": "A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus",
    "text": "A Sociophonetic Analysis of Racial Bias in Commercial ASR Systems Using the Pacific Northwest English Corpus This paper presents a systematic evaluation of racial bias in four major commercial automatic speech recognition (ASR) systems using the Pacific Northwest English (PNWE) corpus. We analyze transcription accuracy across speakers from four ethnic backgrounds (African American, Caucasian American, ChicanX, and Yakama) and examine how sociophonetic variation contributes to differential system performance. We introduce a heuristically-determined Phonetic Error Rate (PER) metric that links recognition errors to specific linguistically motivated variables derived from sociophonetic annotation. Our analysis of eleven sociophonetic features reveals that vowel quality variation, particularly resistance to the low-back merger and pre-nasal merger patterns, is systematically associated with differential error rates across ethnic groups, with the most pronounced effects for African American speakers across all evaluated systems. These findings demonstrate that acoustic modeling of dialectal phonetic variation, rather than lexical or syntactic factors, remains a primary source of bias in commercial ASR systems. The study establishes the PNWE corpus as a valuable resource for bias evaluation in speech technologies and provides actionable guidance for improving ASR performance through targeted representation of sociophonetic diversity in training data.",
    "metadata": {
      "authors": [
        "Michael Scott",
        "Siyu Liang",
        "Alicia Wassink",
        "Gina-Anne Levow"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22495v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_104",
    "source": "arxiv_metadata",
    "title": "The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR",
    "text": "The Limits of Data Scaling: Sub-token Utilization and Acoustic Saturation in Multilingual ASR How much audio is needed to fully observe a multilingual ASR model's learned sub-token inventory across languages, and does data disparity in multilingual pre-training affect how these tokens are utilized during inference? We address this question by analyzing Whisper's decoding behavior during inference across 49 languages. By logging decoding candidate sub-tokens and tracking their cumulative discovery over time, we study the utilization pattern of the model's sub-token space. Results show that the total number of discovered tokens remains largely independent of a language's pre-training hours, indicating that data disparity does not strongly influence lexical diversity in the model's hypothesis space. Sub-token discovery rates follow a consistent exponential saturation pattern across languages, suggesting a stable time window after which additional audio yields minimal new sub-token activation. We refer to this convergence threshold as acoustic saturation time (AST). Further analyses of rank-frequency distributions reveal Zipf-like patterns better modeled by a Zipf-Mandelbrot law, and mean sub-token length shows a positive correlation with resource level. Additionally, those metrics show more favorable patterns for languages in the Latin script than those in scripts such as Cyrillic, CJK, and Semitic. Together, our study suggests that sub-token utilization during multilingual ASR inference is constrained more by the statistical, typological, and orthographic structure of the speech than by training data scale, providing an empirical basis for more equitable corpus construction and cross-lingual evaluation.",
    "metadata": {
      "authors": [
        "Siyu Liang",
        "Nicolas Ballier",
        "Gina-Anne Levow",
        "Richard Wright"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22492v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_105",
    "source": "arxiv_metadata",
    "title": "Frustratingly Easy Task-aware Pruning for Large Language Models",
    "text": "Frustratingly Easy Task-aware Pruning for Large Language Models Pruning provides a practical solution to reduce the resources required to run large language models (LLMs) to benefit from their effective capabilities as well as control their cost for training and inference. Research on LLM pruning often ranks the importance of LLM parameters using their magnitudes and calibration-data activations and removes (or masks) the less important ones, accordingly reducing LLMs' size. However, these approaches primarily focus on preserving the LLM's ability to generate fluent sentences, while neglecting performance on specific domains and tasks. In this paper, we propose a simple yet effective pruning approach for LLMs that preserves task-specific capabilities while shrinking their parameter space. We first analyze how conventional pruning minimizes loss perturbation under general-domain calibration and extend this formulation by incorporating task-specific feature distributions into the importance computation of existing pruning algorithms. Thus, our framework computes separate importance scores using both general and task-specific calibration data, partitions parameters into shared and exclusive groups based on activation-norm differences, and then fuses their scores to guide the pruning process. This design enables our method to integrate seamlessly with various foundation pruning techniques and preserve the LLM's specialized abilities under compression. Experiments on widely used benchmarks demonstrate that our approach is effective and consistently outperforms the baselines with identical pruning ratios and different settings.",
    "metadata": {
      "authors": [
        "Yuanhe Tian",
        "Junjie Liu",
        "Xican Yang",
        "Haishan Ye",
        "Yan Song"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22489v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_106",
    "source": "arxiv_metadata",
    "title": "The Tonogenesis Continuum in Tibetan: A Computational Investigation",
    "text": "The Tonogenesis Continuum in Tibetan: A Computational Investigation Tonogenesis-the historical process by which segmental contrasts evolve into lexical tone-has traditionally been studied through comparative reconstruction and acoustic phonetics. We introduce a computational approach that quantifies the functional role of pitch at different stages of this sound change by measuring how pitch manipulation affects automatic speech recognition (ASR) performance. Through analysis on the sensitivity to pitch-flattening from a set of closely related Tibetan languages, we find evidence of a tonogenesis continuum: atonal Amdo dialects tolerate pitch removal the most, while fully tonal U-Tsang varieties show severe degradation, and intermediate Kham dialects fall measurably between these extremes. These gradient effects demonstrate how ASR models implicitly learn the shifting functional load of pitch as languages transition from consonant-based to tone-based lexical contrasts. Our findings show that computational methods can capture fine-grained stages of sound change and suggest that traditional functional load metrics, based solely on minimal pairs, may overestimate pitch dependence in transitional systems where segmental and suprasegmental cues remain phonetically intertwined.",
    "metadata": {
      "authors": [
        "Siyu Liang",
        "Zhaxi Zerong"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22485v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_107",
    "source": "arxiv_metadata",
    "title": "CHOIR: Collaborative Harmonization fOr Inference Robustness",
    "text": "CHOIR: Collaborative Harmonization fOr Inference Robustness Persona-assigned Large Language Models (LLMs) can adopt diverse roles, enabling personalized and context-aware reasoning. However, even minor demographic perturbations in personas, such as simple pronoun changes, can alter reasoning trajectories, leading to divergent sets of correct answers. Instead of treating these variations as biases to be mitigated, we explore their potential as a constructive resource to improve reasoning robustness. We propose CHOIR (Collaborative Harmonization fOr Inference Robustness), a test-time framework that harmonizes multiple persona-conditioned reasoning signals into a unified prediction. CHOIR orchestrates a collaborative decoding process among counterfactual personas, dynamically balancing agreement and divergence in their reasoning paths. Experiments on various reasoning benchmarks demonstrate that CHOIR consistently enhances performance across demographics, model architectures, scales, and tasks - without additional training. Improvements reach up to 26.4% for individual demographic groups and 19.2% on average across five demographics. It remains effective even when base personas are suboptimal. By reframing persona variation as a constructive signal, CHOIR provides a scalable and generalizable approach to more reliable LLM reasoning.",
    "metadata": {
      "authors": [
        "Xiangjue Dong",
        "Cong Wang",
        "Maria Teleki",
        "Millennium Bismay",
        "James Caverlee"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22475v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_108",
    "source": "arxiv_metadata",
    "title": "Modeling Hierarchical Thinking in Large Reasoning Models",
    "text": "Modeling Hierarchical Thinking in Large Reasoning Models Large Language Models (LLMs) have demonstrated remarkable reasoning abilities when they generate step-by-step solutions, known as chain-of-thought (CoT) reasoning. When trained to using chain-of-thought reasoning examples, the resulting models (called Large Reasoning Models, or LRMs) appear to learn hierarchical thinking strategies similar to those used by humans. However, understanding LRMs emerging reasoning capabilities remains a difficult open problem, with many potential important applications including improving training and understanding robustness. In this paper, we adopt a memoryless Finite State Machine formulation to approximate LRM's emerging hierarchical reasoning dynamics as a structured, interpretable abstraction. We identify a small set of discrete reasoning states including - initialization, deduction, augmentation-strategy, uncertainty-estimation, backtracking, and final-conclusion that capture the high-level states present in the model's reasoning process. By annotating each step of a model's CoT with these states, we can represent the reasoning trajectory as a transition sequence through the state graph. This FSM formulation provides a systematic way to analyze, interpret and visualize how different models approach problems. We describe the FSM model, provide examples of CoT annotations under this scheme, and discuss how it can shed light on differences between available models in their approach to reasoning. Our results demonstrate that this FSM-based analysis reveals distinct reasoning patterns and potential shortcomings, offering a new lens to evaluate and improve LLM reasoning.",
    "metadata": {
      "authors": [
        "G M Shahariar",
        "Ali Nazari",
        "Erfan Shayegani",
        "Nael Abu-Ghazaleh"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22437v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_109",
    "source": "arxiv_metadata",
    "title": "Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection",
    "text": "Confabulations from ACL Publications (CAP): A Dataset for Scientific Hallucination Detection We introduce the CAP (Confabulations from ACL Publications) dataset, a multilingual resource for studying hallucinations in large language models (LLMs) within scientific text generation. CAP focuses on the scientific domain, where hallucinations can distort factual knowledge, as they frequently do. In this domain, however, the presence of specialized terminology, statistical reasoning, and context-dependent interpretations further exacerbates these distortions, particularly given LLMs' lack of true comprehension, limited contextual understanding, and bias toward surface-level generalization. CAP operates in a cross-lingual setting covering five high-resource languages (English, French, Hindi, Italian, and Spanish) and four low-resource languages (Bengali, Gujarati, Malayalam, and Telugu). The dataset comprises 900 curated scientific questions and over 7000 LLM-generated answers from 16 publicly available models, provided as question-answer pairs along with token sequences and corresponding logits. Each instance is annotated with a binary label indicating the presence of a scientific hallucination, denoted as a factuality error, and a fluency label, capturing issues in the linguistic quality or naturalness of the text. CAP is publicly released to facilitate advanced research on hallucination detection, multilingual evaluation of LLMs, and the development of more reliable scientific NLP systems.",
    "metadata": {
      "authors": [
        "Federica Gamba",
        "Aman Sinha",
        "Timothee Mickus",
        "Raul Vazquez",
        "Patanjali Bhamidipati",
        "Claudio Savelli",
        "Ahana Chattopadhyay",
        "Laura A. Zanella",
        "Yash Kankanampati",
        "Binesh Arakkal Remesh",
        "Aryan Ashok Chandramania",
        "Rohit Agarwal",
        "Chuyuan Li",
        "Ioana Buhnila",
        "Radhika Mamidi"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22395v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_110",
    "source": "arxiv_metadata",
    "title": "Label Smoothing Improves Gradient Ascent in LLM Unlearning",
    "text": "Label Smoothing Improves Gradient Ascent in LLM Unlearning LLM unlearning has emerged as a promising approach, aiming to enable models to forget hazardous/undesired knowledge at low cost while preserving as much model utility as possible. Among existing techniques, the most straightforward method is performing Gradient Ascent (GA) w.r.t. the forget data, thereby forcing the model to unlearn the forget dataset. However, GA suffers from severe instability, as it drives updates in a divergent direction, often resulting in drastically degraded model utility. To address this issue, we propose Smoothed Gradient Ascent (SGA). SGA combines the forget data with multiple constructed normal data through a tunable smoothing rate. Intuitively, this extends GA from learning solely on the forget data to jointly learning across both forget and normal data, enabling more stable unlearning while better preserving model utility. Theoretically, we provide the theoretical guidance on the selection of the optimal smoothing rate. Empirically, we evaluate SGA on three benchmarks: TOFU, Harry Potter, and MUSE-NEWS. Experimental results demonstrate that SGA consistently outperforms the original Gradient Ascent (GA) method across all metrics and achieves top-2 performance among all baseline methods on several key metrics.",
    "metadata": {
      "authors": [
        "Zirui Pang",
        "Hao Zheng",
        "Zhijie Deng",
        "Ling Li",
        "Zixin Zhong",
        "Jiaheng Wei"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22376v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_111",
    "source": "arxiv_metadata",
    "title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
    "text": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench.",
    "metadata": {
      "authors": [
        "Yupeng Xie",
        "Zhiyang Zhang",
        "Yifan Wu",
        "Sirong Lu",
        "Jiayi Zhang",
        "Zhaoyang Yu",
        "Jinlin Wang",
        "Sirui Hong",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22373v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_112",
    "source": "arxiv_metadata",
    "title": "Reasoning Models Reason Well, Until They Don't",
    "text": "Reasoning Models Reason Well, Until They Don't Large language models (LLMs) have shown significant progress in reasoning tasks. However, recent studies show that transformers and LLMs fail catastrophically once reasoning problems exceed modest complexity. We revisit these findings through the lens of large reasoning models (LRMs) -- LLMs fine-tuned with incentives for step-by-step argumentation and self-verification. LRM performance on graph and reasoning benchmarks such as NLGraph seem extraordinary, with some even claiming they are capable of generalized reasoning and innovation in reasoning-intensive fields such as mathematics, physics, medicine, and law. However, by more carefully scaling the complexity of reasoning problems, we show existing benchmarks actually have limited complexity. We develop a new dataset, the Deep Reasoning Dataset (DeepRD), along with a generative process for producing unlimited examples of scalable complexity. We use this dataset to evaluate model performance on graph connectivity and natural language proof planning. We find that the performance of LRMs drop abruptly at sufficient complexity and do not generalize. We also relate our LRM results to the distributions of the complexities of large, real-world knowledge graphs, interaction graphs, and proof datasets. We find the majority of real-world examples fall inside the LRMs' success regime, yet the long tails expose substantial failure potential. Our analysis highlights the near-term utility of LRMs while underscoring the need for new methods that generalize beyond the complexity of examples in the training distribution.",
    "metadata": {
      "authors": [
        "Revanth Rameshkumar",
        "Jimson Huang",
        "Yunxin Sun",
        "Fei Xia",
        "Abulhair Saparov"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22371v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_113",
    "source": "arxiv_metadata",
    "title": "GigaEmbeddings: Efficient Russian Language Embedding Model",
    "text": "GigaEmbeddings: Efficient Russian Language Embedding Model We introduce GigaEmbeddings, a novel framework for training high-performance Russian-focused text embeddings through hierarchical instruction tuning of the decoder-only LLM designed specifically for Russian language (GigaChat-3B). Our three-stage pipeline, comprising large-scale contrastive pre-training in web-scale corpora, fine-tuning with hard negatives, and multitask generalization across retrieval, classification, and clustering tasks, addresses key limitations of existing methods by unifying diverse objectives and leveraging synthetic data generation. Architectural innovations include bidirectional attention for contextual modeling, latent attention pooling for robust sequence aggregation, and strategic pruning of 25% of transformer layers to enhance efficiency without compromising performance. Evaluated on the ruMTEB benchmark spanning 23 multilingual tasks, GigaEmbeddings achieves state-of-the-art results (69.1 avg. score), outperforming strong baselines with a larger number of parameters.",
    "metadata": {
      "authors": [
        "Egor Kolodin",
        "Daria Khomich",
        "Nikita Savushkin",
        "Anastasia Ianina",
        "Fyodor Minkin"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22369v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_114",
    "source": "arxiv_metadata",
    "title": "Mapping Faithful Reasoning in Language Models",
    "text": "Mapping Faithful Reasoning in Language Models Chain-of-thought (CoT) traces promise transparency for reasoning language models, but prior work shows they are not always faithful reflections of internal computation. This raises challenges for oversight: practitioners may misinterpret decorative reasoning as genuine. We introduce Concept Walk, a general framework for tracing how a model's internal stance evolves with respect to a concept direction during reasoning. Unlike surface text, Concept Walk operates in activation space, projecting each reasoning step onto the concept direction learned from contrastive data. This allows us to observe whether reasoning traces shape outcomes or are discarded. As a case study, we apply Concept Walk to the domain of Safety using Qwen 3-4B. We find that in 'easy' cases, perturbed CoTs are quickly ignored, indicating decorative reasoning, whereas in 'hard' cases, perturbations induce sustained shifts in internal activations, consistent with faithful reasoning. The contribution is methodological: Concept Walk provides a lens to re-examine faithfulness through concept-specific internal dynamics, helping identify when reasoning traces can be trusted and when they risk misleading practitioners.",
    "metadata": {
      "authors": [
        "Jiazheng Li",
        "Andreas Damianou",
        "J Rosser",
        "José Luis Redondo García",
        "Konstantina Palla"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22362v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_115",
    "source": "arxiv_metadata",
    "title": "Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models",
    "text": "Irony Detection in Urdu Text: A Comparative Study Using Machine Learning Models and Large Language Models Ironic identification is a challenging task in Natural Language Processing, particularly when dealing with languages that differ in syntax and cultural context. In this work, we aim to detect irony in Urdu by translating an English Ironic Corpus into the Urdu language. We evaluate ten state-of-the-art machine learning algorithms using GloVe and Word2Vec embeddings, and compare their performance with classical methods. Additionally, we fine-tune advanced transformer-based models, including BERT, RoBERTa, LLaMA 2 (7B), LLaMA 3 (8B), and Mistral, to assess the effectiveness of large-scale models in irony detection. Among machine learning models, Gradient Boosting achieved the best performance with an F1-score of 89.18%. Among transformer-based models, LLaMA 3 (8B) achieved the highest performance with an F1-score of 94.61%. These results demonstrate that combining transliteration techniques with modern NLP models enables robust irony detection in Urdu, a historically low-resource language.",
    "metadata": {
      "authors": [
        "Fiaz Ahmad",
        "Nisar Hussain",
        "Amna Qasim",
        "Momina Hafeez",
        "Muhammad Usman Grigori Sidorov",
        "Alexander Gelbukh"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22356v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_116",
    "source": "arxiv_metadata",
    "title": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation",
    "text": "FAIR-RAG: Faithful Adaptive Iterative Refinement for Retrieval-Augmented Generation While Retrieval-Augmented Generation (RAG) mitigates hallucination and knowledge staleness in Large Language Models (LLMs), existing frameworks often falter on complex, multi-hop queries that require synthesizing information from disparate sources. Current advanced RAG methods, employing iterative or adaptive strategies, lack a robust mechanism to systematically identify and fill evidence gaps, often propagating noise or failing to gather a comprehensive context. We introduce FAIR-RAG, a novel agentic framework that transforms the standard RAG pipeline into a dynamic, evidence-driven reasoning process. At its core is an Iterative Refinement Cycle governed by a module we term Structured Evidence Assessment (SEA). The SEA acts as an analytical gating mechanism: it deconstructs the initial query into a checklist of required findings and audits the aggregated evidence to identify confirmed facts and, critically, explicit informational gaps. These gaps provide a precise signal to an Adaptive Query Refinement agent, which generates new, targeted sub-queries to retrieve missing information. This cycle repeats until the evidence is verified as sufficient, ensuring a comprehensive context for a final, strictly faithful generation. We conducted experiments on challenging multi-hop QA benchmarks, including HotpotQA, 2WikiMultiHopQA, and MusiQue. In a unified experimental setup, FAIR-RAG significantly outperforms strong baselines. On HotpotQA, it achieves an F1-score of 0.453 -- an absolute improvement of 8.3 points over the strongest iterative baseline -- establishing a new state-of-the-art for this class of methods on these benchmarks. Our work demonstrates that a structured, evidence-driven refinement process with explicit gap analysis is crucial for unlocking reliable and accurate reasoning in advanced RAG systems for complex, knowledge-intensive tasks.",
    "metadata": {
      "authors": [
        "Mohammad Aghajani Asl",
        "Majid Asgari-Bidhendi",
        "Behrooz Minaei-Bidgoli"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22344v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_117",
    "source": "arxiv_metadata",
    "title": "DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry",
    "text": "DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry Solid geometry problem solving demands spatial mathematical reasoning that integrates spatial intelligence and symbolic reasoning. However, most existing multimodal mathematical reasoning benchmarks focus primarily on 2D plane geometry, rely on static datasets prone to data contamination and memorization, and evaluate models solely by final answers, overlooking the reasoning process. To address these limitations, we introduce DynaSolidGeo, the first dynamic benchmark for evaluating genuine spatial reasoning in Vision-Language Models (VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo contains 503 expert-curated seed questions that can, in principle, dynamically generate an unbounded number of diverse multimodal text-visual instances. Beyond answer accuracy, we incorporate process evaluation based on expert-annotated reasoning chains to measure logical validity and causal coherence. Experiments across representative open-source and closed-source VLMs reveal large performance gaps, severe degradation in dynamic settings, and poor performance on tasks requiring high-level spatial intelligence, such as mental rotation and visualization. The code and dataset are available at \\href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.",
    "metadata": {
      "authors": [
        "Changti Wu",
        "Shijie Lian",
        "Zihao Liu",
        "Lei Zhang",
        "Laurence Tianruo Yang",
        "Kai Chen"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22340v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_118",
    "source": "arxiv_metadata",
    "title": "Multilingual Target-Stance Extraction",
    "text": "Multilingual Target-Stance Extraction Social media enables data-driven analysis of public opinion on contested issues. Target-Stance Extraction (TSE) is the task of identifying the target discussed in a document and the document's stance towards that target. Many works classify stance towards a given target in a multilingual setting, but all prior work in TSE is English-only. This work introduces the first multilingual TSE benchmark, spanning Catalan, Estonian, French, Italian, Mandarin, and Spanish corpora. It manages to extend the original TSE pipeline to a multilingual setting without requiring separate models for each language. Our model pipeline achieves a modest F1 score of 12.78, underscoring the increased difficulty of the multilingual task relative to English-only setups and highlighting target prediction as the primary bottleneck. We are also the first to demonstrate the sensitivity of TSE's F1 score to different target verbalizations. Together these serve as a much-needed baseline for resources, algorithms, and evaluation criteria in multilingual TSE.",
    "metadata": {
      "authors": [
        "Ethan Mines",
        "Bonnie Dorr"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22334v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_119",
    "source": "arxiv_metadata",
    "title": "Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling",
    "text": "Memory-based Language Models: An Efficient, Explainable, and Eco-friendly Approach to Large Language Modeling We present memory-based language modeling as an efficient, eco-friendly alternative to deep neural network-based language modeling. It offers log-linearly scalable next-token prediction performance and strong memorization capabilities. Implementing fast approximations of k-nearest neighbor classification, memory-based language modeling leaves a relatively small ecological footprint both in training and in inference mode, as it relies fully on CPUs and attains low token latencies. Its internal workings are simple and fully transparent. We compare our implementation of memory-based language modeling, OLIFANT, with GPT-2 and GPT-Neo on next-token prediction accuracy, estimated emissions and speeds, and offer some deeper analyses of the model.",
    "metadata": {
      "authors": [
        "Antal van den Bosch",
        "Ainhoa Risco Patón",
        "Teun Buijse",
        "Peter Berck",
        "Maarten van Gompel"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22317v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_120",
    "source": "arxiv_metadata",
    "title": "VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription",
    "text": "VietLyrics: A Large-Scale Dataset and Models for Vietnamese Automatic Lyrics Transcription Automatic Lyrics Transcription (ALT) for Vietnamese music presents unique challenges due to its tonal complexity and dialectal variations, but remains largely unexplored due to the lack of a dedicated dataset. Therefore, we curated the first large-scale Vietnamese ALT dataset (VietLyrics), comprising 647 hours of songs with line-level aligned lyrics and metadata to address these issues. Our evaluation of current ASRbased approaches reveal significant limitations, including frequent transcription errors and hallucinations in non-vocal segments. To improve performance, we fine-tuned Whisper models on the VietLyrics dataset, achieving superior results compared to existing multilingual ALT systems, including LyricWhiz. We publicly release VietLyrics and our models, aiming to advance Vietnamese music computing research while demonstrating the potential of this approach for ALT in low-resource language and music.",
    "metadata": {
      "authors": [
        "Quoc Anh Nguyen",
        "Bernard Cheng",
        "Kelvin Soh"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22295v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_121",
    "source": "arxiv_metadata",
    "title": "Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER",
    "text": "Supervised Fine-Tuning or In-Context Learning? Evaluating LLMs for Clinical NER We study clinical Named Entity Recognition (NER) on the CADEC corpus and compare three families of approaches: (i) BERT-style encoders (BERT Base, BioClinicalBERT, RoBERTa-large), (ii) GPT-4o used with few-shot in-context learning (ICL) under simple vs.\\ complex prompts, and (iii) GPT-4o with supervised fine-tuning (SFT). All models are evaluated on standard NER metrics over CADEC's five entity types (ADR, Drug, Disease, Symptom, Finding). RoBERTa-large and BioClinicalBERT offer limited improvements over BERT Base, showing the limit of these family of models. Among LLM settings, simple ICL outperforms a longer, instruction-heavy prompt, and SFT achieves the strongest overall performance (F1 $\\approx$ 87.1%), albeit with higher cost. We find that the LLM achieve higher accuracy on simplified tasks, restricting classification to two labels.",
    "metadata": {
      "authors": [
        "Andrei Baroian"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22285v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_122",
    "source": "arxiv_metadata",
    "title": "CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning",
    "text": "CityRiSE: Reasoning Urban Socio-Economic Status in Vision-Language Models via Reinforcement Learning Harnessing publicly available, large-scale web data, such as street view and satellite imagery, urban socio-economic sensing is of paramount importance for achieving global sustainable development goals. With the emergence of Large Vision-Language Models (LVLMs), new opportunities have arisen to solve this task by treating it as a multi-modal perception and understanding problem. However, recent studies reveal that LVLMs still struggle with accurate and interpretable socio-economic predictions from visual data. To address these limitations and maximize the potential of LVLMs, we introduce \\textbf{CityRiSE}, a novel framework for \\textbf{R}eason\\textbf{i}ng urban \\textbf{S}ocio-\\textbf{E}conomic status in LVLMs through pure reinforcement learning (RL). With carefully curated multi-modal data and verifiable reward design, our approach guides the LVLM to focus on semantically meaningful visual cues, enabling structured and goal-oriented reasoning for generalist socio-economic status prediction. Experiments demonstrate that CityRiSE with emergent reasoning process significantly outperforms existing baselines, improving both prediction accuracy and generalization across diverse urban contexts, particularly for prediction on unseen cities and unseen indicators. This work highlights the promise of combining RL and LVLMs for interpretable and generalist urban socio-economic sensing.",
    "metadata": {
      "authors": [
        "Tianhui Liu",
        "Hetian Pang",
        "Xin Zhang",
        "Jie Feng",
        "Yong Li",
        "Pan Hui"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22282v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_123",
    "source": "arxiv_metadata",
    "title": "WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models",
    "text": "WAON: Large-Scale and High-Quality Japanese Image-Text Pair Dataset for Vision-Language Models Large-scale and high-quality image-text pair datasets play an important role in developing high-performing Vision-Language Models (VLMs). In this work, we introduce WAON, a large-scale and high-quality Japanese image-text pair dataset containing approximately 155 million examples, collected from Common Crawl. Our dataset construction pipeline employs various techniques, including filtering and deduplication, which have been shown to be effective in previous studies. To evaluate its effectiveness, we also construct WAON-Bench, a manually curated benchmark for Japanese cultural image classification, consisting of 374 classes. To assess the effectiveness of our dataset, we conduct experiments using both WAON and the Japanese subset of ReLAION, one of the most widely used vision-language datasets. We fine-tune SigLIP2, a strong multilingual model, on both datasets. The results demonstrate that WAON enhances model performance on WAON-Bench more efficiently than ReLAION and achieves higher accuracy across all evaluated benchmarks. Furthermore, the model fine-tuned on WAON achieves state-of-the-art performance on several Japanese cultural benchmarks. We release our dataset, model, and code at https://speed1313.github.io/WAON.",
    "metadata": {
      "authors": [
        "Issa Sugiura",
        "Shuhei Kurita",
        "Yusuke Oda",
        "Daisuke Kawahara",
        "Yasuo Okabe",
        "Naoaki Okazaki"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22276v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_124",
    "source": "arxiv_metadata",
    "title": "From Slides to Chatbots: Enhancing Large Language Models with University Course Materials",
    "text": "From Slides to Chatbots: Enhancing Large Language Models with University Course Materials Large Language Models (LLMs) have advanced rapidly in recent years. One application of LLMs is to support student learning in educational settings. However, prior work has shown that LLMs still struggle to answer questions accurately within university-level computer science courses. In this work, we investigate how incorporating university course materials can enhance LLM performance in this setting. A key challenge lies in leveraging diverse course materials such as lecture slides and transcripts, which differ substantially from typical textual corpora: slides also contain visual elements like images and formulas, while transcripts contain spoken, less structured language. We compare two strategies, Retrieval-Augmented Generation (RAG) and Continual Pre-Training (CPT), to extend LLMs with course-specific knowledge. For lecture slides, we further explore a multi-modal RAG approach, where we present the retrieved content to the generator in image form. Our experiments reveal that, given the relatively small size of university course materials, RAG is more effective and efficient than CPT. Moreover, incorporating slides as images in the multi-modal setting significantly improves performance over text-only retrieval. These findings highlight practical strategies for developing AI assistants that better support learning and teaching, and we hope they inspire similar efforts in other educational contexts.",
    "metadata": {
      "authors": [
        "Tu Anh Dinh",
        "Philipp Nicolas Schumacher",
        "Jan Niehues"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22272v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_125",
    "source": "arxiv_metadata",
    "title": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding",
    "text": "PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding Patent text embeddings enable prior art search, technology landscaping, and patent analysis, yet existing benchmarks inadequately capture patent-specific challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15 tasks across retrieval, classification, paraphrase, and clustering, with 2.06 million examples. PatenTEB employs domain-stratified splits, domain specific hard negative mining, and systematic coverage of asymmetric fragment-to-document matching scenarios absent from general embedding benchmarks. We develop the patembed model family through multi-task training, spanning 67M to 344M parameters with context lengths up to 4096 tokens. External validation shows strong generalization: patembed-base achieves state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445 previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM. Systematic ablations reveal that multi-task training improves external generalization despite minor benchmark costs, and that domain-pretrained initialization provides consistent advantages across task families. All resources will be made available at https://github.com/iliass-y/patenteb. Keywords: patent retrieval, sentence embeddings, multi-task learning, asymmetric retrieval, benchmark evaluation, contrastive learning.",
    "metadata": {
      "authors": [
        "Iliass Ayaou",
        "Denis Cavallucci"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22264v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_126",
    "source": "arxiv_metadata",
    "title": "SteerX: Disentangled Steering for LLM Personalization",
    "text": "SteerX: Disentangled Steering for LLM Personalization Large language models (LLMs) have shown remarkable success in recent years, enabling a wide range of applications, including intelligent assistants that support users' daily life and work. A critical factor in building such assistants is personalizing LLMs, as user preferences and needs vary widely. Activation steering, which directly leverages directions representing user preference in the LLM activation space to adjust its behavior, offers a cost-effective way to align the model's outputs with individual users. However, existing methods rely on all historical data to compute the steering vector, ignoring that not all content reflects true user preferences, which undermines the personalization signal. To address this, we propose SteerX, a disentangled steering method that isolates preference-driven components from preference-agnostic components. Grounded in causal inference theory, SteerX estimates token-level causal effects to identify preference-driven tokens, transforms these discrete signals into a coherent description, and then leverages them to steer personalized LLM generation. By focusing on the truly preference-driven information, SteerX produces more accurate activation steering vectors and enhances personalization. Experiments on two representative steering backbone methods across real-world datasets demonstrate that SteerX consistently enhances steering vector quality, offering a practical solution for more effective LLM personalization.",
    "metadata": {
      "authors": [
        "Xiaoyan Zhao",
        "Ming Yan",
        "Yilun Qiu",
        "Haoting Ni",
        "Yang Zhang",
        "Fuli Feng",
        "Hong Cheng",
        "Tat-Seng Chua"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22256v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_127",
    "source": "arxiv_metadata",
    "title": "PACR: Progressively Ascending Confidence Reward for LLM Reasoning",
    "text": "PACR: Progressively Ascending Confidence Reward for LLM Reasoning Reinforcement Learning with Verifiable Rewards (RLVR) has significantly improved LLM reasoning, but its sparse, outcome-based reward provides no guidance for intermediate steps, slowing exploration. We propose Progressively Ascending Confidence Reward (PACR), a dense, model-intrinsic reward computed directly from the model's evolving belief in the correct answer. PACR encodes the inductive bias that, along a well-formed reasoning trajectory, the probability of the ground-truth answer should have a generally ascending trend. We provide empirical and theoretical analysis validating that such an inductive bias constrains the exploration search space to regions richer in logically sound reasoning. We demonstrate that PACR accelerates exploration, reaches reward saturation with fewer trajectories, and yields improvements on multiple benchmarks. Our results suggest that dense, model-intrinsic shaping signals can make RLVR training more effective and reliable.",
    "metadata": {
      "authors": [
        "Eunseop Yoon",
        "Hee Suk Yoon",
        "Jaehyun Jang",
        "SooHwan Eom",
        "Qi Dai",
        "Chong Luo",
        "Mark A. Hasegawa-Johnson",
        "Chang D. Yoo"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22255v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_128",
    "source": "arxiv_metadata",
    "title": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion",
    "text": "You Don't Need Prompt Engineering Anymore: The Prompting Inversion Prompt engineering, particularly Chain-of-Thought (CoT) prompting, significantly enhances LLM reasoning capabilities. We introduce \"Sculpting,\" a constrained, rule-based prompting method designed to improve upon standard CoT by reducing errors from semantic ambiguity and flawed common sense. We evaluate three prompting strategies (Zero Shot, standard CoT, and Sculpting) across three OpenAI model generations (gpt-4o-mini, gpt-4o, gpt-5) using the GSM8K mathematical reasoning benchmark (1,317 problems). Our findings reveal a \"Prompting Inversion\": Sculpting provides advantages on gpt-4o (97% vs. 93% for standard CoT), but becomes detrimental on gpt-5 (94.00% vs. 96.36% for CoT on full benchmark). We trace this to a \"Guardrail-to-Handcuff\" transition where constraints preventing common-sense errors in mid-tier models induce hyper-literalism in advanced models. Our detailed error analysis demonstrates that optimal prompting strategies must co-evolve with model capabilities, suggesting simpler prompts for more capable models.",
    "metadata": {
      "authors": [
        "Imran Khan"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22251v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_129",
    "source": "arxiv_metadata",
    "title": "PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading",
    "text": "PaperAsk: A Benchmark for Reliability Evaluation of LLMs in Paper Search and Reading Large Language Models (LLMs) increasingly serve as research assistants, yet their reliability in scholarly tasks remains under-evaluated. In this work, we introduce PaperAsk, a benchmark that systematically evaluates LLMs across four key research tasks: citation retrieval, content extraction, paper discovery, and claim verification. We evaluate GPT-4o, GPT-5, and Gemini-2.5-Flash under realistic usage conditions-via web interfaces where search operations are opaque to the user. Through controlled experiments, we find consistent reliability failures: citation retrieval fails in 48-98% of multi-reference queries, section-specific content extraction fails in 72-91% of cases, and topical paper discovery yields F1 scores below 0.32, missing over 60% of relevant literature. Further human analysis attributes these failures to the uncontrolled expansion of retrieved context and the tendency of LLMs to prioritize semantically relevant text over task instructions. Across basic tasks, the LLMs display distinct failure behaviors: ChatGPT often withholds responses rather than risk errors, whereas Gemini produces fluent but fabricated answers. To address these issues, we develop lightweight reliability classifiers trained on PaperAsk data to identify unreliable outputs. PaperAsk provides a reproducible and diagnostic framework for advancing the reliability evaluation of LLM-based scholarly assistance systems.",
    "metadata": {
      "authors": [
        "Yutao Wu",
        "Xiao Liu",
        "Yunhao Feng",
        "Jiale Ding",
        "Xingjun Ma"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22242v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_130",
    "source": "arxiv_metadata",
    "title": "Evolution of the lexicon: a probabilistic point of view",
    "text": "Evolution of the lexicon: a probabilistic point of view The Swadesh approach for determining the temporal separation between two languages relies on the stochastic process of words replacement (when a complete new word emerges to represent a given concept). It is well known that the basic assumptions of the Swadesh approach are often unrealistic due to various contamination phenomena and misjudgments (horizontal transfers, variations over time and space of the replacement rate, incorrect assessments of cognacy relationships, presence of synonyms, and so on). All of this means that the results cannot be completely correct. More importantly, even in the unrealistic case that all basic assumptions are satisfied, simple mathematics places limits on the accuracy of estimating the temporal separation between two languages. These limits, which are purely probabilistic in nature and which are often neglected in lexicostatistical studies, are analyzed in detail in this article. Furthermore, in this work we highlight that the evolution of a language's lexicon is also driven by another stochastic process: gradual lexical modification of words. We show that this process equally also represents a major contribution to the reshaping of the vocabulary of languages over the centuries and we also show, from a purely probabilistic perspective, that taking into account this second random process significantly increases the precision in determining the temporal separation between two languages.",
    "metadata": {
      "authors": [
        "Maurizio Serva"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22220v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_131",
    "source": "arxiv_metadata",
    "title": "Estimating the Error of Large Language Models at Pairwise Text Comparison",
    "text": "Estimating the Error of Large Language Models at Pairwise Text Comparison We measure LLMs' output error at pairwise text comparison, noting the probability of error in their preferences. Our method does not rely on the ground truth and supports two scenarios: (i) uniform error rate regardless of the order of comparison, estimated with two comparisons for each text pair with either text placed first; (ii) binary positional bias assuming distinct error rates for the two orders of comparison, estimated with repeated comparisons between the texts. The Copeland counting constructs a ranking over the compared texts from pairwise preferences; the ranking reveals the poor scalability of LLM-based pairwise comparison and helps yield the estimates for LLMs' error rates. We apply the method to six LLMs (ChatGPT, Claude, DeepSeek, Gemini, Grok, Qwen) with five types of text input and obtain consistent estimates of LLMs' error. In general, the measured two positional bias terms are similar, close to the uniform error. Considering both the error rates and the robustness to the variation of prompts, Claude obtained the most desirable performance in this experiment. Our model outperforms the biased Bradley-Terry model and the commutativity score in indicating LLMs' error at this task.",
    "metadata": {
      "authors": [
        "Tianyi Li"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22219v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_132",
    "source": "arxiv_metadata",
    "title": "DETECT: Determining Ease and Textual Clarity of German Text Simplifications",
    "text": "DETECT: Determining Ease and Textual Clarity of German Text Simplifications Current evaluation of German automatic text simplification (ATS) relies on general-purpose metrics such as SARI, BLEU, and BERTScore, which insufficiently capture simplification quality in terms of simplicity, meaning preservation, and fluency. While specialized metrics like LENS have been developed for English, corresponding efforts for German have lagged behind due to the absence of human-annotated corpora. To close this gap, we introduce DETECT, the first German-specific metric that holistically evaluates ATS quality across all three dimensions of simplicity, meaning preservation, and fluency, and is trained entirely on synthetic large language model (LLM) responses. Our approach adapts the LENS framework to German and extends it with (i) a pipeline for generating synthetic quality scores via LLMs, enabling dataset creation without human annotation, and (ii) an LLM-based refinement step for aligning grading criteria with simplification requirements. To the best of our knowledge, we also construct the largest German human evaluation dataset for text simplification to validate our metric directly. Experimental results show that DETECT achieves substantially higher correlations with human judgments than widely used ATS metrics, with particularly strong gains in meaning preservation and fluency. Beyond ATS, our findings highlight both the potential and the limitations of LLMs for automatic evaluation and provide transferable guidelines for general language accessibility tasks.",
    "metadata": {
      "authors": [
        "Maria Korobeynikova",
        "Alessia Battisti",
        "Lukas Fischer",
        "Yingqiang Gao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22212v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_133",
    "source": "arxiv_metadata",
    "title": "The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I)",
    "text": "The Lossy Horizon: Error-Bounded Predictive Coding for Lossy Text Compression (Episode I) Large Language Models (LLMs) can achieve near-optimal lossless compression by acting as powerful probability models. We investigate their use in the lossy domain, where reconstruction fidelity is traded for higher compression ratios. This paper introduces Error-Bounded Predictive Coding (EPC), a lossy text codec that leverages a Masked Language Model (MLM) as a decompressor. Instead of storing a subset of original tokens, EPC allows the model to predict masked content and stores minimal, rank-based corrections only when the model's top prediction is incorrect. This creates a residual channel that offers continuous rate-distortion control. We compare EPC to a simpler Predictive Masking (PM) baseline and a transform-based Vector Quantisation with a Residual Patch (VQ+RE) approach. Through an evaluation that includes precise bit accounting and rate-distortion analysis, we demonstrate that EPC consistently dominates PM, offering superior fidelity at a significantly lower bit rate by more efficiently utilising the model's intrinsic knowledge.",
    "metadata": {
      "authors": [
        "Nnamdi Aghanya",
        "Jun Li",
        "Kewei Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22207v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_134",
    "source": "arxiv_metadata",
    "title": "M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR",
    "text": "M-CIF: Multi-Scale Alignment For CIF-Based Non-Autoregressive ASR The Continuous Integrate-and-Fire (CIF) mechanism provides effective alignment for non-autoregressive (NAR) speech recognition. This mechanism creates a smooth and monotonic mapping from acoustic features to target tokens, achieving performance on Mandarin competitive with other NAR approaches. However, without finer-grained guidance, its stability degrades in some languages such as English and French. In this paper, we propose Multi-scale CIF (M-CIF), which performs multi-level alignment by integrating character and phoneme level supervision progressively distilled into subword representations, thereby enhancing robust acoustic-text alignment. Experiments show that M-CIF reduces WER compared to the Paraformer baseline, especially on CommonVoice by 4.21% in German and 3.05% in French. To further investigate these gains, we define phonetic confusion errors (PE) and space-related segmentation errors (SE) as evaluation metrics. Analysis of these metrics across different M-CIF settings reveals that the phoneme and character layers are essential for enhancing progressive CIF alignment.",
    "metadata": {
      "authors": [
        "Ruixiang Mao",
        "Xiangnan Ma",
        "Qing Yang",
        "Ziming Zhu",
        "Yucheng Qiao",
        "Yuan Ge",
        "Tong Xiao",
        "Shengxiang Gao",
        "Zhengtao Yu",
        "Jingbo Zhu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22172v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_135",
    "source": "arxiv_metadata",
    "title": "Surface Reading LLMs: Synthetic Text and its Styles",
    "text": "Surface Reading LLMs: Synthetic Text and its Styles Despite a potential plateau in ML advancement, the societal impact of large language models lies not in approaching superintelligence but in generating text surfaces indistinguishable from human writing. While Critical AI Studies provides essential material and socio-technical critique, it risks overlooking how LLMs phenomenologically reshape meaning-making. This paper proposes a semiotics of \"surface integrity\" as attending to the immediate plane where LLMs inscribe themselves into human communication. I distinguish three knowledge interests in ML research (epistemology, epist\\=em\\=e, and epistemics) and argue for integrating surface-level stylistic analysis alongside depth-oriented critique. Through two case studies examining stylistic markers of synthetic text, I argue how attending to style as a semiotic phenomenon reveals LLMs as cultural actors that transform the conditions of meaning emergence and circulation in contemporary discourse, independent of questions about machine consciousness.",
    "metadata": {
      "authors": [
        "Hannes Bajohr"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22162v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_136",
    "source": "arxiv_metadata",
    "title": "SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language",
    "text": "SentiMaithili: A Benchmark Dataset for Sentiment and Reason Generation for the Low-Resource Maithili Language Developing benchmark datasets for low-resource languages poses significant challenges, primarily due to the limited availability of native linguistic experts and the substantial time and cost involved in annotation. Given these challenges, Maithili is still underrepresented in natural language processing research. It is an Indo-Aryan language spoken by more than 13 million people in the Purvanchal region of India, valued for its rich linguistic structure and cultural significance. While sentiment analysis has achieved remarkable progress in high-resource languages, resources for low-resource languages, such as Maithili, remain scarce, often restricted to coarse-grained annotations and lacking interpretability mechanisms. To address this limitation, we introduce a novel dataset comprising 3,221 Maithili sentences annotated for sentiment polarity and accompanied by natural language justifications. Moreover, the dataset is carefully curated and validated by linguistic experts to ensure both label reliability and contextual fidelity. Notably, the justifications are written in Maithili, thereby promoting culturally grounded interpretation and enhancing the explainability of sentiment models. Furthermore, extensive experiments using both classical machine learning and state-of-the-art transformer architectures demonstrate the dataset's effectiveness for interpretable sentiment analysis. Ultimately, this work establishes the first benchmark for explainable affective computing in Maithili, thus contributing a valuable resource to the broader advancement of multilingual NLP and explainable AI.",
    "metadata": {
      "authors": [
        "Rahul Ranjan",
        "Mahendra Kumar Gurve",
        " Anuj",
        " Nitin",
        "Yamuna Prasad"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22160v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_137",
    "source": "arxiv_metadata",
    "title": "Power to the Clients: Federated Learning in a Dictatorship Setting",
    "text": "Power to the Clients: Federated Learning in a Dictatorship Setting Federated learning (FL) has emerged as a promising paradigm for decentralized model training, enabling multiple clients to collaboratively learn a shared model without exchanging their local data. However, the decentralized nature of FL also introduces vulnerabilities, as malicious clients can compromise or manipulate the training process. In this work, we introduce dictator clients, a novel, well-defined, and analytically tractable class of malicious participants capable of entirely erasing the contributions of all other clients from the server model, while preserving their own. We propose concrete attack strategies that empower such clients and systematically analyze their effects on the learning process. Furthermore, we explore complex scenarios involving multiple dictator clients, including cases where they collaborate, act independently, or form an alliance in order to ultimately betray one another. For each of these settings, we provide a theoretical analysis of their impact on the global model's convergence. Our theoretical algorithms and findings about the complex scenarios including multiple dictator clients are further supported by empirical evaluations on both computer vision and natural language processing benchmarks.",
    "metadata": {
      "authors": [
        "Mohammadsajad Alipour",
        "Mohammad Mohammadi Amiri"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22149v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_138",
    "source": "arxiv_metadata",
    "title": "OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue",
    "text": "OlaMind: Towards Human-Like and Hallucination-Safe Customer Service for Retrieval-Augmented Dialogue Intelligent customer service (ICS) systems via retrieval-augmented generation (RAG) have been widely adopted in Web-based domains such as social platforms and e-commerce, achieving remarkable improvements in automation and efficiency. However, notable limitations still remain: these systems are prone to hallucinations and often generate rigid, mechanical responses, which can introduce business risks and undermine user experience, especially in Web-based customer service interactions under the RAG scenarios. In this paper, we introduce OlaMind, a human-like and hallucination-safe customer service framework for retrieval-augmented dialogue. Specifically, it first leverages a Learn-to-Think stage to learn the reasoning processes and response strategies from human experts, and then employs a Learn-to-Respond stage to perform cold-start supervised fine-tuning (SFT) combined with reinforcement learning (RL) for basic-to-hard self-refinement. Our method significantly enhances human-likeness and naturalness while effectively mitigating hallucinations and critical business risks. We have conducted large-scale online A/B experiments in an industry-level social customer service setting, and extensive experimental results show that OlaMind achieves significant cumulative relative improvements with intelligent resolution rates +28.92%/+18.42% and human takeover rate -6.08%/-7.12% in community-support/livestream-interaction scenarios, respectively, which highlights its consistent effectiveness across diverse real-world applications. The code and data will be publicly available.",
    "metadata": {
      "authors": [
        "Tianhong Gao",
        "Jundong Shen",
        "Bei Shi",
        "Jiapeng Wang",
        "Ying Ju",
        "Junfeng Yao",
        "Jiao Ran",
        "Yong Zhang",
        "Lin Dong",
        "Huiyu Yu",
        "Tingting Ye"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22143v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_139",
    "source": "arxiv_metadata",
    "title": "LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction",
    "text": "LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction Vision-Language Models (VLMs) have shown significant progress in open-set challenges. However, the limited availability of 3D datasets hinders their effective application in 3D scene understanding. We propose LOC, a general language-guided framework adaptable to various occupancy networks, supporting both supervised and self-supervised learning paradigms. For self-supervised tasks, we employ a strategy that fuses multi-frame LiDAR points for dynamic/static scenes, using Poisson reconstruction to fill voids, and assigning semantics to voxels via K-Nearest Neighbor (KNN) to obtain comprehensive voxel representations. To mitigate feature over-homogenization caused by direct high-dimensional feature distillation, we introduce Densely Contrastive Learning (DCL). DCL leverages dense voxel semantic information and predefined textual prompts. This efficiently enhances open-set recognition without dense pixel-level supervision, and our framework can also leverage existing ground truth to further improve performance. Our model predicts dense voxel features embedded in the CLIP feature space, integrating textual and image pixel information, and classifies based on text and semantic similarity. Experiments on the nuScenes dataset demonstrate the method's superior performance, achieving high-precision predictions for known classes and distinguishing unknown classes without additional training data.",
    "metadata": {
      "authors": [
        "Yuhang Gao",
        "Xiang Xiang",
        "Sheng Zhong",
        "Guoyou Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22141v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_140",
    "source": "arxiv_metadata",
    "title": "Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs",
    "text": "Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs Lifelong knowledge editing enables continuous, precise updates to outdated knowledge in large language models (LLMs) without computationally expensive full retraining. However, existing methods often accumulate errors throughout the editing process, causing a gradual decline in both editing accuracy and generalization. To tackle this problem, we propose Neuron-Specific Masked Knowledge Editing (NMKE), a novel fine-grained editing framework that combines neuron-level attribution with dynamic sparse masking. Leveraging neuron functional attribution, we identify two key types of knowledge neurons, with knowledge-general neurons activating consistently across prompts and knowledge-specific neurons activating to specific prompts. NMKE further introduces an entropy-guided dynamic sparse mask, locating relevant neurons to the target knowledge. This strategy enables precise neuron-level knowledge editing with fewer parameter modifications. Experimental results from thousands of sequential edits demonstrate that NMKE outperforms existing methods in maintaining high editing success rates and preserving model general capabilities in lifelong editing.",
    "metadata": {
      "authors": [
        "Jinzhe Liu",
        "Junshu Sun",
        "Shufan Shen",
        "Chenxue Yang",
        "Shuhui Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22139v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_141",
    "source": "arxiv_metadata",
    "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation",
    "text": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.",
    "metadata": {
      "authors": [
        " Ling-Team",
        "Ang Li",
        "Ben Liu",
        "Binbin Hu",
        "Bing Li",
        "Bingwei Zeng",
        "Borui Ye",
        "Caizhi Tang",
        "Changxin Tian",
        "Chao Huang",
        "Chao Zhang",
        "Chen Qian",
        "Chenchen Ju",
        "Chenchen Li",
        "Chengfu Tang",
        "Chili Fu",
        "Chunshao Ren",
        "Chunwei Wu",
        "Cong Zhang",
        "Cunyin Peng",
        "Dafeng Xu",
        "Daixin Wang",
        "Dalong Zhang",
        "Dingnan Jin",
        "Dingyuan Zhu",
        "Dongke Hu",
        "Fangzheng Zhao",
        "Feifan Wu",
        "Feng Zhu",
        "Gangshan Wang",
        "Haitao Zhang",
        "Hailin Zhao",
        "Hanxiao Zhang",
        "Hanzi Wang",
        "Hao Qian",
        "Haoyi Yu",
        "Heng Zhang",
        "Hongliang Zhang",
        "Hongzhi Luan",
        "Huirong Dong",
        "Huizhong Li",
        "Jia Li",
        "Jia Liu",
        "Jialong Zhu",
        "Jian Sha",
        "Jianping Wei",
        "Jiaolong Yang",
        "Jieyue Ma",
        "Jiewei Wu",
        "Jinjing Huang",
        "Jingyun Tian",
        "Jingyuan Zhang",
        "Jinquan Sun",
        "Juanhui Tu",
        "Jun Liu",
        "Jun Xu",
        "Jun Zhou",
        "Junjie Ou",
        "Junpeng Fang",
        "Kaihong Zhang",
        "Kaiqin Hu",
        "Ke Shi",
        "Kun Tang",
        "Kunlong Chen",
        "Lanyin Mei",
        "Lei Liang",
        "Lei Xu",
        "Libo Zhang",
        "Lin Ju",
        "Lin Yuan",
        "Ling Zhong",
        "Lintao Ma",
        "Lu Liu",
        "Lu Yu",
        "Lun Cai",
        "Meiqi Zhu",
        "Mengying Li",
        "Min Chen",
        "Minghao Xue",
        "Minghong Cai",
        "Mingming Yin",
        "Peijie Jiang",
        "Peilong Zhao",
        "Pingping Liu",
        "Qian Zhao",
        "Qing Cui",
        "Qingxiang Huang",
        "Qingyuan Yang",
        "Quankun Yu",
        "Shaowei Wei",
        "Shijie Lian",
        "Shoujian Zheng",
        "Shun Song",
        "Shungen Zhang",
        "Shuo Zhang",
        "Siyuan Li",
        "Song Liu",
        "Ting Guo",
        "Tong Zhao",
        "Wanli Gu",
        "Weichang Wu",
        "Weiguang Han",
        "Wenjing Fang",
        "Wubin Wang",
        "Xiang Shu",
        "Xiao Shi",
        "Xiaoshun Lan",
        "Xiaolu Zhang",
        "Xiaqing Sun",
        "Xin Zhao",
        "Xingyu Lu",
        "Xiong Xu",
        "Xudong Wang",
        "Xudong Wang",
        "Xuemin Yang",
        "Yajie Yang",
        "Yang Xiang",
        "Yanzhe Li",
        "Yi Zhang",
        "Yilong Wang",
        "Yingxue Li",
        "Yongzhen Guo",
        "Yuzhuo Fu",
        "Yuanyuan Wang",
        "Yue Yang",
        "Yue Yu",
        "Yufeng Deng",
        "Yun Zhang",
        "Yunfei Xu",
        "Yuqi Zhang",
        "Yuxiao He",
        "Zengke Gui",
        "Zhaoxin Huan",
        "Zhaoyang Wang",
        "Zhibo Zhu",
        "Zhihao Wang",
        "Zhiqiang Zhang",
        "Zhoufei Wang",
        "Zihang Zeng",
        "Ziqi Liu",
        "Zitao Xuan",
        "Zuoli Tang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22115v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_142",
    "source": "arxiv_metadata",
    "title": "Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows",
    "text": "Gradual Forgetting: Logarithmic Compression for Extending Transformer Context Windows Most approaches to long-context processing increase the complexity of the transformer's internal architecture by integrating mechanisms such as recurrence or auxiliary memory modules. In this work, we introduce an alternative approach that modifies the input representation itself, rather than the transformer architecture. Inspired by cognitive models of human memory, our method applies a scale-invariant logarithmic compression to the input tokens. The resulting compressed representation is processed by a standard, unmodified transformer, preserving architectural simplicity. We evaluate this approach on the WikiText-103 and PG-19 language modeling benchmarks, showing a reduction in perplexity compared to uncompressed baselines. Moreover, performance improves consistently with longer compressed temporal contexts, showing that input-level logarithmic compression is a simple and effective way to extend a transformer's long-range memory.",
    "metadata": {
      "authors": [
        "Billy Dickson",
        "Zoran Tiganj"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22109v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_143",
    "source": "arxiv_metadata",
    "title": "Mitigating Coordinate Prediction Bias from Positional Encoding Failures",
    "text": "Mitigating Coordinate Prediction Bias from Positional Encoding Failures Multimodal large language models (MLLMs) excel at vision-language tasks such as VQA and document understanding, yet precise coordinate prediction remains challenging. High-resolution inputs exacerbate this difficulty by producing long token sequences that weaken positional encodings and introduce directional biases in coordinate outputs. We investigate this phenomenon by analyzing how MLLMs behave when visual positional encodings (VPEs) are deliberately perturbed through shuffling. Our analysis reveals that such perturbations induce predictable, non-random coordinate biases rather than random errors, suggesting that models rely on internal positional priors when spatial grounding signals are degraded. Crucially, we observe similar directional error patterns in natural high-resolution datasets, indicating that positional encoding failures are a key bottleneck for accurate coordinate prediction at scale. To address this issue, we propose Vision-PE Shuffle Guidance (VPSG), a training-free test-time method that leverages the directional nature of these biases for correction. VPSG runs auxiliary decoding with shuffled VPEs to isolate position-unconditioned tendencies, then uses this as negative evidence to guide digit prediction while preserving coordinate format through a lightweight finite-state machine. Experiments on ScreenSpot-Pro demonstrate reliable improvements, highlighting positional encoding robustness as a critical factor for spatial reasoning in MLLMs.",
    "metadata": {
      "authors": [
        "Xingjian Tao",
        "Yiwei Wang",
        "Yujun Cai",
        "Yihong Luo",
        "Jing Tang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22102v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_144",
    "source": "arxiv_metadata",
    "title": "Generalization or Memorization: Dynamic Decoding for Mode Steering",
    "text": "Generalization or Memorization: Dynamic Decoding for Mode Steering Large Language Models (LLMs) exhibit a troubling duality, capable of both remarkable generalization and brittle, verbatim memorization of their training data. This unpredictability undermines their reliability in high-stakes applications. In this work, we propose a unified framework to understand, identify, and control these distinct reasoning modes. First, we introduce a theoretical model based on the Information Bottleneck (IB) principle, formalizing generalization as the learning of a compressed, task-relevant representation and memorization as a failure to compress. Building on this theory, we develop Dynamic Mode Steering (DMS), a novel inference-time algorithm which comprises two components: (1) a lightweight, causally-grounded linear probe that identifies the model's instantaneous reliance on memorization, and (2) a dynamic activation steering mechanism that nudges the model's computation towards pre-identified generalization circuits. We frame DMS as a form of adaptive, self-contrastive decoding. Experiments on reasoning and faithfulness tasks demonstrate that DMS significantly improves logical consistency and factual accuracy, thereby offering a principled approach to enhancing LLM reliability.",
    "metadata": {
      "authors": [
        "Xuanming Zhang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22099v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_145",
    "source": "arxiv_metadata",
    "title": "Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies",
    "text": "Embracing Trustworthy Brain-Agent Collaboration as Paradigm Extension for Intelligent Assistive Technologies Brain-Computer Interfaces (BCIs) offer a direct communication pathway between the human brain and external devices, holding significant promise for individuals with severe neurological impairments. However, their widespread adoption is hindered by critical limitations, such as low information transfer rates and extensive user-specific calibration. To overcome these challenges, recent research has explored the integration of Large Language Models (LLMs), extending the focus from simple command decoding to understanding complex cognitive states. Despite these advancements, deploying agentic AI faces technical hurdles and ethical concerns. Due to the lack of comprehensive discussion on this emerging direction, this position paper argues that the field is poised for a paradigm extension from BCI to Brain-Agent Collaboration (BAC). We emphasize reframing agents as active and collaborative partners for intelligent assistance rather than passive brain signal data processors, demanding a focus on ethical data handling, model reliability, and a robust human-agent collaboration framework to ensure these systems are safe, trustworthy, and effective.",
    "metadata": {
      "authors": [
        "Yankai Chen",
        "Xinni Zhang",
        "Yifei Zhang",
        "Yangning Li",
        "Henry Peng Zou",
        "Chunyu Miao",
        "Weizhi Zhang",
        "Xue Liu",
        "Philip S. Yu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22095v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_146",
    "source": "arxiv_metadata",
    "title": "Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models",
    "text": "Jailbreak Mimicry: Automated Discovery of Narrative-Based Jailbreaks for Large Language Models Large language models (LLMs) remain vulnerable to sophisticated prompt engineering attacks that exploit contextual framing to bypass safety mechanisms, posing significant risks in cybersecurity applications. We introduce Jailbreak Mimicry, a systematic methodology for training compact attacker models to automatically generate narrative-based jailbreak prompts in a one-shot manner. Our approach transforms adversarial prompt discovery from manual craftsmanship into a reproducible scientific process, enabling proactive vulnerability assessment in AI-driven security systems. Developed for the OpenAI GPT-OSS-20B Red-Teaming Challenge, we use parameter-efficient fine-tuning (LoRA) on Mistral-7B with a curated dataset derived from AdvBench, achieving an 81.0% Attack Success Rate (ASR) against GPT-OSS-20B on a held-out test set of 200 items. Cross-model evaluation reveals significant variation in vulnerability patterns: our attacks achieve 66.5% ASR against GPT-4, 79.5% on Llama-3 and 33.0% against Gemini 2.5 Flash, demonstrating both broad applicability and model-specific defensive strengths in cybersecurity contexts. This represents a 54x improvement over direct prompting (1.5% ASR) and demonstrates systematic vulnerabilities in current safety alignment approaches. Our analysis reveals that technical domains (Cybersecurity: 93% ASR) and deception-based attacks (Fraud: 87.8% ASR) are particularly vulnerable, highlighting threats to AI-integrated threat detection, malware analysis, and secure systems, while physical harm categories show greater resistance (55.6% ASR). We employ automated harmfulness evaluation using Claude Sonnet 4, cross-validated with human expert assessment, ensuring reliable and scalable evaluation for cybersecurity red-teaming. Finally, we analyze failure mechanisms and discuss defensive strategies to mitigate these vulnerabilities in AI for cybersecurity.",
    "metadata": {
      "authors": [
        "Pavlos Ntais"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22085v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_147",
    "source": "arxiv_metadata",
    "title": "Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds",
    "text": "Compositional Bias Control in Large Language Models: Preference Learning Fails, Supervision Succeeds Large Language Models (LLMs) still produce gender-stereotyped language even in occupation-neutral contexts that reflect deep societal biases (Rudinger et al., 2018). To address this, prior work has proposed prompting, constrained decoding (Dathathri et al., 2020; Zhou et al., 2024), post-processing, and fine-tuning-based alignment (Rafailov et al., 2023; Ravfogel et al., 2022). However, the comparative efficacy and learning dynamics remain little understood. We report a comparative analysis of six control techniques for bias mitigation: prompt-only, generate-and-filter, DFA-based Ctrl-G decoding, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Iterative Nullspace Projection (INLP). We evaluate each method on a compositional constraint task. This task requires generating sentences that contain at least one agentic and one communal descriptor for each of the twenty Winogender-derived occupations. We quantify trade-offs between control strength and naturalness with evaluations of constraint compliance, lexical diversity, and fluency. Our results reveal key contrasts among the methods: SFT achieves 99.87 +- 0.15% compliance and high lexical diversity, while DPO, despite similar training stability, fails at 4.53 +- 0.82%. Ctrl-G guarantees perfect compliance, but at the cost of severely reduced fluency and diversity. Preference-based learning fundamentally differs: it cannot satisfy compositional constraints, as binary preference signals encode ranking, not logical conjunctions. Only explicit positive supervision enables mitigation of compositional biases; preference-based alignment fails to generalize logical structures, underscoring the limitations of preference learning and the necessity of explicit supervision for fair and fluent controlled generation.",
    "metadata": {
      "authors": [
        "Atij Mahesh"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22084v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_148",
    "source": "arxiv_metadata",
    "title": "Agentic Reinforcement Learning for Real-World Code Repair",
    "text": "Agentic Reinforcement Learning for Real-World Code Repair We tackle the challenge of training reliable code-fixing agents in real repositories, where complex builds and shifting dependencies make evaluation unstable. We developed a verifiable pipeline with success defined as post-fix build validation and improved reproducibility across ~1K real issues by pinning dependencies and disabling automatic upgrades. Building on this, we introduced a scalable simplified pipeline for large-scale reinforcement learning (RL). Using this setup, we supervised fine-tuned Qwen3-32B in the full pipeline and applied RL on top of the SFT model in the simplified environment. The SFT model distilled from GPT-4.1 trajectories performs on par while being 56x smaller, and RL added 7-20% absolute gains under matched train-test conditions. \"Thinking mode\" was on par or worse in our experiments. Both SFT and RL models failed to generalize across environments, highlighting the importance of matching train-test environments for building reliable real-world code-fixing agents.",
    "metadata": {
      "authors": [
        "Siyu Zhu",
        "Anastasiya Karpovich",
        "Albert Chen",
        "Jessica Koscheka",
        "Shailesh Jannu",
        "Di Wen",
        "Yuqing Zhu",
        "Rohit Jain",
        "Alborz Geramifard"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22075v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_149",
    "source": "arxiv_metadata",
    "title": "A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition",
    "text": "A Benchmark for Open-Domain Numerical Fact-Checking Enhanced by Claim Decomposition Fact-checking numerical claims is critical as the presence of numbers provide mirage of veracity despite being fake potentially causing catastrophic impacts on society. The prior works in automatic fact verification do not primarily focus on natural numerical claims. A typical human fact-checker first retrieves relevant evidence addressing the different numerical aspects of the claim and then reasons about them to predict the veracity of the claim. Hence, the search process of a human fact-checker is a crucial skill that forms the foundation of the verification process. Emulating a real-world setting is essential to aid in the development of automated methods that encompass such skills. However, existing benchmarks employ heuristic claim decomposition approaches augmented with weakly supervised web search to collect evidences for verifying claims. This sometimes results in less relevant evidences and noisy sources with temporal leakage rendering a less realistic retrieval setting for claim verification. Hence, we introduce QuanTemp++: a dataset consisting of natural numerical claims, an open domain corpus, with the corresponding relevant evidence for each claim. The evidences are collected through a claim decomposition process approximately emulating the approach of human fact-checker and veracity labels ensuring there is no temporal leakage. Given this dataset, we also characterize the retrieval performance of key claim decomposition paradigms. Finally, we observe their effect on the outcome of the verification pipeline and draw insights. The code for data pipeline along with link to data can be found at https://github.com/VenkteshV/QuanTemp_Plus",
    "metadata": {
      "authors": [
        "V Venktesh",
        "Deepali Prabhu",
        "Avishek Anand"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22055v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_150",
    "source": "arxiv_metadata",
    "title": "Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models",
    "text": "Emotions Where Art Thou: Understanding and Characterizing the Emotional Latent Space of Large Language Models This work investigates how large language models (LLMs) internally represent emotion by analyzing the geometry of their hidden-state space. The paper identifies a low-dimensional emotional manifold and shows that emotional representations are directionally encoded, distributed across layers, and aligned with interpretable dimensions. These structures are stable across depth and generalize to eight real-world emotion datasets spanning five languages. Cross-domain alignment yields low error and strong linear probe performance, indicating a universal emotional subspace. Within this space, internal emotion perception can be steered while preserving semantics using a learned intervention module, with especially strong control for basic emotions across languages. These findings reveal a consistent and manipulable affective geometry in LLMs and offer insight into how they internalize and process emotion.",
    "metadata": {
      "authors": [
        "Benjamin Reichman",
        "Adar Avsian",
        "Larry Heck"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22042v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_151",
    "source": "arxiv_metadata",
    "title": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality",
    "text": "ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.",
    "metadata": {
      "authors": [
        "Shayne Longpre",
        "Sneha Kudugunta",
        "Niklas Muennighoff",
        "I-Hung Hsu",
        "Isaac Caswell",
        "Alex Pentland",
        "Sercan Arik",
        "Chen-Yu Lee",
        "Sayna Ebrahimi"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22037v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_152",
    "source": "arxiv_metadata",
    "title": "Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics",
    "text": "Penalizing Length: Uncovering Systematic Bias in Quality Estimation Metrics Quality Estimation (QE) metrics are vital in machine translation for reference-free evaluation and as a reward signal in tasks like reinforcement learning. However, the prevalence and impact of length bias in QE have been underexplored. Through a systematic study of top-performing regression-based and LLM-as-a-Judge QE metrics across 10 diverse language pairs, we reveal two critical length biases: First, QE metrics consistently over-predict errors with increasing translation length, even for high-quality, error-free texts. Second, they exhibit a preference for shorter translations when multiple candidates are available for the same source text. These inherent length biases risk unfairly penalizing longer, correct translations and can lead to sub-optimal decision-making in applications such as QE reranking and QE guided reinforcement learning. To mitigate this, we propose two strategies: (a) applying length normalization during model training, and (b) incorporating reference texts during evaluation. Both approaches were found to effectively reduce the identified length bias.",
    "metadata": {
      "authors": [
        "Yilin Zhang",
        "Wenda Xu",
        "Zhongtao Liu",
        "Tetsuji Nakagawa",
        "Markus Freitag"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22028v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_153",
    "source": "arxiv_metadata",
    "title": "Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models",
    "text": "Toward Understanding the Transferability of Adversarial Suffixes in Large Language Models Discrete optimization-based jailbreaking attacks on large language models aim to generate short, nonsensical suffixes that, when appended onto input prompts, elicit disallowed content. Notably, these suffixes are often transferable -- succeeding on prompts and models for which they were never optimized. And yet, despite the fact that transferability is surprising and empirically well-established, the field lacks a rigorous analysis of when and why transfer occurs. To fill this gap, we identify three statistical properties that strongly correlate with transfer success across numerous experimental settings: (1) how much a prompt without a suffix activates a model's internal refusal direction, (2) how strongly a suffix induces a push away from this direction, and (3) how large these shifts are in directions orthogonal to refusal. On the other hand, we find that prompt semantic similarity only weakly correlates with transfer success. These findings lead to a more fine-grained understanding of transferability, which we use in interventional experiments to showcase how our statistical analysis can translate into practical improvements in attack success.",
    "metadata": {
      "authors": [
        "Sarah Ball",
        "Niki Hasrati",
        "Alexander Robey",
        "Avi Schwarzschild",
        "Frauke Kreuter",
        "Zico Kolter",
        "Andrej Risteski"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22014v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_154",
    "source": "arxiv_metadata",
    "title": "Optimal Detection for Language Watermarks with Pseudorandom Collision",
    "text": "Optimal Detection for Language Watermarks with Pseudorandom Collision Text watermarking plays a crucial role in ensuring the traceability and accountability of large language model (LLM) outputs and mitigating misuse. While promising, most existing methods assume perfect pseudorandomness. In practice, repetition in generated text induces collisions that create structured dependence, compromising Type I error control and invalidating standard analyses. We introduce a statistical framework that captures this structure through a hierarchical two-layer partition. At its core is the concept of minimal units -- the smallest groups treatable as independent across units while permitting dependence within. Using minimal units, we define a non-asymptotic efficiency measure and cast watermark detection as a minimax hypothesis testing problem. Applied to Gumbel-max and inverse-transform watermarks, our framework produces closed-form optimal rules. It explains why discarding repeated statistics often improves performance and shows that within-unit dependence must be addressed unless degenerate. Both theory and experiments confirm improved detection power with rigorous Type I error control. These results provide the first principled foundation for watermark detection under imperfect pseudorandomness, offering both theoretical insight and practical guidance for reliable tracing of model outputs.",
    "metadata": {
      "authors": [
        "T. Tony Cai",
        "Xiang Li",
        "Qi Long",
        "Weijie J. Su",
        "Garrett G. Wen"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.22007v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_155",
    "source": "arxiv_metadata",
    "title": "From Social Division to Cohesion with AI Message Suggestions in Online Chat Groups",
    "text": "From Social Division to Cohesion with AI Message Suggestions in Online Chat Groups Social cohesion is difficult to sustain in societies marked by opinion diversity, particularly in online communication. As large language model (LLM)-driven messaging assistance becomes increasingly embedded in these contexts, it raises critical questions about its societal impact. We present an online experiment with 557 participants who engaged in multi-round discussions on politically controversial topics while freely reconfiguring their discussion groups. In some conditions, participants received real-time message suggestions generated by an LLM, either personalized to the individual or adapted to their group context. We find that subtle shifts in linguistic style during communication, mediated by AI assistance, can scale up to reshape collective structures. While individual-focused assistance leads users to segregate into like-minded groups, relational assistance that incorporates group members' stances enhances cohesion through more receptive exchanges. These findings demonstrate that AI-mediated communication can support social cohesion in diverse groups, but outcomes critically depend on how personalization is designed.",
    "metadata": {
      "authors": [
        "Faria Huq",
        "Elijah L. Claggett",
        "Hirokazu Shirado"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21984v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_156",
    "source": "arxiv_metadata",
    "title": "Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks",
    "text": "Uncovering the Persuasive Fingerprint of LLMs in Jailbreaking Attacks Despite recent advances, Large Language Models remain vulnerable to jailbreak attacks that bypass alignment safeguards and elicit harmful outputs. While prior research has proposed various attack strategies differing in human readability and transferability, little attention has been paid to the linguistic and psychological mechanisms that may influence a model's susceptibility to such attacks. In this paper, we examine an interdisciplinary line of research that leverages foundational theories of persuasion from the social sciences to craft adversarial prompts capable of circumventing alignment constraints in LLMs. Drawing on well-established persuasive strategies, we hypothesize that LLMs, having been trained on large-scale human-generated text, may respond more compliantly to prompts with persuasive structures. Furthermore, we investigate whether LLMs themselves exhibit distinct persuasive fingerprints that emerge in their jailbreak responses. Empirical evaluations across multiple aligned LLMs reveal that persuasion-aware prompts significantly bypass safeguards, demonstrating their potential to induce jailbreak behaviors. This work underscores the importance of cross-disciplinary insight in addressing the evolving challenges of LLM safety. The code and data are available.",
    "metadata": {
      "authors": [
        "Havva Alizadeh Noughabi",
        "Julien Serbanescu",
        "Fattane Zarrinkalam",
        "Ali Dehghantanha"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21983v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_157",
    "source": "arxiv_metadata",
    "title": "Performance Trade-offs of Optimizing Small Language Models for E-Commerce",
    "text": "Performance Trade-offs of Optimizing Small Language Models for E-Commerce Large Language Models (LLMs) offer state-of-the-art performance in natural language understanding and generation tasks. However, the deployment of leading commercial models for specialized tasks, such as e-commerce, is often hindered by high computational costs, latency, and operational expenses. This paper investigates the viability of smaller, open-weight models as a resource-efficient alternative. We present a methodology for optimizing a one-billion-parameter Llama 3.2 model for multilingual e-commerce intent recognition. The model was fine-tuned using Quantized Low-Rank Adaptation (QLoRA) on a synthetically generated dataset designed to mimic real-world user queries. Subsequently, we applied post-training quantization techniques, creating GPU-optimized (GPTQ) and CPU-optimized (GGUF) versions. Our results demonstrate that the specialized 1B model achieves 99% accuracy, matching the performance of the significantly larger GPT-4.1 model. A detailed performance analysis revealed critical, hardware-dependent trade-offs: while 4-bit GPTQ reduced VRAM usage by 41%, it paradoxically slowed inference by 82% on an older GPU architecture (NVIDIA T4) due to dequantization overhead. Conversely, GGUF formats on a CPU achieved a speedup of up to 18x in inference throughput and a reduction of over 90% in RAM consumption compared to the FP16 baseline. We conclude that small, properly optimized open-weight models are not just a viable but a more suitable alternative for domain-specific applications, offering state-of-the-art accuracy at a fraction of the computational cost.",
    "metadata": {
      "authors": [
        "Josip Tomo Licardo",
        "Nikola Tankovic"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21970v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_158",
    "source": "arxiv_metadata",
    "title": "Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing",
    "text": "Parallel Sampling from Masked Diffusion Models via Conditional Independence Testing Masked diffusion models (MDMs) offer a compelling alternative to autoregressive models (ARMs) for discrete text generation because they enable parallel token sampling, rather than sequential, left-to-right generation. This means potentially much faster inference. However, effective parallel sampling faces two competing requirements: (i) simultaneously updated tokens must be conditionally independent, and (ii) updates should prioritise high-confidence predictions. These goals conflict because high-confidence predictions often cluster and depend on each other, opportunities for parallel updates. We present PUNT, a model-agnostic sampler that reconciles this trade-off. Our method identifies token dependencies and removes lower-confidence tokens from conflicting groups. This produces sets of indices for unmasking that satisfy both independence and confidence criteria. Our approach ensures improved parallel unmasking through approximate conditional independence testing. Our experiments show that PUNT delivers a superior trade-off between accuracy and compute when compared to other strong training-free baselines, especially for generation of longer sequences. On the IFEval benchmark, it achieves up to 16\\% higher accuracy over baseline methods, including sequential generation (one-by-one). These gains hold across different values of hyperparameters, mitigating the need for brittle hyperparameter tuning. Moreover, we observe that PUNT induces an emergent hierarchical generation strategy, where the model first establishes high-level paragraph structure before local refinement, suggesting a planning-like generation process that contributes to strong alignment performance.",
    "metadata": {
      "authors": [
        "Iskander Azangulov",
        "Teodora Pandeva",
        "Niranjani Prasad",
        "Javier Zazo",
        "Sushrut Karmalkar"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21961v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_159",
    "source": "arxiv_metadata",
    "title": "A Stylometric Application of Large Language Models",
    "text": "A Stylometric Application of Large Language Models We show that large language models (LLMs) can be used to distinguish the writings of different authors. Specifically, an individual GPT-2 model, trained from scratch on the works of one author, will predict held-out text from that author more accurately than held-out text from other authors. We suggest that, in this way, a model trained on one author's works embodies the unique writing style of that author. We first demonstrate our approach on books written by eight different (known) authors. We also use this approach to confirm R. P. Thompson's authorship of the well-studied 15th book of the Oz series, originally attributed to F. L. Baum.",
    "metadata": {
      "authors": [
        "Harrison F. Stropkay",
        "Jiayi Chen",
        "Mohammad J. Latifi",
        "Daniel N. Rockmore",
        "Jeremy R. Manning"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21958v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_160",
    "source": "arxiv_metadata",
    "title": "Transformer Based Linear Attention with Optimized GPU Kernel Implementation",
    "text": "Transformer Based Linear Attention with Optimized GPU Kernel Implementation The original softmax-based attention mechanism (regular attention) in the extremely successful Transformer architecture computes attention between $N$ tokens, each embedded in a $D$-dimensional head, with a time complexity of $O(N^2D)$. Given the success of Transformers, improving their runtime during both training and inference is a popular research area. One such approach is the introduction of the linear attention (LA) mechanisms, which offers a linear time complexity of $O(ND^2)$ and have demonstrated comparable accuracy to regular attention. However, LA in practice lags behind its theoretical efficiency. We propose a novel method for LA's forward and backward passes, along with a highly-optimized CUDA implementation. Our approach outperforms the state-of-the-art by 3.3 times in speed and reduces memory consumption by 3.6 times. We validate these improvements in both single-layer and end-to-end settings by training a 1.4 billion parameter language model, which demonstrates similar expressivity to regular attention on major reasoning benchmarks.",
    "metadata": {
      "authors": [
        "Armin Gerami",
        "Ramani Duraiswami"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21956v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_161",
    "source": "arxiv_metadata",
    "title": "Model-Aware Tokenizer Transfer",
    "text": "Model-Aware Tokenizer Transfer Large Language Models (LLMs) are trained to support an increasing number of languages, yet their predefined tokenizers remain a bottleneck for adapting models to lower-resource or distinct-script languages. Existing tokenizer transfer methods typically rely on semantic heuristics to initialize new embeddings, ignoring higher-layer model dynamics and limiting transfer quality. We propose Model-Aware Tokenizer Transfer (MATT), a method that incorporates model internals into the tokenizer transfer process. MATT introduces an Attention Influence Modeling (AIM) objective that distills inter-token communication patterns from a source model into a target model with a new tokenizer, providing an efficient warm-up before standard language modeling. Unlike approaches that focus solely on embedding similarity, MATT leverages attention behavior to guide embedding initialization and adaptation. Experiments across diverse linguistic settings show that MATT recovers a large fraction of the original model's performance within a few GPU hours, outperforming heuristic baselines. These results demonstrate that incorporating model-level signals offers a practical and effective path toward robust tokenizer transfer in multilingual LLMs.",
    "metadata": {
      "authors": [
        "Mykola Haltiuk",
        "Aleksander Smywiński-Pohl"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21954v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_162",
    "source": "arxiv_metadata",
    "title": "Explaining and Mitigating Crosslingual Tokenizer Inequities",
    "text": "Explaining and Mitigating Crosslingual Tokenizer Inequities The number of tokens it takes to encode parallel text in different languages is known to vary. These disparities are called token premiums. Having high token premiums leads to less throughput during training and increases costs at inference. In this paper, we show that even after controlling for dataset size, vocabulary size, and data content, monolingual tokenizers exhibit a wide range of token premiums across languages. To understand the cross-linguistic differences that cause these token premiums, we train a suite of approximately 7,000 comparable monolingual tokenizers for 97 languages, manipulating tokenization algorithm, vocabulary size, and dataset size. We measure token premiums and test for a relationship between factors such as data similarity (between tokenizer training and evaluation), vocabulary size, and pre-tokenization. We also investigate the role of language-specific features such as writing system and word length. We find that similarity between training and test data does not impact token premiums, but vocabulary size and pre-tokenization do. While simply increasing vocabulary size does not lead to reduced token premium effects, we can determine an ``optimal'' vocabulary size for each language to achieve significantly reduced token premium effects. We also train superword tokenizers which allow merges over whitespaces, and we find that they both reduce token premium effects and improve compression overall. Thus, intervening on the vocabulary size or the pre-tokenizer significantly reduces crosslingual token premium effects.",
    "metadata": {
      "authors": [
        "Catherine Arnett",
        "Tyler A. Chang",
        "Stella Biderman",
        "Benjamin K. Bergen"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21909v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_163",
    "source": "arxiv_metadata",
    "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite",
    "text": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose \"deep research\" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.",
    "metadata": {
      "authors": [
        "Jonathan Bragg",
        "Mike D'Arcy",
        "Nishant Balepur",
        "Dan Bareket",
        "Bhavana Dalvi",
        "Sergey Feldman",
        "Dany Haddad",
        "Jena D. Hwang",
        "Peter Jansen",
        "Varsha Kishore",
        "Bodhisattwa Prasad Majumder",
        "Aakanksha Naik",
        "Sigal Rahamimov",
        "Kyle Richardson",
        "Amanpreet Singh",
        "Harshit Surana",
        "Aryeh Tiktinsky",
        "Rosni Vasu",
        "Guy Wiener",
        "Chloe Anastasiades",
        "Stefan Candra",
        "Jason Dunkelberger",
        "Dan Emery",
        "Rob Evans",
        "Malachi Hamada",
        "Regan Huff",
        "Rodney Kinney",
        "Matt Latzke",
        "Jaron Lochner",
        "Ruben Lozano-Aguilera",
        "Cecile Nguyen",
        "Smita Rao",
        "Amber Tanaka",
        "Brooke Vlahos",
        "Peter Clark",
        "Doug Downey",
        "Yoav Goldberg",
        "Ashish Sabharwal",
        "Daniel S. Weld"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21652v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_164",
    "source": "arxiv_metadata",
    "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
    "text": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations Knowledge distillation is a promising approach to transfer capabilities from complex teacher models to smaller, resource-efficient student models that can be deployed easily, particularly in task-aware scenarios. However, existing methods of task-aware distillation typically require substantial quantities of data which may be unavailable or expensive to obtain in many practical scenarios. In this paper, we address this challenge by introducing a novel strategy called Counterfactual-explanation-infused Distillation CoD for few-shot task-aware knowledge distillation by systematically infusing counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs that can flip the output prediction of the teacher model with minimum perturbation. Our strategy CoD leverages these CFEs to precisely map the teacher's decision boundary with significantly fewer samples. We provide theoretical guarantees for motivating the role of CFEs in distillation, from both statistical and geometric perspectives. We mathematically show that CFEs can improve parameter estimation by providing more informative examples near the teacher's decision boundary. We also derive geometric insights on how CFEs effectively act as knowledge probes, helping the students mimic the teacher's decision boundaries more effectively than standard data. We perform experiments across various datasets and LLMs to show that CoD outperforms standard distillation approaches in few-shot regimes (as low as 8-512 samples). Notably, CoD only uses half of the original samples used by the baselines, paired with their corresponding CFEs and still improves performance.",
    "metadata": {
      "authors": [
        "Faisal Hamman",
        "Pasan Dissanayake",
        "Yanjun Fu",
        "Sanghamitra Dutta"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21631v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_165",
    "source": "arxiv_metadata",
    "title": "The Universal Landscape of Human Reasoning",
    "text": "The Universal Landscape of Human Reasoning Understanding how information is dynamically accumulated and transformed in human reasoning has long challenged cognitive psychology, philosophy, and artificial intelligence. Existing accounts, from classical logic to probabilistic models, illuminate aspects of output or individual modelling, but do not offer a unified, quantitative description of general human reasoning dynamics. To solve this, we introduce Information Flow Tracking (IF-Track), that uses large language models (LLMs) as probabilistic encoder to quantify information entropy and gain at each reasoning step. Through fine-grained analyses across diverse tasks, our method is the first successfully models the universal landscape of human reasoning behaviors within a single metric space. We show that IF-Track captures essential reasoning features, identifies systematic error patterns, and characterizes individual differences. Applied to discussion of advanced psychological theory, we first reconcile single- versus dual-process theories in IF-Track and discover the alignment of artificial and human cognition and how LLMs reshaping human reasoning process. This approach establishes a quantitative bridge between theory and measurement, offering mechanistic insights into the architecture of reasoning.",
    "metadata": {
      "authors": [
        "Qiguang Chen",
        "Jinhao Liu",
        "Libo Qin",
        "Yimeng Zhang",
        "Yihao Liang",
        "Shangxu Ren",
        "Chengyu Luan",
        "Dengyun Peng",
        "Hanjing Li",
        "Jiannan Guan",
        "Zheng Yan",
        "Jiaqi Wang",
        "Mengkang Hu",
        "Yantao Du",
        "Zhi Chen",
        "Xie Chen",
        "Wanxiang Che"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21623v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_166",
    "source": "arxiv_metadata",
    "title": "DeepAgent: A General Reasoning Agent with Scalable Toolsets",
    "text": "DeepAgent: A General Reasoning Agent with Scalable Toolsets Large reasoning models have demonstrated strong problem-solving abilities, yet real-world tasks often require external tools and long-horizon interactions. Existing agent frameworks typically follow predefined workflows, which limit autonomous and global task completion. In this paper, we introduce DeepAgent, an end-to-end deep reasoning agent that performs autonomous thinking, tool discovery, and action execution within a single, coherent reasoning process. To address the challenges of long-horizon interactions, particularly the context length explosion from multiple tool calls and the accumulation of interaction history, we introduce an autonomous memory folding mechanism that compresses past interactions into structured episodic, working, and tool memories, reducing error accumulation while preserving critical information. To teach general-purpose tool use efficiently and stably, we develop an end-to-end reinforcement learning strategy, namely ToolPO, that leverages LLM-simulated APIs and applies tool-call advantage attribution to assign fine-grained credit to the tool invocation tokens. Extensive experiments on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank, TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA, HLE), demonstrate that DeepAgent consistently outperforms baselines across both labeled-tool and open-set tool retrieval scenarios. This work takes a step toward more general and capable agents for real-world applications. The code and demo are available at https://github.com/RUC-NLPIR/DeepAgent.",
    "metadata": {
      "authors": [
        "Xiaoxi Li",
        "Wenxiang Jiao",
        "Jiarui Jin",
        "Guanting Dong",
        "Jiajie Jin",
        "Yinuo Wang",
        "Hao Wang",
        "Yutao Zhu",
        "Ji-Rong Wen",
        "Yuan Lu",
        "Zhicheng Dou"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21618v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_167",
    "source": "arxiv_metadata",
    "title": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models",
    "text": "RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models Recently, large language models (LLMs) have demonstrated outstanding reasoning capabilities on mathematical and coding tasks. However, their application to financial tasks-especially the most fundamental task of stock movement prediction-remains underexplored. We study a three-class classification problem (up, hold, down) and, by analyzing existing reasoning responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from different sources without weighing adversarial evidence, yet such counterevidence is crucial for reliable prediction. It shows that the model does not make good use of its reasoning ability to complete the task. To address this, we propose Reflective Evidence Tuning (RETuning), a cold-start method prior to reinforcement learning, to enhance prediction ability. While generating CoT, RETuning encourages dynamically constructing an analytical framework from diverse information sources, organizing and scoring evidence for price up or down based on that framework-rather than on contextual viewpoints-and finally reflecting to derive the prediction. This approach maximally aligns the model with its learned analytical framework, ensuring independent logical reasoning and reducing undue influence from context. We also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks, with long contexts (32K tokens) and over 200K samples. In addition to price and news, it incorporates analysts' opinions, quantitative reports, fundamental data, macroeconomic indicators, and similar stocks. Experiments show that RETuning successfully unlocks the model's reasoning ability in the financial domain. Inference-time scaling still works even after 6 months or on out-of-distribution stocks, since the models gain valuable insights about stock movement prediction.",
    "metadata": {
      "authors": [
        "Xueyuan Lin",
        "Cehao Yang",
        "Ye Ma",
        "Ming Li",
        "Rongjunchen Zhang",
        "Yang Ni",
        "Xiaojun Wu",
        "Chengjin Xu",
        "Jian Guo",
        "Hui Xiong"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21604v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_168",
    "source": "arxiv_metadata",
    "title": "Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research",
    "text": "Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research Deep Research systems have revolutionized how LLMs solve complex questions through iterative reasoning and evidence gathering. However, current systems remain fundamentally constrained to textual web data, overlooking the vast knowledge embedded in multimodal documents Processing such documents demands sophisticated parsing to preserve visual semantics (figures, tables, charts, and equations), intelligent chunking to maintain structural coherence, and adaptive retrieval across modalities, which are capabilities absent in existing systems. In response, we present Doc-Researcher, a unified system that bridges this gap through three integrated components: (i) deep multimodal parsing that preserves layout structure and visual semantics while creating multi-granular representations from chunk to document level, (ii) systematic retrieval architecture supporting text-only, vision-only, and hybrid paradigms with dynamic granularity selection, and (iii) iterative multi-agent workflows that decompose complex queries, progressively accumulate evidence, and synthesize comprehensive answers across documents and modalities. To enable rigorous evaluation, we introduce M4DocBench, the first benchmark for Multi-modal, Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158 expert-annotated questions with complete evidence chains across 304 documents, M4DocBench tests capabilities that existing benchmarks cannot assess. Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter than state-of-the-art baselines, validating that effective document research requires not just better retrieval, but fundamentally deep parsing that preserve multimodal integrity and support iterative research. Our work establishes a new paradigm for conducting deep research on multimodal document collections.",
    "metadata": {
      "authors": [
        "Kuicai Dong",
        "Shurui Huang",
        "Fangda Ye",
        "Wei Han",
        "Zhi Zhang",
        "Dexun Li",
        "Wenjun Li",
        "Qu Yang",
        "Gang Wang",
        "Yichao Wang",
        "Chen Zhang",
        "Yong Liu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21603v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_169",
    "source": "arxiv_metadata",
    "title": "Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist",
    "text": "Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist Lexical data collection in language documentation often contains transcription errors and undocumented borrowings that can mislead linguistic analysis. We present unsupervised anomaly detection methods to identify phonotactic inconsistencies in wordlists, applying them to a multilingual dataset of Kokborok varieties with Bangla. Using character-level and syllable-level phonotactic features, our algorithms identify potential transcription errors and borrowings. While precision and recall remain modest due to the subtle nature of these anomalies, syllable-aware features significantly outperform character-level baselines. The high-recall approach provides fieldworkers with a systematic method to flag entries requiring verification, supporting data quality improvement in low-resourced language documentation.",
    "metadata": {
      "authors": [
        "Kellen Parker van Dam",
        "Abishek Stephen"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21584v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_170",
    "source": "arxiv_metadata",
    "title": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene",
    "text": "From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene Large language models are demonstrating increasing capabilities, excelling at benchmarks once considered very difficult. As their capabilities grow, there is a need for more challenging evaluations that go beyond surface-level linguistic competence. Namely, language competence involves not only syntax and semantics but also pragmatics, i.e., understanding situational meaning as shaped by context as well as linguistic and cultural norms. To contribute to this line of research, we introduce SloPragEval and SloPragMega, the first pragmatics understanding benchmarks for Slovene that contain altogether 405 multiple-choice questions. We discuss the difficulties of translation, describe the campaign to establish a human baseline, and report pilot evaluations with LLMs. Our results indicate that current models have greatly improved in understanding nuanced language but may still fail to infer implied speaker meaning in non-literal utterances, especially those that are culture-specific. We also observe a significant gap between proprietary and open-source models. Finally, we argue that benchmarks targeting nuanced language understanding and knowledge of the target culture must be designed with care, preferably constructed from native data, and validated with human responses.",
    "metadata": {
      "authors": [
        "Mojca Brglez",
        "Špela Vintar"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21575v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_171",
    "source": "arxiv_metadata",
    "title": "ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem",
    "text": "ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem With the rapid development of (multimodal) large language model-based agents, the landscape of agentic service management has evolved from single-agent systems to multi-agent systems, and now to massive-agent ecosystems. Current massive-agent ecosystems face growing challenges, including impersonal service experiences, a lack of standardization, and untrustworthy behavior. To address these issues, we propose ColorEcosystem, a novel blueprint designed to enable personalized, standardized, and trustworthy agentic service at scale. Concretely, ColorEcosystem consists of three key components: agent carrier, agent store, and agent audit. The agent carrier provides personalized service experiences by utilizing user-specific data and creating a digital twin, while the agent store serves as a centralized, standardized platform for managing diverse agentic services. The agent audit, based on the supervision of developer and user activities, ensures the integrity and credibility of both service providers and users. Through the analysis of challenges, transitional forms, and practical considerations, the ColorEcosystem is poised to power personalized, standardized, and trustworthy agentic service across massive-agent ecosystems. Meanwhile, we have also implemented part of ColorEcosystem's functionality, and the relevant code is open-sourced at https://github.com/opas-lab/color-ecosystem.",
    "metadata": {
      "authors": [
        "Fangwen Wu",
        "Zheng Wu",
        "Jihong Wang",
        "Yunku Chen",
        "Ruiguang Pei",
        "Heyuan Huang",
        "Xin Liao",
        "Xingyu Lou",
        "Huarong Deng",
        "Zhihui Fu",
        "Weiwen Liu",
        "Zhuosheng Zhang",
        "Weinan Zhang",
        "Jun Wang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21566v2",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_172",
    "source": "arxiv_metadata",
    "title": "Are the LLMs Capable of Maintaining at Least the Language Genus?",
    "text": "Are the LLMs Capable of Maintaining at Least the Language Genus? Large Language Models (LLMs) display notable variation in multilingual behavior, yet the role of genealogical language structure in shaping this variation remains underexplored. In this paper, we investigate whether LLMs exhibit sensitivity to linguistic genera by extending prior analyses on the MultiQ dataset. We first check if models prefer to switch to genealogically related languages when prompt language fidelity is not maintained. Next, we investigate whether knowledge consistency is better preserved within than across genera. We show that genus-level effects are present but strongly conditioned by training resource availability. We further observe distinct multilingual strategies across LLMs families. Our findings suggest that LLMs encode aspects of genus-level structure, but training data imbalances remain the primary factor shaping their multilingual performance.",
    "metadata": {
      "authors": [
        "Sandra Mitrović",
        "David Kletz",
        "Ljiljana Dolamic",
        "Fabio Rinaldi"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21561v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_173",
    "source": "arxiv_metadata",
    "title": "Document Understanding, Measurement, and Manipulation Using Category Theory",
    "text": "Document Understanding, Measurement, and Manipulation Using Category Theory We apply category theory to extract multimodal document structure which leads us to develop information theoretic measures, content summarization and extension, and self-supervised improvement of large pretrained models. We first develop a mathematical representation of a document as a category of question-answer pairs. Second, we develop an orthogonalization procedure to divide the information contained in one or more documents into non-overlapping pieces. The structures extracted in the first and second steps lead us to develop methods to measure and enumerate the information contained in a document. We also build on those steps to develop new summarization techniques, as well as to develop a solution to a new problem viz. exegesis resulting in an extension of the original document. Our question-answer pair methodology enables a novel rate distortion analysis of summarization techniques. We implement our techniques using large pretrained models, and we propose a multimodal extension of our overall mathematical framework. Finally, we develop a novel self-supervised method using RLVR to improve large pretrained models using consistency constraints such as composability and closure under certain operations that stem naturally from our category theoretic framework.",
    "metadata": {
      "authors": [
        "Jared Claypoole",
        "Yunye Gong",
        "Noson S. Yanofsky",
        "Ajay Divakaran"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21553v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_174",
    "source": "arxiv_metadata",
    "title": "InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation",
    "text": "InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation Retrieval-Augmented Generation (RAG) integrates external knowledge to mitigate hallucinations, yet models often generate outputs inconsistent with retrieved content. Accurate hallucination detection requires disentangling the contributions of external context and parametric knowledge, which prior methods typically conflate. We investigate the mechanisms underlying RAG hallucinations and find they arise when later-layer FFN modules disproportionately inject parametric knowledge into the residual stream. To address this, we explore a mechanistic detection approach based on external context scores and parametric knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and attention heads and train regression-based classifiers to predict hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5, GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore, classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses, demonstrating the potential of proxy-model evaluation. Our results highlight mechanistic signals as efficient, generalizable predictors for hallucination detection in RAG systems.",
    "metadata": {
      "authors": [
        "Likun Tan",
        "Kuan-Wei Huang",
        "Joy Shi",
        "Kevin Wu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21538v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_175",
    "source": "arxiv_metadata",
    "title": "Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models",
    "text": "Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models Pretrained language models are remarkably effective in aligning with human brain responses elicited by natural language stimuli, positioning them as promising model organisms for studying language processing in the brain. However, existing approaches for both estimating and improving this brain alignment are participant-dependent and highly affected by the amount of data available per participant, hindering both generalization to new participants and population-level analyses. In this work, we address these limitations by introducing a scalable, generalizable brain-tuning method, in which we fine-tune pretrained speech language models to jointly predict fMRI responses from multiple participants. We demonstrate that the resulting brain-tuned models exhibit strong individual brain alignment while generalizing across participants. Specifically, our method leads to 1) a 5-fold decrease in the amount of fMRI data needed to predict brain data from new participants, 2) up to a 50% increase in the overall brain alignment, and 3) strong generalization to new unseen datasets. Furthermore, this multi-participant brain-tuning additionally improves downstream performance on semantic tasks, suggesting that training using brain data from multiple participants leads to more generalizable semantic representations. Taken together, these findings demonstrate a bidirectional benefit between neuroscience and AI, helping bridge the gap between the two fields. We make our code and models publicly available at https://github.com/bridge-ai-neuro/multi-brain-tuning.",
    "metadata": {
      "authors": [
        "Omer Moussa",
        "Mariya Toneva"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21520v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_176",
    "source": "arxiv_metadata",
    "title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers",
    "text": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.",
    "metadata": {
      "authors": [
        "Lorenzo Basile",
        "Valentino Maiorca",
        "Diego Doimo",
        "Francesco Locatello",
        "Alberto Cazzaniga"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21518v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_177",
    "source": "arxiv_metadata",
    "title": "Deep Literature Survey Automation with an Iterative Workflow",
    "text": "Deep Literature Survey Automation with an Iterative Workflow Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often leads to noisy retrieval, fragmented structures, and context overload, ultimately limiting survey quality. Inspired by the iterative reading process of human researchers, we propose \\ours, a framework based on recurrent outline generation, in which a planning agent incrementally retrieves, reads, and updates the outline to ensure both exploration and coherence. To provide faithful paper-level grounding, we design paper cards that distill each paper into its contributions, methods, and findings, and introduce a review-and-refine loop with visualization enhancement to improve textual flow and integrate multimodal elements such as figures and tables. Experiments on both established and emerging topics show that \\ours\\ substantially outperforms state-of-the-art baselines in content coverage, structural coherence, and citation quality, while producing more accessible and better-organized surveys. To provide a more reliable assessment of such improvements, we further introduce Survey-Arena, a pairwise benchmark that complements absolute scoring and more clearly positions machine-generated surveys relative to human-written ones. The code is available at https://github.com/HancCui/IterSurvey\\_Autosurveyv2.",
    "metadata": {
      "authors": [
        "Hongbo Zhang",
        "Han Cui",
        "Yidong Wang",
        "Yijian Tian",
        "Qi Guo",
        "Cunxiang Wang",
        "Jian Wu",
        "Chiyu Song",
        "Yue Zhang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21900v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_178",
    "source": "arxiv_metadata",
    "title": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair",
    "text": "Wisdom and Delusion of LLM Ensembles for Code Generation and Repair Today's pursuit of a single Large Language Model (LMM) for all software engineering tasks is resource-intensive and overlooks the potential benefits of complementarity, where different models contribute unique strengths. However, the degree to which coding LLMs complement each other and the best strategy for maximizing an ensemble's potential are unclear, leaving practitioners without a clear path to move beyond single-model systems. To address this gap, we empirically compare ten individual LLMs from five families, and three ensembles of these LLMs across three software engineering benchmarks covering code generation and program repair. We assess the complementarity between models and the performance gap between the best individual model and the ensembles. Next, we evaluate various selection heuristics to identify correct solutions from an ensemble's candidate pool. We find that the theoretical upperbound for an ensemble's performance can be 83% above the best single model. Our results show that consensus-based strategies for selecting solutions fall into a \"popularity trap,\" amplifying common but incorrect outputs. In contrast, a diversity-based strategy realizes up to 95% of this theoretical potential, and proves effective even in small two-model ensembles, enabling a cost-efficient way to enhance performance by leveraging multiple LLMs.",
    "metadata": {
      "authors": [
        "Fernando Vallecillos Ruiz",
        "Max Hort",
        "Leon Moonen"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21513v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_179",
    "source": "arxiv_metadata",
    "title": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization",
    "text": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization Recent advances in diffusion language models (DLMs) have presented a promising alternative to traditional autoregressive large language models (LLMs). However, DLMs still lag behind LLMs in reasoning performance, especially as the number of denoising steps decreases. Our analysis reveals that this shortcoming arises primarily from the independent generation of masked tokens across denoising steps, which fails to capture the token correlation. In this paper, we define two types of token correlation: intra-sequence correlation and inter-sequence correlation, and demonstrate that enhancing these correlations improves reasoning performance. To this end, we propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to consider the token correlation during the denoising process. More specifically, our MRO approach leverages test-time scaling, reject sampling, and reinforcement learning to directly optimize the token correlation with multiple elaborate rewards. Additionally, we introduce group step and importance sampling strategies to mitigate reward variance and enhance sampling efficiency. Through extensive experiments, we demonstrate that MRO not only improves reasoning performance but also achieves significant sampling speedups while maintaining high performance on reasoning benchmarks.",
    "metadata": {
      "authors": [
        "Chenglong Wang",
        "Yang Gan",
        "Hang Zhou",
        "Chi Hu",
        "Yongyu Mu",
        "Kai Song",
        "Murun Yang",
        "Bei Li",
        "Chunliang Zhang",
        "Tongran Liu",
        "Jingbo Zhu",
        "Zhengtao Yu",
        "Tong Xiao"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21473v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_180",
    "source": "arxiv_metadata",
    "title": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots",
    "text": "SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots Honeypots are decoy systems used for gathering valuable threat intelligence or diverting attackers away from production systems. Maximising attacker engagement is essential to their utility. However research has highlighted that context-awareness, such as the ability to respond to new attack types, systems and attacker agents, is necessary to increase engagement. Large Language Models (LLMs) have been shown as one approach to increase context awareness but suffer from several challenges including accuracy and timeliness of response time, high operational costs and data-protection issues due to cloud deployment. We propose the System-Based Attention Shell Honeypot (SBASH) framework which manages data-protection issues through the use of lightweight local LLMs. We investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and non-RAG LLMs for Linux shell commands and evaluate them using several different metrics such as response time differences, realism from human testers, and similarity to a real system calculated with Levenshtein distance, SBert, and BertScore. We show that RAG improves accuracy for untuned models while models that have been tuned via a system prompt that tells the LLM to respond like a Linux system achieve without RAG a similar accuracy as untuned with RAG, while having a slightly lower latency.",
    "metadata": {
      "authors": [
        "Adetayo Adebimpe",
        "Helmut Neukirchen",
        "Thomas Welsh"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21459v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_181",
    "source": "arxiv_metadata",
    "title": "REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring",
    "text": "REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring With the widespread adoption of wearable devices in our daily lives, the demand and appeal for remote patient monitoring have significantly increased. Most research in this field has concentrated on collecting sensor data, visualizing it, and analyzing it to detect anomalies in specific diseases such as diabetes, heart disease and depression. However, this domain has a notable gap in the aspect of human-machine interaction. This paper proposes REMONI, an autonomous REmote health MONItoring system that integrates multimodal large language models (MLLMs), the Internet of Things (IoT), and wearable devices. The system automatically and continuously collects vital signs, accelerometer data from a special wearable (such as a smartwatch), and visual data in patient video clips collected from cameras. This data is processed by an anomaly detection module, which includes a fall detection model and algorithms to identify and alert caregivers of the patient's emergency conditions. A distinctive feature of our proposed system is the natural language processing component, developed with MLLMs capable of detecting and recognizing a patient's activity and emotion while responding to healthcare worker's inquiries. Additionally, prompt engineering is employed to integrate all patient information seamlessly. As a result, doctors and nurses can access real-time vital signs and the patient's current state and mood by interacting with an intelligent agent through a user-friendly web application. Our experiments demonstrate that our system is implementable and scalable for real-life scenarios, potentially reducing the workload of medical professionals and healthcare costs. A full-fledged prototype illustrating the functionalities of the system has been developed and being tested to demonstrate the robustness of its various capabilities.",
    "metadata": {
      "authors": [
        "Thanh Cong Ho",
        "Farah Kharrat",
        "Abderrazek Abid",
        "Fakhri Karray"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21445v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_182",
    "source": "arxiv_metadata",
    "title": "Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification",
    "text": "Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification [Context and motivation] Large language models (LLMs) show notable results in natural language processing (NLP) tasks for requirements engineering (RE). However, their use is compromised by high computational cost, data sharing risks, and dependence on external services. In contrast, small language models (SLMs) offer a lightweight, locally deployable alternative. [Question/problem] It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms of accuracy. [Results] Our preliminary study compares eight models, including three LLMs and five SLMs, on requirements classification tasks using the PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not statistically significant. SLMs almost reach LLMs performance across all datasets and even outperform them in recall on the PROMISE Reclass dataset, despite being up to 300 times smaller. We also found that dataset characteristics play a more significant role in performance than model size. [Contribution] Our study contributes with evidence that SLMs are a valid alternative to LLMs for requirements classification, offering advantages in privacy, cost, and local deployability.",
    "metadata": {
      "authors": [
        "Mohammad Amin Zadenoori",
        "Vincenzo De Martino",
        "Jacek Dabrowski",
        "Xavier Franch",
        "Alessio Ferrari"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21443v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_183",
    "source": "arxiv_metadata",
    "title": "Redefining Retrieval Evaluation in the Era of LLMs",
    "text": "Redefining Retrieval Evaluation in the Era of LLMs Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR, assume that human users sequentially examine documents with diminishing attention to lower ranks. This assumption breaks down in Retrieval Augmented Generation (RAG) systems, where search results are consumed by Large Language Models (LLMs), which, unlike humans, process all retrieved documents as a whole rather than sequentially. Additionally, traditional IR metrics do not account for related but irrelevant documents that actively degrade generation quality, rather than merely being ignored. Due to these two major misalignments, namely human vs. machine position discount and human relevance vs. machine utility, classical IR metrics do not accurately predict RAG performance. We introduce a utility-based annotation schema that quantifies both the positive contribution of relevant passages and the negative impact of distracting ones. Building on this foundation, we propose UDCG (Utility and Distraction-aware Cumulative Gain), a metric using an LLM-oriented positional discount to directly optimize the correlation with the end-to-end answer accuracy. Experiments on five datasets and six LLMs demonstrate that UDCG improves correlation by up to 36% compared to traditional metrics. Our work provides a critical step toward aligning IR evaluation with LLM consumers and enables more reliable assessment of RAG components",
    "metadata": {
      "authors": [
        "Giovanni Trappolini",
        "Florin Cuconasu",
        "Simone Filice",
        "Yoelle Maarek",
        "Fabrizio Silvestri"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21440v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_184",
    "source": "arxiv_metadata",
    "title": "Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings",
    "text": "Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings As generative AI continues to evolve, Vision Language Models (VLMs) have emerged as promising tools in various healthcare applications. One area that remains relatively underexplored is their use in human activity recognition (HAR) for remote health monitoring. VLMs offer notable strengths, including greater flexibility and the ability to overcome some of the constraints of traditional deep learning models. However, a key challenge in applying VLMs to HAR lies in the difficulty of evaluating their dynamic and often non-deterministic outputs. To address this gap, we introduce a descriptive caption data set and propose comprehensive evaluation methods to evaluate VLMs in HAR. Through comparative experiments with state-of-the-art deep learning models, our findings demonstrate that VLMs achieve comparable performance and, in some cases, even surpass conventional approaches in terms of accuracy. This work contributes a strong benchmark and opens new possibilities for the integration of VLMs into intelligent healthcare systems.",
    "metadata": {
      "authors": [
        "Abderrazek Abid",
        "Thanh-Cong Ho",
        "Fakhri Karray"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21424v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_185",
    "source": "arxiv_metadata",
    "title": "HalleluBERT: Let every token that has meaning bear its weight",
    "text": "HalleluBERT: Let every token that has meaning bear its weight Transformer-based models have advanced NLP, yet Hebrew still lacks a large-scale RoBERTa encoder which is extensively trained. Existing models such as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or training depth. We present HalleluBERT, a RoBERTa-based encoder family (base and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER and sentiment classification benchmarks, HalleluBERT outperforms both monolingual and multilingual baselines. HalleluBERT sets a new state of the art for Hebrew and highlights the benefits of fully converged monolingual pretraining.",
    "metadata": {
      "authors": [
        "Raphael Scheible-Schmitt"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21372v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_186",
    "source": "arxiv_metadata",
    "title": "HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences",
    "text": "HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences HIKMA Semi-Autonomous Conference is the first experiment in reimagining scholarly communication through an end-to-end integration of artificial intelligence into the academic publishing and presentation pipeline. This paper presents the design, implementation, and evaluation of the HIKMA framework, which includes AI dataset curation, AI-based manuscript generation, AI-assisted peer review, AI-driven revision, AI conference presentation, and AI archival dissemination. By combining language models, structured research workflows, and domain safeguards, HIKMA shows how AI can support - not replace traditional scholarly practices while maintaining intellectual property protection, transparency, and integrity. The conference functions as a testbed and proof of concept, providing insights into the opportunities and challenges of AI-enabled scholarship. It also examines questions about AI authorship, accountability, and the role of human-AI collaboration in research.",
    "metadata": {
      "authors": [
        "Zain Ul Abideen Tariq",
        "Mahmood Al-Zubaidi",
        "Uzair Shah",
        "Marco Agus",
        "Mowafa Househ"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21370v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_187",
    "source": "arxiv_metadata",
    "title": "SindBERT, the Sailor: Charting the Seas of Turkish NLP",
    "text": "SindBERT, the Sailor: Charting the Seas of Turkish NLP Transformer models have revolutionized NLP, yet many morphologically rich languages remain underrepresented in large-scale pre-training efforts. With SindBERT, we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base and large configurations, representing the first large-scale encoder-only language model available for Turkish. We evaluate SindBERT on part-of-speech tagging, named entity recognition, offensive language detection, and the TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT performs competitively with existing Turkish and multilingual models, with the large variant achieving the best scores in two of four tasks but showing no consistent scaling advantage overall. This flat scaling trend, also observed for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be saturated. At the same time, comparisons with smaller but more curated models such as BERTurk highlight that corpus quality and diversity can outweigh sheer data volume. Taken together, SindBERT contributes both as an openly released resource for Turkish NLP and as an empirical case study on the limits of scaling and the central role of corpus composition in morphologically rich languages. The SindBERT models are released under the MIT license and made available in both fairseq and Huggingface formats.",
    "metadata": {
      "authors": [
        "Raphael Scheible-Schmitt",
        "Stefan Schweter"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21364v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_188",
    "source": "arxiv_metadata",
    "title": "FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models",
    "text": "FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models Text-to-image diffusion models, such as Stable Diffusion, have demonstrated remarkable capabilities in generating high-quality and diverse images from natural language prompts. However, recent studies reveal that these models often replicate and amplify societal biases, particularly along demographic attributes like gender and race. In this paper, we introduce FairImagen (https://github.com/fuzihaofzh/FairImagen), a post-hoc debiasing framework that operates on prompt embeddings to mitigate such biases without retraining or modifying the underlying diffusion model. Our method integrates Fair Principal Component Analysis to project CLIP-based input embeddings into a subspace that minimizes group-specific information while preserving semantic content. We further enhance debiasing effectiveness through empirical noise injection and propose a unified cross-demographic projection method that enables simultaneous debiasing across multiple demographic attributes. Extensive experiments across gender, race, and intersectional settings demonstrate that FairImagen significantly improves fairness with a moderate trade-off in image quality and prompt fidelity. Our framework outperforms existing post-hoc methods and offers a simple, scalable, and model-agnostic solution for equitable text-to-image generation.",
    "metadata": {
      "authors": [
        "Zihao Fu",
        "Ryan Brown",
        "Shun Shao",
        "Kai Rawal",
        "Eoin Delaney",
        "Chris Russell"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21363v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_189",
    "source": "arxiv_metadata",
    "title": "A Diagnostic Benchmark for Sweden-Related Factual Knowledge",
    "text": "A Diagnostic Benchmark for Sweden-Related Factual Knowledge Many Swedish benchmarks are translated US-centric benchmarks, and therefore not suitable for testing knowledge that is particularly relevant, or even specific, to Sweden. We therefore introduce a manually written question-answering benchmark specifically targeted to Sweden-related personalities and events, many of which receive very limited coverage in international media. Our annotators drew inspiration from a popular radio program featuring public figures from culture and media, as well as major sports events in Sweden. The dataset can be used to measure factual recall across models of varying sizes and degrees of Swedish coverage, and allows to probe cross-lingual factual consistency as to contains English translations. Using the dataset, we find that smaller models with stronger Swedish coverage perform comparably to a three times larger multilingual model in recalling Sweden-related facts. We also observe that continued pre-training on Swedish generally improves factual knowledge but also leads to forgetting of a part of the previously known information. These results demonstrate the dataset's potential as a diagnostic tool for studying language adaptation and knowledge retention in multilingual models and during language adaptation.",
    "metadata": {
      "authors": [
        "Jenny Kunz"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21360v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_190",
    "source": "arxiv_metadata",
    "title": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation",
    "text": "Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation Large Language Models (LLMs) often struggle with generating truly innovative ideas, typically defaulting to high-probability, familiar concepts within their training data's \"gravity wells.\" While advanced search-based methods like Tree of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by their reliance on unprincipled, inconsistent self-evaluation heuristics to guide exploration. To address this gap, we introduce \\textbf{Magellan}, a novel framework that reframes creative generation as a principled, guided exploration of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo Tree Search (MCTS) governed by a hierarchical guidance system. For long-range direction, a \"semantic compass\" vector, formulated via orthogonal projection, steers the search towards relevant novelty. For local, step-by-step decisions, a landscape-aware value function replaces flawed self-evaluation with an explicit reward structure that balances intrinsic coherence, extrinsic novelty, and narrative progress. Extensive experiments demonstrate that Magellan significantly outperforms strong baselines, including ReAct and ToT, in generating scientific ideas with superior plausibility and innovation. Our work shows that for creative discovery, a principled, guided search is more effective than unconstrained agency, paving the way for LLMs to become more capable partners in innovation.",
    "metadata": {
      "authors": [
        "Lufan Chang"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21341v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_191",
    "source": "arxiv_metadata",
    "title": "Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning",
    "text": "Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning The reasoning capabilities of Large Language Models (LLMs) are typically developed through the single-turn reinforcement learning, whereas real-world applications often involve multi-turn interactions with human feedback, leading to a potential mismatch between training and deployment conditions. In this work, we study whether multi-turn training with human feedback is necessary for reasoning tasks. We compare conventional single-turn training with three multi-turn strategies and reach contrary conclusions to previous research. We find that models trained in a single-turn setting generalize effectively to both single- and multi-turn evaluations, while models trained with multi-turn strategies exhibit a significant degradation in single-turn reasoning performance. These results suggest that for tasks with complete information, robust single-turn training remains more effective and reliable, as multi-turn training with basic feedback provides limited benefits and can even degrade reasoning capabilities.",
    "metadata": {
      "authors": [
        "Qiang Liu",
        "Wuganjing Song",
        "Zhenzhou Lin",
        "Feifan Chen",
        "Qiaolong Cai",
        "Chen Li",
        "Yongduo Sui"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21339v2",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_192",
    "source": "arxiv_metadata",
    "title": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions",
    "text": "TripTide: A Benchmark for Adaptive Travel Planning under Disruptions Recent efforts like TripCraft and TravelPlanner have advanced the use of Large Language Models ( LLMs) for personalized, constraint aware travel itinerary generation. Yet, real travel often faces disruptions. To address this, we present TripTide, the first benchmark evaluating LLM's ability to revise itineraries under realistic disruptions. TripTide models key dimensions such as disruption severity and traveler tolerance, enabling nuanced assessment of LLM adaptability to events like flight cancellations, weather closures, or overbooked attractions. We conduct a threefold evaluation. First, we introduce automatic metrics including Preservation of Intent (how well the revised plan maintains feasibility and goals), Responsiveness (promptness and appropriateness of disruption handling), and Adaptability (semantic, spatial, and sequential divergence between original and revised plans). Second, we apply an LLM-as-a-judge approach to automatically assess revision quality. Third, we perform manual expert evaluation to verify whether revisions preserve semantic, spatial, sequential, and responsive aspects. Our experiments show that LLMs maintain strong sequential consistency and semantic stability, while spatial deviations are larger for shorter trips but decrease with longer ones, indicating that extended plans encourage better geographic coherence. However, disruption-handling ability declines as plan length increases, highlighting limits in LLM robustness. TripTide establishes a benchmark for evaluating adaptability, personalization, and resilience in LLM-based travel planning under real-world uncertainty.",
    "metadata": {
      "authors": [
        "Priyanshu Karmakar",
        "Soumyabrata Chaudhuri",
        "Shubhojit Mallick",
        "Manish Gupta",
        "Abhik Jana",
        "Shreya Ghosh"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21329v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_193",
    "source": "arxiv_metadata",
    "title": "Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words",
    "text": "Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words Research in linguistics has shown that humans can read words with internally scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP models have recently been proposed that similarly demonstrate robustness to such distortions by ignoring the internal order of characters by design. This raises a fundamental question: how can models perform well when many distinct words (e.g., form and from) collapse into identical representations under typoglycemia? Our work, focusing exclusively on the English language, seeks to shed light on the underlying aspects responsible for this robustness. We hypothesize that the main reasons have to do with the fact that (i) relatively few English words collapse under typoglycemia, and that (ii) collapsed words tend to occur in contexts so distinct that disambiguation becomes trivial. In our analysis, we (i) analyze the British National Corpus to quantify word collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to disambiguate collapsing forms, and (iii) conduct a probing experiment by comparing variants of BERT trained from scratch on clean versus typoglycemic Wikipedia text; our results reveal that the performance degradation caused by scrambling is smaller than expected.",
    "metadata": {
      "authors": [
        "Gianluca Sperduti",
        "Alejandro Moreo"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21326v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_194",
    "source": "arxiv_metadata",
    "title": "Leverage Unlearning to Sanitize LLMs",
    "text": "Leverage Unlearning to Sanitize LLMs Pre-trained large language models (LLMs) are becoming useful for various tasks. To improve their performance on certain tasks, it is necessary to fine-tune them on specific data corpora (e.g., medical reports, business data). These specialized data corpora may contain sensitive data (e.g., personal or confidential data) that will be memorized by the model and likely to be regurgitated during its subsequent use. This memorization of sensitive information by the model poses a significant privacy or confidentiality issue. To remove this memorization and sanitize the model without requiring costly additional fine-tuning on a secured data corpus, we propose SANI. SANI is an unlearning approach to sanitize language models. It relies on both an erasure and repair phases that 1) reset certain neurons in the last layers of the model to disrupt the memorization of fine-grained information, and then 2) fine-tune the model while avoiding memorizing sensitive information. We comprehensively evaluate SANI to sanitize both a model fine-tuned and specialized with medical data by removing directly and indirectly identifiers from the memorization of the model, and a standard pre-trained model by removing specific terms defined as confidential information from the model. Results show that with only few additional epochs of unlearning, the model is sanitized and the number of regurgitations is drastically reduced. This approach can be particularly useful for hospitals or other industries that have already spent significant resources training models on large datasets and wish to sanitize them before sharing.",
    "metadata": {
      "authors": [
        "Antoine Boutet",
        "Lucas Magnana"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21322v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_195",
    "source": "arxiv_metadata",
    "title": "Efficient semantic uncertainty quantification in language models via diversity-steered sampling",
    "text": "Efficient semantic uncertainty quantification in language models via diversity-steered sampling Accurately estimating semantic aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a diversity-steered sampler that discourages semantically redundant outputs during decoding, covers both autoregressive and masked diffusion paradigms, and yields substantial sample-efficiency gains. The key idea is to inject a continuous semantic-similarity penalty into the model's proposal distribution using a natural language inference (NLI) model lightly finetuned on partial prefixes or intermediate diffusion states. We debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates. Across four QA benchmarks, our method matches or surpasses baselines while covering more semantic clusters with the same number of samples. Being modular and requiring no gradient access to the base LLM, the framework promises to serve as a drop-in enhancement for uncertainty estimation in risk-sensitive model deployments.",
    "metadata": {
      "authors": [
        "Ji Won Park",
        "Kyunghyun Cho"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21310v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_196",
    "source": "arxiv_metadata",
    "title": "PARL: Prompt-based Agents for Reinforcement Learning",
    "text": "PARL: Prompt-based Agents for Reinforcement Learning Large language models (LLMs) have demonstrated high performance on tasks expressed in natural language, particularly in zero- or few-shot settings. These are typically framed as supervised (e.g., classification) or unsupervised (e.g., clustering) problems. However, limited work evaluates LLMs as agents in reinforcement learning (RL) tasks (e.g., playing games), where learning occurs through interaction with an environment and a reward system. While prior work focused on representing tasks that rely on a language representation, we study structured, non-linguistic reasoning - such as interpreting positions in a grid world. We therefore introduce PARL (Prompt-based Agent for Reinforcement Learning), a method that uses LLMs as RL agents through prompting, without any fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling the model to learn through trial-and-error interaction. We evaluate PARL on three standard RL tasks that do not entirely rely on natural language. We show that it can match or outperform traditional RL agents in simple environments by leveraging pretrained knowledge. However, we identify performance limitations in tasks that require complex mathematical operations or decoding states and actions.",
    "metadata": {
      "authors": [
        "Yarik Menchaca Resendiz",
        "Roman Klinger"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21306v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_197",
    "source": "arxiv_metadata",
    "title": "When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails",
    "text": "When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex reasoning tasks but remain vulnerable to severe safety risks, including harmful content generation and jailbreak attacks. Existing mitigation strategies rely on injecting heuristic safety signals during training, which often suppress reasoning ability and fail to resolve the safety-reasoning trade-off. To systematically investigate this issue, we analyze the reasoning trajectories of diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models override their own risk assessments and justify responding to unsafe prompts. This finding reveals that LRMs inherently possess the ability to reject unsafe queries, but this ability is compromised, resulting in harmful outputs. Building on these insights, we propose the Chain-of-Guardrail (CoG), a training framework that recomposes or backtracks unsafe reasoning steps, steering the model back onto safe trajectories while preserving valid reasoning chains. Extensive experiments across multiple reasoning and safety benchmarks demonstrate that CoG substantially improves the safety of current LRMs while preserving comparable reasoning ability, significantly outperforming prior methods that suffer from severe safety-reasoning trade-offs.",
    "metadata": {
      "authors": [
        "Yingzhi Mao",
        "Chunkang Zhang",
        "Junxiang Wang",
        "Xinyan Guan",
        "Boxi Cao",
        "Yaojie Lu",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21285v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_198",
    "source": "arxiv_metadata",
    "title": "Pctx: Tokenizing Personalized Context for Generative Recommendation",
    "text": "Pctx: Tokenizing Personalized Context for Generative Recommendation Generative recommendation (GR) models tokenize each action into a few discrete tokens (called semantic IDs) and autoregressively generate the next tokens as predictions, showing advantages such as memory efficiency, scalability, and the potential to unify retrieval and ranking. Despite these benefits, existing tokenization methods are static and non-personalized. They typically derive semantic IDs solely from item features, assuming a universal item similarity that overlooks user-specific perspectives. However, under the autoregressive paradigm, semantic IDs with the same prefixes always receive similar probabilities, so a single fixed mapping implicitly enforces a universal item similarity standard across all users. In practice, the same item may be interpreted differently depending on user intentions and preferences. To address this issue, we propose a personalized context-aware tokenizer that incorporates a user's historical interactions when generating semantic IDs. This design allows the same item to be tokenized into different semantic IDs under different user contexts, enabling GR models to capture multiple interpretive standards and produce more personalized predictions. Experiments on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over non-personalized action tokenization baselines. Our code is available at https://github.com/YoungZ365/Pctx.",
    "metadata": {
      "authors": [
        "Qiyong Zhong",
        "Jiajie Su",
        "Yunshan Ma",
        "Julian McAuley",
        "Yupeng Hou"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21276v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "arxiv_199",
    "source": "arxiv_metadata",
    "title": "Sparser Block-Sparse Attention via Token Permutation",
    "text": "Sparser Block-Sparse Attention via Token Permutation Scaling the context length of large language models (LLMs) offers significant benefits but is computationally expensive. This expense stems primarily from the self-attention mechanism, whose $O(N^2)$ complexity with respect to sequence length presents a major bottleneck for both memory and latency. Fortunately, the attention matrix is often sparse, particularly for long sequences, suggesting an opportunity for optimization. Block-sparse attention has emerged as a promising solution that partitions sequences into blocks and skips computation for a subset of these blocks. However, the effectiveness of this method is highly dependent on the underlying attention patterns, which can lead to sub-optimal block-level sparsity. For instance, important key tokens for queries within a single block may be scattered across numerous other blocks, leading to computational redundancy. In this work, we propose Permuted Block-Sparse Attention (\\textbf{PBS-Attn}), a plug-and-play method that leverages the permutation properties of attention to increase block-level sparsity and enhance the computational efficiency of LLM prefilling. We conduct comprehensive experiments on challenging real-world long-context datasets, demonstrating that PBS-Attn consistently outperforms existing block-sparse attention methods in model accuracy and closely matches the full attention baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn achieves an end-to-end speedup of up to $2.75\\times$ in long-context prefilling, confirming its practical viability. Code available at https://github.com/xinghaow99/pbs-attn",
    "metadata": {
      "authors": [
        "Xinghao Wang",
        "Pengyu Wang",
        "Dong Zhang",
        "Chenkun Tan",
        "Shaojun Zhou",
        "Zhaoxiang Liu",
        "Shiguo Lian",
        "Fangxu Liu",
        "Kai Song",
        "Xipeng Qiu"
      ],
      "published": "",
      "url": "http://arxiv.org/abs/2510.21270v1",
      "categories": [],
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_01",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_01",
    "text": "We are so excited to be launching the 2025 Masterclasses and 2026 Conference start of the 2026 Conference. F Masterclasses Per year recordings community forum NLP Festivals * Regional and international 1 day event Enjoy community and connection Exclusive, accessible, low cost WLP International co’ Karen Falconer presentations we are so excited to be launching the 2025 master classes and 2026 conference start of the 2026 conference so we've So we've got new presenters this year including Sir Ed Colcan Car, Isaiah Gibbons and Nikki Emerton. got new presenters this year including suren Co kenar kiah Gibbons and Nikki Emon we've got some favorites back and We've got some favourites back and confirmed for 2026 Conference on Michael Hall. confirmed for 2026 conference are Michael Hall Sue Knight and a beautiful Soonites, and a beautiful combination of Nishit Shah and Judith Lowe. combination of Nish and Judith low we've Internationa P Event 2026 NLP CONFERENCE PRESENTERS CONFIRMED SO FAR... Sue Knight Nishith Shah & SOON Lindsey Agness Emma McNally Juidth Lowe & Sophie Baker Fiona Campbell Joanna Ellis Michael Hall Joanna Harper Reb Veale We've got fresh innovative new topics for you over the next couple of years including the groundic model from Jairna Harpa, got fresh Innovative new topics for you over the next couple of years including the granded model from Joanna Harper emotional identity techniques and living life in 4D. emotional identity techniques and living life in 4D and of course this all starts And of course this all starts in February 2025 with Richard Bolster presenting a six hour masterclass on the creation cycle. in February 2025 with Richard bolad presenting a six-hour master class on the creation cycle so how do you access all these So how do you access all these fresh innovative and new presentations? fresh Innovative and new presentations A Oac Mas terclasses Per year to recordings NLP Festivals gional and international 1 day event oy community and connection xclusive, accessible, low cost International Con se ns karen You can choose individual masterclasses and just cherry pick those that you want. you can choose individual master classes and just cherry-pick those that you want You can save money by opting for the masterclass collection. you can save money by opting for the masterclass collection and if you're And if you're intending to come to the conference in person in 2026 then that is by far the best option because included in your conference ticket are all the masterclasses for 2025 and 2026. intending to come to the conference in person in 2026 then that is by far the best option because included in your conference ticket are all the master classes for 2025 and 2026 you can even You can even spread the cost of your conference ticket if you apply before the 31st of December. spread the cost of your conference ticket if if you apply before the 31st Festivals ! and international 1 day event and connection low cost Int conf of December and in case you're And in case you're interested, here's some feedback from the 2024 Conference. interested here's some feedback from the 2024 conference I have had the best time I have had the best time so to take a weekend out of your week when you're really busy on the first big scenes like a lot. so to take a weekend out um of your week when you're really busy on the Facebook seems like a lot but I'm so glad I've But I'm so glad I've come to this conference. I've met so many amazing people. come to this conference I've met so many amazing people and when I heard Karen I'm going to hear Karen say that next year there's going to be 10 masterclasses so you get to go to one like every month or 10 months. say that next year there's going to be 10 master classes so you get to go to one like every month for 10 months it'll It'll really help embed that learning and keep connected. really help embed that learning and keep connected so much content so many great So much content, so many great people, wonderful presenters. people wonderful presenters I can't wait I can't wait to catch up with all the videos of the one that haven't been able to get to one person. to catch up with all the videos of the ones I haven't been able to get to in person so yeah thanks they and I'll be So yeah, thanks there and I'll be another roaring success from my point of view. another roaring success from my point of view so with so many options available So with so many options available to you, which one are you going to choose? to you which one are you going to choose Visit the website, go to upcoming events and choose which ever options you want to go for. visit the website go to upcoming events and choose whichever options you want to go for and remember the festivals too in And remember the festivals too in 2024 and in 2025. 2024 and in LPM rclasses per year nmunity forum > NLP Festivals and international 1 day event joy community and connection ive, accessible, low cost onal Conf entations Internati karen 2025 we really look forward to welcoming We really look forward to welcoming you to one of our events over the next couple of years. you to one of our events over the next couple of years",
    "metadata": {
      "segments": 86,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_02",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_02",
    "text": "This is just a short message to say congratulations. You are in as a presenter for the in-person conference next May. this is just a short message to say congratulations you are in as a presenter for the in-person conference next May there were over a hundred There were over a hundred proposals and put in for eight slots at the in-person conference. So well done. You have one of those slots. proposals um put in for eight slots at the in-person conference so well done you have one of those slots we will be We will be sending a follow-up email with all the details including the times and all that sort of thing and what will be required from you in terms of photos and all that sort of thing. sending a follow-up email with all the details including the times and all that sort of thing and what will be required from you in terms of uh photos and all national ference that sort of thing I would say please do I would say please do read all the emails we send you from now on. They do contain important information and we did end up with a couple of presenters this year at the in-person conference being quite surprised by what they found when they arrived at the conference in terms of table layouts and things like that. read all the emails we send you from now on they do contain important information and we did end up with a couple of presenters this year at the in-person conference being quite surprised by what they found when they arrived at the conference in terms of table layouts and things like that because they hadn't Because they hadn't read the emails, please understand that every email we send does contain important information at the time into your presentation. read the emails please understand that every email we send does contain important information at pertaining to your presentation so do read the emails So do read the emails and the first email that will be coming out will be inviting you to book a chat with me so we can talk more about your presentation for social media purposes. and the first email that will be coming out will be inviting you to book a chat with me so we can talk more about your presentation for social media purposes so looking forward to talking to you So looking forward to talking to you soon and congratulations. soon and congratulations",
    "metadata": {
      "segments": 38,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_03",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_03",
    "text": "P International nference This is just a quick email to say congratulations, you have made it. You are going to be presenting this is just a quick email to say congratulations you have made it you are going to be presenting at the virtual at the virtual conference in February. Thank you so much for your proposal. We received over conference in February uh thank you so much for your proposal we received over a hundred proposals altogether, so to get one of those coveted slots is really brilliant, a hundred proposals altogether so to get one of those coveted slots is really brilliant well done and thank you so well done and thank you. So just let you know we will be following up with an email confirming the just to let you know we will be following up with an email confirming the date and the time of your date and the time of your presentation and if you submitted more than one application then we'll presentation and if you've submitted more than one application then we'll be be confirming which presentation we've accepted. We will also be inviting you to come and have a confirming which presentation we've accepted um we will also be inviting you to come and have a chat with us in October if chat with us in October, have a chat with me so that we can talk more about your presentation and you have a chat with me so that we can talk more about your presentation and then we can use that for social media purposes. Please do read every email that we send you because then we can use that for social media purposes please do read every email that we send you because they always contain always contain important information, appertaining to your presentation and we have had people in important information pertaining to your presentation and we have had people in the past turn up at their presentations quite surprised because things aren't set out exactly how the past turn up at their presentations quite surprised because things aren't set out exactly how they were thinking they were thinking they were going to be. So those emails are important and they will help you get they were going to be so those emails are important and they will help you get the best out of your presentation at the conference as well as allow us to put on the most professional the best out of your presentation at the conference as well as allow us to put on the most professional show that we shows that we possibly can. So well done, congratulations and I look forward to talking to you in October. possibly can so well done congratulations and I look forward to talking to you in October",
    "metadata": {
      "segments": 49,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_04",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_04",
    "text": "Welcome to the 2024 NLP International Conference, com to aren Falconer welcome to the 2024 NLP International Conference taking place both virtually Haking place both virtually and at the Sheraton Skyline Hotel in Heathrow. and at the Sheran Skyline Hotel in Heathrow we are so excited to be doing a We are so excited to be doing a pre-conference visit at the Sheraton Skyline here in Heathrow. pre-conference visit at the Sheran Skyline here here in heo and this is our And this is our venue space for next year. venue space for next year and this is And this is where we will be holding all the drinks and social and community elements of the conference. where we will be holding all the drinks and social and Community elements of the conference we have a fantastic lineup We have a fantastic lineup for the in-person element of the conference, Karen Falconer for the in-person element of the conference which is taking place on the which is taking place on the 11th and 12th of May 2024, 11th and 12th of May 20124 including our amazing masterclass including our amazing masterclass presenters, presenters Penny Tomkins and James ly Penny Tonkins and James Lawley, and an array of other amazing presenters and an array of other amazing presenters who will be delivering their presentations on the Sunday. who will be delivering their presentations on the Sunday in the In the change to our 2023 event, change to our 20123 event our virtual our virtual weekend in 2024 will be held on the 10th and 11th weekend in 2024 will be held on the 10th and 11th of Fe February with multiple a Bak-Maier & r & Jan Cisek Richard Bolstad rd Grey with multiple streams running simultaneously and again an amazing array of presenters over the two days. streams running simultaneously and again an amazing array of presenters over the two days we also have a superb lineup of We also have a superb lineup of exhibitors who you will be able to meet at both the virtual and in-person parts of the conference. exhibitors who you will be able to meet at both the virtual and inperson parts of the conference of course feedback is really Of course, feedback is really important, important so rather than listen to what so rather than listen to what we have to say, we have to say how about hearing from how about hearing from some of the other delegates from the 2023 event? some of the other delegates from the 2023 event the whole event has run Before we're being first run, beautifully. beautifully and I have learned so much and I always And I have learned so much. And I always love the fact that we do even we were so much. love the fact that you know you away with so much I wanted to connect and hug I wanted to connect to the conference everyone. everyone I'm a real hugger you are and I'm a real hugger. You are? And yeah, I just think it's really nice to back in the room with everyone. um yeah I just think it's really nice to back in the room with everyone it's It's wonderful isn't it? wonderful isn't it real energy Wonder We are emigrating back to our recovery. Honestly, I'm really enjoying it. honestly I'm really enjoying it it's the It's the energy is so lovely. energy is so lovely everybody's so Everybody's so supportive. supportive so we are so looking forward So we are so looking forward to seeing you here next May 2024 for the NLP International Conference. to seeing you here next May 2024 for the NLP International Conference",
    "metadata": {
      "segments": 81,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_05",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_05",
    "text": "Of course, feedback is really important, so rather than listen to what we have to say, tearning and the most from opportunities collaborations at the conference. a at 0 Reb Veale f the NLP nity and of course feedback is really important so rather than listen to what we have to say how about hearing from some of the how about hearing some of the other delegates from the 2023 event? other delegates from the 2023 event the Before we've been as one, beautifully, and I have learned so much, whole event has run beautifully and I have learned so much and I always loved and I always loved the fact that we were so much. the fact that you know you away with so much I wanted to connect and hug I wanted to connect to the club everyone. everyone I'm a real hugger you are and I'm a real hugger. You are? Yeah, I just think it's really nice to back in the room with everyone. um yeah I just think it's really nice to back in the room with everyone it's It's wonderful, it's like, we are doing much more of the creation. wonderful Honestly, I'm really enjoying it. honestly I'm really enjoying it it's the It's the energy is so lovely, everybody's so supportive. energy is so lovely everybody's so supportive so we are so looking forward So we are so looking forward to seeing you here next May 2024 for the NLP International Conference. to seeing you here next May 2024 for the NLP International Conference",
    "metadata": {
      "segments": 31,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_06",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_06",
    "text": "Hi, this is a quick video to show delegates of the NLP International Conference 2024, how hi this is a quick video to show delegates of the NLP International Conference 2024 how to log to Hoover on the website to log into Hoover on the website, the Hoover website. So here in the top left of my screen the Hoover website so here in the top left of my screen is the is the Hoover URL for the web application. If you key that in, that will bring up this screen, Hoover URL for the web application if you key that in will Sign in using your Whova account Two events - One Conference Cor org Virtual Weekend - 10th & 11th February 2024 In-Person Weekend - 11th & 12th May 2024 Dont have an account? to search bring up this screen and you'll have the and you'll have the opportunity to fill in your email address and password if you have an account. opportunity to fill in your email address and password if you have an account if you don't have an account you If you don't have an account, you need to register and sign up here, but it says sign up here. It's need to register and sign up here where it says sign up here it's a two- minute thing and you must use in all cases the email address that you used to purchase to purchase your NLP 2024 International Conference ticket with on the website. It's your NLP 2024 International Conference ticket with on the website it's vitally vitally important you do that so that we can tie up all the records. You'll notice, first of all, important you do that so that we can can Sign in using your Whova account Two events - One Conference ord Virtual Weekend - 10th & 11th February 2024 Password * In-Person Weekend - 11th & 12th May 2024 k tie up all the records you'll notice first of all as as well that I'm using the Chrome browser because this is the browser that gives you the best well that I'm using the Chrome browser because this is the browser that gives you the best uh facility to use Hoover facility to use Hoover to view the live streaming videos and to watch the recordings as well. to view the live streaming videos and to watch the recordings as well this will This will give you the access to the recording six months after. So once you've set up an account, give you the access to the recordings six months after so once you've set up an account or if you already have an or if you already have an account, you key in your email address and the password and simply account you key in your email address and the password and simply K click sign Inte Cor Sign in using your Whova account Two events - One Conference Virtual Weekend - 10th & 11th February 2024 In-Person Weekend - 11th & 12th May 2024 click, click, sign in. And here you have the opening screen for the 2024 conference with the virtual in and here you have the opening screen for the 2024 conference with the virtual weekend in February and the in-person weekend in May. There'll be another short video with you weekend in February and the inperson weekend in May there'll be another short video um with you to show you how to use to show you how to use the facilities on the left hand side of the screen. Thanks very much the facilities on the left hand side of the screen thanks very much and look and look forward to seeing you at the 2024 conference. Thanks. forward to seeing you at the 2024 conference thanks 2024 NLP International Conference - Virtual Live = E Feed sam Powell answered an icebreaker community Photos sm International ce breaker Two events - One Conference. Virtual Weekend - 10th & 11th February 2024 In-Person Weekend - & 12th May 2024 Welcome to the 2024 NLP International LIVE ‘Saturday 10th and Sunday 11th February 2024 Saturday 11% ane 12th May 2028 ‘An in-person event a the Sheraton Hotel, the Mastecass win of running on Sunday. Tips ‘The Masterciass and Sunday presentations wil also be streamed live on",
    "metadata": {
      "segments": 60,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_07",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_07",
    "text": "- x > CD Gownor 2024 NLP International Conference - Virtual Live Search My Agenda Mon @  ax o CO > Search speaker name Full Agenda My Agenda May6 May? Mays @ Photos - The Power View My } A ‘Sat - Consciousness around Wise Making view e Sat 10 Fb Coaching view | The bonus sessions you will find on Thursday, May the 9th. bonus sessions you will find on Thursday May the 9th again we have actually Again, we have actually listed when the bonus sessions are taking place. listed when the bonus sessions are taking place so you can access the bonus So you can access the bonus sessions live or the recordings on Thursday, May the 9th. sessions live or the recordings on Thursday May the 9th by the weekend of By the weekend of the in-person conference Saturday, May the 11th and Sunday, May the 12th, we've actually got, we're back on track with the proper dates on the calendar. the in-person conference Saturday May the 11th and Sunday May the 12th we've actually got we're back on track with the proper dates on the calendar um However, for consistency, we have still actually labeled all these dates on each presentation. however for consistency we have still 221 2024 NLP International Conference - Virtual Live @ Seach speaker name Full sat ‘Sat 1 May - Less is Mind, Metaphor ang 1 - My Agenda View [ My Agenda actually labeled all these dates on each presentation so that explains how the So that explains how the agenda presents to you as delegates for the conference in 2024. agenda presents to you as delegates for the conference in 2024",
    "metadata": {
      "segments": 53,
      "language": "en"
    }
  },
  {
    "id": "nlp_talk_10",
    "source": "youtube_transcript",
    "title": "Video: nlp_talk_10",
    "text": "Hi, I'm Tom Petruzzi and I'm going to be presenting at the NLP conference in 2017. Have you ever wondered how the most successful people in their field sport in business athletics, creative people, people who are the most successful in their field, have their ability to turn around to their right side. What they're getting ahead on, they seem to be focused, they seem to be driven, motivated. Have you ever been in a situation where you thought really focused, driven, motivated, and driven, Pete State? It could be at work, it could be when you're presenting, doing an interview, maybe playing sport yourself. You're in that real Pete State. Alternatively, have you ever been in a situation where you felt other world, sad, frustrated, and it will be off, and you get out of bed in the morning, you just want to get back in the bed again. Have you ever been like that? We all have. Even most successful people in the world have days where they don't feel 100% life has many challenges. Life, people go through breakups, they go through challenges, divorces, separations, they go through business failures, they job losses. Life has adversity. The key is to find a strength of character to take on end incomes your way. The key is to be in the right state of mind at the right time. That's what the most successful people do really, really well. They focus and they put their game ahead on at the right time. The key is to bounce back reversely, to come back from life's challenges. I'm going to teach you in the seminar, Pete Middle School, and the conference, how to be in the right state at the right time. I've had the experience of what we've been people over many years in business, in sports, at the highest level. So the worst athletes in sports people and business people be the best they can be. And the key is to be able to turn on when it matters. Now, for a small taste and flavour of what I'm going to be doing at the conference next year, in April, I want you to close your eyes. I actually want you to close your eyes. And as you close your eyes, I want you to just let go of any negative thoughts. If you haven't gone, and as you let go of any negative thoughts, imagine breathing in positive energy. And perhaps you've got the close your eyes. And as you close your eyes, let anything be forced, it's in the right state of mind into the atmosphere. And breathe in positive energy. Now, think of a time you're really, really confident. Think of a time you're really confident. Think of someone who is really confident. Or what confidence is to you. And really, breathe in that confidence. Let us spread for every cell in the body and muscle. And as you breathe in that confidence, notice the colour, the sound, the feelings. Let us feel it. We're really spread for every cell in the body. And breathe out your negative energy. And spread your energy. All that is wonderful feelings. And as you breathe in as one of our feelings, let us spread for every cell in the body. Imagine taking on any challenge throughout the day in that peak mental state. Join me at the conference in 217 for a full peak mental state workout. Thank you. NLP is making a difference... N LP and the difference is INTERNATIONAL CONFERENCE",
    "metadata": {
      "segments": 23,
      "language": "en"
    }
  }
]