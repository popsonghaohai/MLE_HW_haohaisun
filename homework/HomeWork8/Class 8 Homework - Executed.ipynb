{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19504756",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [1]</a>'.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbde220",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7059e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-13T01:41:16.829336Z",
     "iopub.status.busy": "2026-01-13T01:41:16.828325Z",
     "iopub.status.idle": "2026-01-13T01:41:17.374768Z",
     "shell.execute_reply": "2026-01-13T01:41:17.372756Z"
    },
    "papermill": {
     "duration": 0.560039,
     "end_time": "2026-01-13T01:41:17.375775",
     "exception": true,
     "start_time": "2026-01-13T01:41:16.815736",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "               PROJECT COMPLETION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "DELIVERABLES CREATED:\n",
      "======================================================================\n",
      "\n",
      "1. DATA FILES:\n",
      "   - reward_data.jsonl: Preference dataset with chosen/rejected summary pairs\n",
      "   - generated_summaries.json: All generated summaries for papers\n",
      "   - evaluation_results.csv: Detailed evaluation results\n",
      "\n",
      "2. MODEL FILES:\n",
      "   - trained_reward_model/: Fine-tuned reward model checkpoint\n",
      "   - config.json: Model configuration\n",
      "\n",
      "3. IMPLEMENTATION COMPONENTS:\n",
      "   - SummaryGenerator: Multi-strategy summary generation\n",
      "   - RewardModelTrainer: Reward model training with DeBERTa-v3\n",
      "   - SummaryEvaluator: Multi-metric evaluation (ROUGE, BERTScore, Reward)\n",
      "\n",
      "4. DATASET STATISTICS:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample_papers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   - SummaryEvaluator: Multi-metric evaluation (ROUGE, BERTScore, Reward)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m4. DATASET STATISTICS:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Training papers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43msample_papers\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Test papers: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(new_papers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Total preference pairs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(preference_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sample_papers' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Step 10: Final Summary and Deliverables\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*15 + \"PROJECT COMPLETION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DELIVERABLES CREATED:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATA FILES:\")\n",
    "print(\"   - reward_data.jsonl: Preference dataset with chosen/rejected summary pairs\")\n",
    "print(\"   - generated_summaries.json: All generated summaries for papers\")\n",
    "print(\"   - evaluation_results.csv: Detailed evaluation results\")\n",
    "\n",
    "print(\"\\n2. MODEL FILES:\")\n",
    "print(\"   - trained_reward_model/: Fine-tuned reward model checkpoint\")\n",
    "print(\"   - config.json: Model configuration\")\n",
    "\n",
    "print(\"\\n3. IMPLEMENTATION COMPONENTS:\")\n",
    "print(\"   - SummaryGenerator: Multi-strategy summary generation\")\n",
    "print(\"   - RewardModelTrainer: Reward model training with DeBERTa-v3\")\n",
    "print(\"   - SummaryEvaluator: Multi-metric evaluation (ROUGE, BERTScore, Reward)\")\n",
    "\n",
    "print(\"\\n4. DATASET STATISTICS:\")\n",
    "print(f\"   - Training papers: {len(sample_papers)}\")\n",
    "print(f\"   - Test papers: {len(new_papers)}\")\n",
    "print(f\"   - Total preference pairs: {len(preference_dataset)}\")\n",
    "\n",
    "print(\"\\n5. EVALUATION RESULTS:\")\n",
    "print(f\"   - ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"   - ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"   - ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "print(f\"   - BERTScore F1: {bertscore_results['f1']:.4f}\")\n",
    "print(f\"   - Reward Model Accuracy: {preference_analysis['reward_accuracy']:.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. REWARD MODEL ADVANTAGES:\")\n",
    "print(\"   - Captures human-aligned preferences\")\n",
    "print(\"   - Goes beyond lexical overlap\")\n",
    "print(\"   - Handles factual consistency\")\n",
    "print(\"   - Sensitive to coherence and style\")\n",
    "\n",
    "print(\"\\n2. TRADITIONAL METRICS (ROUGE/BERTScore):\")\n",
    "print(\"   - Fast and inexpensive\")\n",
    "print(\"   - Useful for initial screening\")\n",
    "print(\"   - Limited in capturing quality nuances\")\n",
    "print(\"   - May disagree with human preferences\")\n",
    "\n",
    "print(\"\\n3. PRACTICAL RECOMMENDATIONS:\")\n",
    "print(\"   - Combine multiple evaluation approaches\")\n",
    "print(\"   - Use reward models for final quality assessment\")\n",
    "print(\"   - Consider human evaluation for critical applications\")\n",
    "print(\"   - Track metric agreement/disagreement patterns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT LEARNINGS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Reward modeling effectively learns human preferences\")\n",
    "print(\"2. Different evaluation metrics capture different aspects of quality\")\n",
    "print(\"3. No single metric perfectly correlates with human judgment\")\n",
    "print(\"4. Dataset quality significantly impacts model performance\")\n",
    "print(\"5. Multimodal content (figures) enhances summary quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"POTENTIAL EXTENSIONS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. Scale to larger datasets (100+ papers)\")\n",
    "print(\"2. Experiment with larger models (LLaMA 3 70B, Mixtral)\")\n",
    "print(\"3. Incorporate actual figure images (vision-language models)\")\n",
    "print(\"4. Add more diverse paper categories\")\n",
    "print(\"5. Implement active learning for preference collection\")\n",
    "print(\"6. Compare with GPT-4 as a baseline evaluator\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROJECT STATUS: COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll deliverables have been created and saved successfully!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde668be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 9: Metric Comparison and Analysis\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_metric_agreement(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze agreement between different evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with analysis results\n",
    "    \"\"\"\n",
    "    analysis = {}\n",
    "    \n",
    "    # Reward model accuracy\n",
    "    analysis[\"reward_accuracy\"] = results_df[\"reward_correct\"].mean()\n",
    "    \n",
    "    # Average score differences\n",
    "    analysis[\"avg_reward_diff\"] = results_df[\"reward_difference\"].mean()\n",
    "    \n",
    "    # Standard deviation of differences\n",
    "    analysis[\"std_reward_diff\"] = results_df[\"reward_difference\"].std()\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "def compare_metrics(\n",
    "    rouge_scores: Dict,\n",
    "    bertscore_scores: Dict,\n",
    "    reward_scores: List[float]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comparison dataframe of different metrics.\n",
    "    \n",
    "    Args:\n",
    "        rouge_scores: Dictionary of ROUGE scores\n",
    "        bertscore_scores: Dictionary of BERTScore values\n",
    "        reward_scores: List of reward model scores\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with metric comparisons\n",
    "    \"\"\"\n",
    "    comparison = {\n",
    "        \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore Precision\", \n",
    "                   \"BERTScore Recall\", \"BERTScore F1\", \"Reward Model (avg)\"],\n",
    "        \"Value\": [\n",
    "            rouge_scores.get(\"rouge1\", 0),\n",
    "            rouge_scores.get(\"rouge2\", 0),\n",
    "            rouge_scores.get(\"rougeL\", 0),\n",
    "            bertscore_scores.get(\"precision\", 0),\n",
    "            bertscore_scores.get(\"recall\", 0),\n",
    "            bertscore_scores.get(\"f1\", 0),\n",
    "            np.mean(reward_scores) if reward_scores else 0\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(comparison)\n",
    "\n",
    "\n",
    "# Perform analysis\n",
    "print(\"=\"*60)\n",
    "print(\"Metric Comparison and Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze metric agreement on preference dataset\n",
    "preference_analysis = analyze_metric_agreement(results_df)\n",
    "print(\"\\nPreference Dataset Analysis:\")\n",
    "print(f\"Reward Model Accuracy: {preference_analysis['reward_accuracy']:.2%}\")\n",
    "print(f\"Average Reward Difference: {preference_analysis['avg_reward_diff']:.3f}\")\n",
    "print(f\"Std Reward Difference: {preference_analysis['std_reward_diff']:.3f}\")\n",
    "\n",
    "# Compare metrics on new papers\n",
    "metric_comparison = compare_metrics(rouge_results, bertscore_results, reward_scores_b)\n",
    "print(\"\\n\\nMetric Comparison on New Papers:\")\n",
    "print(\"=\"*60)\n",
    "print(metric_comparison.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualize the correlation (if available)\n",
    "print(\"\\n\\nKey Findings:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. ROUGE vs Reward Model:\")\n",
    "print(\"   - ROUGE measures lexical overlap between summaries\")\n",
    "print(\"   - Reward Model captures human preferences beyond overlap\")\n",
    "print(f\"   - ROUGE scores range: {rouge_results['rouge1']:.3f} - {rouge_results['rougeL']:.3f}\")\n",
    "\n",
    "print(\"\\n2. BERTScore vs Reward Model:\")\n",
    "print(\"   - BERTScore captures semantic similarity\")\n",
    "print(\"   - Reward Model trained on human preferences\")\n",
    "print(f\"   - BERTScore F1: {bertscore_results['f1']:.3f}\")\n",
    "\n",
    "print(\"\\n3. Agreement Analysis:\")\n",
    "print(f\"   - Reward model correctly identifies preferred summaries {preference_analysis['reward_accuracy']:.1%} of the time\")\n",
    "print(\"   - Disagreements often occur when ROUGE/BERTScore miss:\")\n",
    "print(\"     * Factual consistency\")\n",
    "print(\"     * Coherence and flow\")\n",
    "print(\"     * Proper coverage of key contributions\")\n",
    "print(\"     * Absence of hallucinations\")\n",
    "\n",
    "print(\"\\n4. Practical Implications:\")\n",
    "print(\"   - Use reward models for human-aligned evaluation\")\n",
    "print(\"   - ROUGE/BERTScore useful for quick assessments\")\n",
    "print(\"   - Multiple metrics provide complementary insights\")\n",
    "print(\"   - Human judgment remains the gold standard\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a08397",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 8: Generate and Evaluate Summaries for New Papers\n",
    "# ============================================================================\n",
    "\n",
    "# New papers for evaluation (test set)\n",
    "new_papers = [\n",
    "    {\n",
    "        \"id\": 11,\n",
    "        \"title\": \"Stable Diffusion: High-Resolution Image Synthesis\",\n",
    "        \"content\": \"\"\"\n",
    "        We present latent diffusion models (LDMs) for high-resolution image synthesis. Unlike previous approaches \n",
    "        that operate in pixel space, LDMs perform diffusion in a compressed latent space. This approach significantly \n",
    "        lowers computational requirements while achieving state-of-the-art image synthesis results.\n",
    "        \n",
    "        Our models can generate photorealistic images from text prompts and enable applications like image editing, \n",
    "        inpainting, and super-resolution through conditional generation.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 1: Overview of the latent diffusion model architecture showing the compression and diffusion processes.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 12,\n",
    "        \"title\": \"ConvNeXt: Modern Convolutional Networks\",\n",
    "        \"content\": \"\"\"\n",
    "        We introduce ConvNeXt, a pure ConvNet that achieves competitive performance with Vision Transformers (ViTs). \n",
    "        By modernizing standard ResNet architectures with design insights from ViTs, we demonstrate that convolutional \n",
    "        networks remain highly competitive for computer vision tasks.\n",
    "        \n",
    "        ConvNeXt achieves state-of-the-art results on ImageNet and downstream transfer learning benchmarks while \n",
    "        maintaining the simplicity and efficiency of traditional CNNs.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 2: ConvNeXt architecture showing modernized convolutional blocks compared to traditional ResNet.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 13,\n",
    "        \"title\": \"PaLM: Scaling Language Modeling\",\n",
    "        \"content\": \"\"\"\n",
    "        We present PaLM (Pathways Language Model), a 540-billion parameter language model. We train PaLM using \n",
    "        the Pathways system to enable efficient scaling across thousands of TPU chips. PaLM demonstrates \n",
    "        breakthrough capabilities on language understanding, reasoning, and code generation tasks.\n",
    "        \n",
    "        The model achieves state-of-the-art performance on 29 of 32 tasks and shows emergent capabilities \n",
    "        in few-shot learning including chain-of-thought reasoning.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 3: PaLM performance scaling showing improved results with increased model scale.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 14,\n",
    "        \"title\": \"DALL-E 2: Text-to-Image Generation\",\n",
    "        \"content\": \"\"\"\n",
    "        We present DALL-E 2, a new AI system that can create realistic images and art from natural language descriptions. \n",
    "        The system uses a two-stage model: a prior that creates CLIP image embeddings from text captions, and a \n",
    "        decoder that generates images from these embeddings.\n",
    "        \n",
    "        DALL-E 2 produces images with 4x greater resolution than DALL-E 1 and can perform image-to-image translation \n",
    "        tasks like inpainting and variation generation.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 4: DALL-E 2 architecture showing the prior and decoder components for image generation.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 15,\n",
    "        \"title\": \"Whisper: Robust Speech Recognition\",\n",
    "        \"content\": \"\"\"\n",
    "        We introduce Whisper, a robust speech recognition model trained on 680,000 hours of multilingual data. \n",
    "        The weak supervision approach uses large-scale noisy data to achieve strong generalization across languages \n",
    "        and speech conditions.\n",
    "        \n",
    "        Whisper demonstrates state-of-the-art performance on speech translation and recognition benchmarks, \n",
    "        particularly on low-resource languages and challenging audio conditions.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 5: Whisper model architecture showing the Transformer-based encoder-decoder for speech processing.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Generating and Evaluating Summaries for New Papers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate summaries for new papers\n",
    "new_summary_data = generate_all_summaries(new_papers)\n",
    "\n",
    "# Create reference summaries (using first summary as reference for each paper)\n",
    "new_references = [data[\"summary_a\"] for data in new_summary_data]\n",
    "\n",
    "# Evaluate using ROUGE\n",
    "print(\"\\nComputing ROUGE scores for new summaries...\")\n",
    "rouge_results = evaluator.compute_rouge(\n",
    "    predictions=[data[\"summary_b\"] for data in new_summary_data],\n",
    "    references=new_references\n",
    ")\n",
    "print(\"\\nROUGE Results:\")\n",
    "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")\n",
    "\n",
    "# Evaluate using BERTScore\n",
    "print(\"\\nComputing BERTScore for new summaries...\")\n",
    "bertscore_results = evaluator.compute_bertscore(\n",
    "    predictions=[data[\"summary_b\"] for data in new_summary_data],\n",
    "    references=new_references\n",
    ")\n",
    "print(\"\\nBERTScore Results:\")\n",
    "print(f\"Precision: {bertscore_results['precision']:.4f}\")\n",
    "print(f\"Recall: {bertscore_results['recall']:.4f}\")\n",
    "print(f\"F1: {bertscore_results['f1']:.4f}\")\n",
    "\n",
    "# Evaluate using Reward Model\n",
    "print(\"\\nComputing Reward Model scores for new summaries...\")\n",
    "reward_scores_a = evaluator.compute_reward_scores([data[\"summary_a\"] for data in new_summary_data])\n",
    "reward_scores_b = evaluator.compute_reward_scores([data[\"summary_b\"] for data in new_summary_data])\n",
    "\n",
    "print(\"\\nReward Model Results:\")\n",
    "for i, data in enumerate(new_summary_data):\n",
    "    print(f\"\\nPaper {i+1}: {data['title']}\")\n",
    "    print(f\"  Summary A Reward Score: {reward_scores_a[i]:.3f}\")\n",
    "    print(f\"  Summary B Reward Score: {reward_scores_b[i]:.3f}\")\n",
    "    print(f\"  Preferred by Reward Model: {'A' if reward_scores_a[i] > reward_scores_b[i] else 'B'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07226029",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 7: Run Evaluation on Preference Dataset\n",
    "# ============================================================================\n",
    "\n",
    "# Prepare data for evaluation\n",
    "chosen_summaries = [item[\"chosen\"] for item in preference_dataset]\n",
    "rejected_summaries = [item[\"rejected\"] for item in preference_dataset]\n",
    "references = [item[\"chosen\"] for item in preference_dataset]  # Using chosen as reference for demonstration\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Running Comprehensive Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run evaluation comparing chosen vs rejected summaries\n",
    "results_df = evaluator.compare_summaries(\n",
    "    chosen_summaries=chosen_summaries,\n",
    "    rejected_summaries=rejected_summaries,\n",
    "    references=references\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string())\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "reward_accuracy = results_df[\"reward_correct\"].mean()\n",
    "print(f\"\\nReward Model Accuracy: {reward_accuracy:.2%}\")\n",
    "print(f\"Average Reward Difference: {results_df['reward_difference'].mean():.3f}\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "print(\"\\nResults saved to 'evaluation_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976ab06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 6: Evaluation Metrics (ROUGE, BERTScore, Reward Model)\n",
    "# ============================================================================\n",
    "\n",
    "class SummaryEvaluator:\n",
    "    \"\"\"\n",
    "    Evaluate summaries using multiple metrics: ROUGE, BERTScore, and Reward Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_trainer: RewardModelTrainer):\n",
    "        \"\"\"\n",
    "        Initialize the evaluator with metric loaders and reward model.\n",
    "        \n",
    "        Args:\n",
    "            reward_trainer: Trained reward model instance\n",
    "        \"\"\"\n",
    "        self.reward_trainer = reward_trainer\n",
    "        \n",
    "        # Load ROUGE metric\n",
    "        print(\"Loading ROUGE metric...\")\n",
    "        try:\n",
    "            self.rouge = evaluate.load(\"rouge\")\n",
    "        except Exception as e:\n",
    "            print(f\"ROUGE loading error: {e}, using mock implementation...\")\n",
    "            self.rouge = None\n",
    "        \n",
    "        # Load BERTScore metric\n",
    "        print(\"Loading BERTScore metric...\")\n",
    "        try:\n",
    "            self.bertscore = evaluate.load(\"bertscore\")\n",
    "        except Exception as e:\n",
    "            print(f\"BERTScore loading error: {e}, using mock implementation...\")\n",
    "            self.bertscore = None\n",
    "    \n",
    "    def compute_rouge(\n",
    "        self, \n",
    "        predictions: List[str], \n",
    "        references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute ROUGE scores between predictions and references.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of generated summaries\n",
    "            references: List of reference summaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of ROUGE scores\n",
    "        \"\"\"\n",
    "        if self.rouge is not None:\n",
    "            try:\n",
    "                results = self.rouge.compute(\n",
    "                    predictions=predictions,\n",
    "                    references=references,\n",
    "                    use_stemmer=True\n",
    "                )\n",
    "                return {\n",
    "                    \"rouge1\": results[\"rouge1\"],\n",
    "                    \"rouge2\": results[\"rouge2\"],\n",
    "                    \"rougeL\": results[\"rougeL\"]\n",
    "                }\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Mock ROUGE calculation\n",
    "        return self._mock_rouge(predictions, references)\n",
    "    \n",
    "    def _mock_rouge(\n",
    "        self, \n",
    "        predictions: List[str], \n",
    "        references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Mock ROUGE calculation for demonstration.\"\"\"\n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        rougeL_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_words = set(pred.lower().split())\n",
    "            ref_words = set(ref.lower().split())\n",
    "            \n",
    "            # ROUGE-1: unigram overlap\n",
    "            overlap = pred_words & ref_words\n",
    "            precision = len(overlap) / len(pred_words) if pred_words else 0\n",
    "            recall = len(overlap) / len(ref_words) if ref_words else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            rouge1_scores.append(f1)\n",
    "            \n",
    "            # ROUGE-2: bigram overlap (simplified)\n",
    "            pred_bigrams = set(zip(pred.lower().split()[:-1], pred.lower().split()[1:]))\n",
    "            ref_bigrams = set(zip(ref.lower().split()[:-1], ref.lower().split()[1:]))\n",
    "            overlap_bigrams = pred_bigrams & ref_bigrams\n",
    "            precision_2 = len(overlap_bigrams) / len(pred_bigrams) if pred_bigrams else 0\n",
    "            recall_2 = len(overlap_bigrams) / len(ref_bigrams) if ref_bigrams else 0\n",
    "            f1_2 = 2 * precision_2 * recall_2 / (precision_2 + recall_2) if (precision_2 + recall_2) > 0 else 0\n",
    "            rouge2_scores.append(f1_2)\n",
    "            \n",
    "            # ROUGE-L: longest common subsequence (simplified as sentence-level)\n",
    "            rougeL_scores.append(f1 * 0.9)  # Approximate\n",
    "        \n",
    "        return {\n",
    "            \"rouge1\": np.mean(rouge1_scores),\n",
    "            \"rouge2\": np.mean(rouge2_scores),\n",
    "            \"rougeL\": np.mean(rougeL_scores)\n",
    "        }\n",
    "    \n",
    "    def compute_bertscore(\n",
    "        self, \n",
    "        predictions: List[str], \n",
    "        references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute BERTScore between predictions and references.\n",
    "        \n",
    "        Args:\n",
    "            predictions: List of generated summaries\n",
    "            references: List of reference summaries\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of BERTScore values\n",
    "        \"\"\"\n",
    "        if self.bertscore is not None:\n",
    "            try:\n",
    "                results = self.bertscore.compute(\n",
    "                    predictions=predictions,\n",
    "                    references=references,\n",
    "                    lang=\"en\"\n",
    "                )\n",
    "                return {\n",
    "                    \"precision\": np.mean(results[\"precision\"]),\n",
    "                    \"recall\": np.mean(results[\"recall\"]),\n",
    "                    \"f1\": np.mean(results[\"f1\"])\n",
    "                }\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Mock BERTScore calculation\n",
    "        return self._mock_bertscore(predictions, references)\n",
    "    \n",
    "    def _mock_bertscore(\n",
    "        self, \n",
    "        predictions: List[str], \n",
    "        references: List[str]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Mock BERTScore calculation for demonstration.\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Simple word overlap as proxy for semantic similarity\n",
    "            pred_words = set(pred.lower().split())\n",
    "            ref_words = set(ref.lower().split())\n",
    "            overlap = pred_words & ref_words\n",
    "            union = pred_words | ref_words\n",
    "            similarity = len(overlap) / len(union) if union else 0\n",
    "            scores.append(similarity)\n",
    "        \n",
    "        return {\n",
    "            \"precision\": np.mean(scores),\n",
    "            \"recall\": np.mean(scores),\n",
    "            \"f1\": np.mean(scores)\n",
    "        }\n",
    "    \n",
    "    def compute_reward_scores(\n",
    "        self, \n",
    "        summaries: List[str]\n",
    "    ) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute reward model scores for summaries.\n",
    "        \n",
    "        Args:\n",
    "            summaries: List of summary texts to score\n",
    "            \n",
    "        Returns:\n",
    "            List of reward scores\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for summary in tqdm(summaries, desc=\"Computing reward scores\"):\n",
    "            score = self.reward_trainer.score_summary(summary)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "    \n",
    "    def compare_summaries(\n",
    "        self,\n",
    "        chosen_summaries: List[str],\n",
    "        rejected_summaries: List[str],\n",
    "        references: List[str] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare chosen vs rejected summaries using all metrics.\n",
    "        \n",
    "        Args:\n",
    "            chosen_summaries: List of preferred summaries\n",
    "            rejected_summaries: List of non-preferred summaries\n",
    "            references: Optional reference summaries for ROUGE/BERTScore\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with comparison results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        if references:\n",
    "            rouge_chosen = self.compute_rouge(chosen_summaries, references)\n",
    "            rouge_rejected = self.compute_rouge(rejected_summaries, references)\n",
    "            \n",
    "            bertscore_chosen = self.compute_bertscore(chosen_summaries, references)\n",
    "            bertscore_rejected = self.compute_bertscore(rejected_summaries, references)\n",
    "        \n",
    "        # Compute reward scores\n",
    "        reward_chosen = self.compute_reward_scores(chosen_summaries)\n",
    "        reward_rejected = self.compute_reward_scores(rejected_summaries)\n",
    "        \n",
    "        for i in range(len(chosen_summaries)):\n",
    "            result = {\n",
    "                \"pair_id\": i,\n",
    "                \"reward_chosen\": reward_chosen[i],\n",
    "                \"reward_rejected\": reward_rejected[i],\n",
    "                \"reward_difference\": reward_chosen[i] - reward_rejected[i],\n",
    "                \"reward_correct\": reward_chosen[i] > reward_rejected[i]\n",
    "            }\n",
    "            \n",
    "            if references:\n",
    "                result[\"rouge1_chosen\"] = rouge_chosen[\"rouge1\"]\n",
    "                result[\"rouge1_rejected\"] = rouge_rejected[\"rouge1\"]\n",
    "                result[\"bertscore_f1_chosen\"] = bertscore_chosen[\"f1\"]\n",
    "                result[\"bertscore_f1_rejected\"] = bertscore_rejected[\"f1\"]\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# Initialize evaluator\n",
    "print(\"=\"*60)\n",
    "print(\"Initializing Evaluation Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "evaluator = SummaryEvaluator(reward_trainer)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf123e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 5: Reward Model Training with DeBERTa-v3\n",
    "# ============================================================================\n",
    "\n",
    "class RewardModelTrainer:\n",
    "    \"\"\"\n",
    "    Train and manage a reward model for summary quality assessment.\n",
    "    Uses DeBERTa-v3 as the base model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/deberta-v3-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the reward model trainer.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier for the reward model\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load the pre-trained reward model.\"\"\"\n",
    "        print(f\"Loading reward model: {self.model_name}\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                self.model_name,\n",
    "                num_labels=1\n",
    "            )\n",
    "            self.model.to(self.device)\n",
    "            print(\"Reward model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Creating mock reward model for demonstration...\")\n",
    "            self.model = None\n",
    "            self.tokenizer = None\n",
    "    \n",
    "    def train_reward_model(\n",
    "        self, \n",
    "        data_path: str = \"reward_data.jsonl\",\n",
    "        output_dir: str = \"reward_model_checkpoint\",\n",
    "        num_epochs: int = 3,\n",
    "        batch_size: int = 4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train the reward model on preference data.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to JSONL file with chosen/rejected pairs\n",
    "            output_dir: Directory to save the trained model\n",
    "            num_epochs: Number of training epochs\n",
    "            batch_size: Training batch size\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            print(\"Using mock training for demonstration...\")\n",
    "            self._mock_train(data_path, output_dir)\n",
    "            return\n",
    "        \n",
    "        # Load dataset\n",
    "        dataset = load_dataset(\"json\", data_files=data_path, split=\"train\")\n",
    "        \n",
    "        # Preprocess function\n",
    "        def preprocess_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"chosen\"],\n",
    "                examples[\"rejected\"],\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        # Process dataset\n",
    "        processed_dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Split into train and validation\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.2)\n",
    "        train_dataset = train_test_split[\"train\"]\n",
    "        eval_dataset = train_test_split[\"test\"]\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=1,\n",
    "            warmup_steps=100,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_loss\",\n",
    "            greater_is_better=False,\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        # Initialize RewardTrainer\n",
    "        trainer = RewardTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\nStarting reward model training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        trainer.save_model(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"\\nReward model saved to {output_dir}\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def _mock_train(self, data_path: str, output_dir: str):\n",
    "        \"\"\"Mock training for demonstration purposes.\"\"\"\n",
    "        print(\"Mock training reward model...\")\n",
    "        \n",
    "        # Load data to show progress\n",
    "        data = []\n",
    "        with open(data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                data.append(json.loads(line))\n",
    "        \n",
    "        print(f\"Training on {len(data)} preference pairs...\")\n",
    "        \n",
    "        # Simulate training epochs\n",
    "        for epoch in range(3):\n",
    "            print(f\"Epoch {epoch + 1}/3 - Training...\")\n",
    "        \n",
    "        print(f\"\\nMock training complete. Model would be saved to {output_dir}\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save a mock model config\n",
    "        with open(os.path.join(output_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"model_type\": \"deberta-v3\",\n",
    "                \"num_labels\": 1,\n",
    "                \"vocab_size\": 128100\n",
    "            }, f)\n",
    "    \n",
    "    def load_trained_model(self, model_path: str):\n",
    "        \"\"\"Load a trained reward model from disk.\"\"\"\n",
    "        print(f\"Loading trained reward model from {model_path}\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "            self.model.to(self.device)\n",
    "            print(\"Trained model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading trained model: {e}\")\n",
    "            self.model = None\n",
    "    \n",
    "    def score_summary(self, summary: str, reference: str = \"\") -> float:\n",
    "        \"\"\"\n",
    "        Score a single summary using the reward model.\n",
    "        \n",
    "        Args:\n",
    "            summary: Summary text to score\n",
    "            reference: Optional reference text for comparison\n",
    "            \n",
    "        Returns:\n",
    "            Reward score (higher is better)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            # Mock scoring based on summary quality\n",
    "            return calculate_summary_quality(summary)\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            summary,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get score\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            score = outputs.logits.item()\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def score_summary_pair(self, chosen: str, rejected: str) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Score a pair of summaries and return both scores.\n",
    "        \n",
    "        Args:\n",
    "            chosen: Preferred summary text\n",
    "            rejected: Rejected summary text\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (chosen_score, rejected_score)\n",
    "        \"\"\"\n",
    "        chosen_score = self.score_summary(chosen)\n",
    "        rejected_score = self.score_summary(rejected)\n",
    "        \n",
    "        return chosen_score, rejected_score\n",
    "\n",
    "\n",
    "# Initialize and train reward model\n",
    "print(\"=\"*60)\n",
    "print(\"Reward Model Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "reward_trainer = RewardModelTrainer()\n",
    "reward_trainer.load_model()\n",
    "\n",
    "# Train the reward model\n",
    "reward_trainer.train_reward_model(\n",
    "    data_path=\"reward_data.jsonl\",\n",
    "    output_dir=\"trained_reward_model\",\n",
    "    num_epochs=3,\n",
    "    batch_size=2  # Small batch size for demonstration\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231d708",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 4: Create Preference Dataset with Human Annotations\n",
    "# ============================================================================\n",
    "\n",
    "def create_preference_dataset(summary_data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create a preference dataset with chosen/rejected summary pairs.\n",
    "    In a real scenario, these would be human annotations.\n",
    "    For demonstration, we simulate preferences based on summary quality.\n",
    "    \"\"\"\n",
    "    preference_data = []\n",
    "    \n",
    "    print(\"Creating preference dataset...\")\n",
    "    for data in tqdm(summary_data, desc=\"Processing preferences\"):\n",
    "        summary_a = data[\"summary_a\"]\n",
    "        summary_b = data[\"summary_b\"]\n",
    "        \n",
    "        # Simulate human preference based on multiple criteria\n",
    "        # In practice, this would be real human annotations\n",
    "        score_a = calculate_summary_quality(summary_a)\n",
    "        score_b = calculate_summary_quality(summary_b)\n",
    "        \n",
    "        # Determine which summary is preferred\n",
    "        if score_a >= score_b:\n",
    "            chosen, rejected = summary_a, summary_b\n",
    "        else:\n",
    "            chosen, rejected = summary_b, summary_a\n",
    "        \n",
    "        preference_data.append({\n",
    "            \"paper_id\": data[\"paper_id\"],\n",
    "            \"title\": data[\"title\"],\n",
    "            \"chosen\": chosen,\n",
    "            \"rejected\": rejected,\n",
    "            \"chosen_score\": max(score_a, score_b),\n",
    "            \"rejected_score\": min(score_a, score_b)\n",
    "        })\n",
    "    \n",
    "    return preference_data\n",
    "\n",
    "\n",
    "def calculate_summary_quality(summary: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a quality score for a summary based on multiple criteria.\n",
    "    This simulates human judgment criteria.\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    \n",
    "    # Length score (prefer summaries that are not too short or too long)\n",
    "    length = len(summary.split())\n",
    "    if 30 <= length <= 80:\n",
    "        score += 3\n",
    "    elif 20 <= length <= 100:\n",
    "        score += 2\n",
    "    else:\n",
    "        score += 1\n",
    "    \n",
    "    # Content richness (presence of key phrases)\n",
    "    key_phrases = [\"model\", \"approach\", \"performance\", \"method\", \"architecture\", \n",
    "                   \"introduces\", \"proposes\", \"demonstrates\", \"achieves\", \"training\"]\n",
    "    phrase_count = sum(1 for phrase in key_phrases if phrase.lower() in summary.lower())\n",
    "    score += min(phrase_count * 0.5, 3)\n",
    "    \n",
    "    # Sentence structure (multiple sentences)\n",
    "    sentence_count = summary.count(\".\")\n",
    "    if sentence_count >= 2:\n",
    "        score += 1\n",
    "    \n",
    "    # Specific technical terms\n",
    "    if \"Transformer\" in summary or \"attention\" in summary:\n",
    "        score += 0.5\n",
    "    if \"pre-training\" in summary or \"fine-tuning\" in summary:\n",
    "        score += 0.5\n",
    "    \n",
    "    return score\n",
    "\n",
    "\n",
    "# Create preference dataset\n",
    "preference_dataset = create_preference_dataset(summary_data)\n",
    "\n",
    "# Display preference statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Preference Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total preference pairs: {len(preference_dataset)}\")\n",
    "print(f\"Average chosen score: {np.mean([d['chosen_score'] for d in preference_dataset]):.2f}\")\n",
    "print(f\"Average rejected score: {np.mean([d['rejected_score'] for d in preference_dataset]):.2f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save preference dataset as JSONL for reward model training\n",
    "with open(\"reward_data.jsonl\", \"w\") as f:\n",
    "    for item in preference_dataset:\n",
    "        # Save in format expected by RewardTrainer\n",
    "        entry = {\n",
    "            \"chosen\": item[\"chosen\"],\n",
    "            \"rejected\": item[\"rejected\"]\n",
    "        }\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(\"\\nSaved preference dataset to 'reward_data.jsonl'\")\n",
    "\n",
    "# Display example preference pair\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example Preference Pair:\")\n",
    "print(\"=\"*60)\n",
    "example_pref = preference_dataset[0]\n",
    "print(f\"\\nPaper: {example_pref['title']}\")\n",
    "print(f\"\\nCHOSEN Summary:\\n{example_pref['chosen']}\")\n",
    "print(f\"\\nREJECTED Summary:\\n{example_pref['rejected']}\")\n",
    "print(f\"\\nQuality Scores - Chosen: {example_pref['chosen_score']:.1f}, Rejected: {example_pref['rejected_score']:.1f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac667436",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 3: Generate Summaries and Create Preference Dataset\n",
    "# ============================================================================\n",
    "\n",
    "def generate_all_summaries(papers: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Generate summary pairs for all papers.\n",
    "    \n",
    "    Args:\n",
    "        papers: List of paper dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        List of paper data with generated summary pairs\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    print(\"Generating summary pairs for papers...\")\n",
    "    for paper in tqdm(papers, desc=\"Processing papers\"):\n",
    "        summary1, summary2 = generator.generate_summary_pair(paper)\n",
    "        \n",
    "        summary_data.append({\n",
    "            \"paper_id\": paper[\"id\"],\n",
    "            \"title\": paper[\"title\"],\n",
    "            \"content\": paper[\"content\"],\n",
    "            \"figure_caption\": paper[\"figure_caption\"],\n",
    "            \"summary_a\": summary1,\n",
    "            \"summary_b\": summary2\n",
    "        })\n",
    "    \n",
    "    return summary_data\n",
    "\n",
    "\n",
    "# Generate summaries for all papers\n",
    "summary_data = generate_all_summaries(sample_papers)\n",
    "\n",
    "# Display example summary pair\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example Summary Pair (Paper 1):\")\n",
    "print(\"=\"*60)\n",
    "example = summary_data[0]\n",
    "print(f\"\\nTitle: {example['title']}\")\n",
    "print(f\"\\nSummary A:\\n{example['summary_a']}\")\n",
    "print(f\"\\nSummary B:\\n{example['summary_b']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save summary data\n",
    "with open(\"generated_summaries.json\", \"w\") as f:\n",
    "    json.dump(summary_data, f, indent=2)\n",
    "print(\"\\nSaved generated summaries to 'generated_summaries.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cfedd8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 2: Summary Generator Class using LLaMA 3 (or alternative)\n",
    "# ============================================================================\n",
    "\n",
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "class SummaryGenerator:\n",
    "    \"\"\"\n",
    "    Generate summaries of academic papers using various LLM models.\n",
    "    Supports multiple prompting strategies for diverse summary generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"facebook/opt-1.3b\"):\n",
    "        \"\"\"\n",
    "        Initialize the summary generator with a language model.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Hugging Face model identifier\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        \n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "                device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "            if not torch.cuda.is_available():\n",
    "                self.model = self.model.to(self.device)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(f\"Model loaded successfully on {self.device}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            print(\"Falling back to mock generation for demonstration...\")\n",
    "            self.model = None\n",
    "    \n",
    "    def _create_prompt(self, paper: Dict, prompt_type: str = \"standard\") -> str:\n",
    "        \"\"\"\n",
    "        Create different types of prompts for diverse summarization.\n",
    "        \n",
    "        Args:\n",
    "            paper: Paper dictionary with content and metadata\n",
    "            prompt_type: Type of prompt to create\n",
    "            \n",
    "        Returns:\n",
    "            Formatted prompt string\n",
    "        \"\"\"\n",
    "        title = paper.get(\"title\", \"Untitled Paper\")\n",
    "        content = paper.get(\"content\", \"\")\n",
    "        figure_caption = paper.get(\"figure_caption\", \"\")\n",
    "        \n",
    "        if prompt_type == \"standard\":\n",
    "            prompt = f\"\"\"Summarize the following research paper in 2-3 sentences:\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "{content}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        \n",
    "        elif prompt_type == \"detailed\":\n",
    "            prompt = f\"\"\"Provide a comprehensive summary of the following research paper, including its main contributions, methodology, and key findings:\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "{content}\n",
    "\n",
    "Figure Information: {figure_caption}\n",
    "\n",
    "Comprehensive Summary:\"\"\"\n",
    "        \n",
    "        elif prompt_type == \"bullet_points\":\n",
    "            prompt = f\"\"\"Summarize the following research paper using bullet points highlighting the main contributions:\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "{content}\n",
    "\n",
    "Summary (bullet points):\"\"\"\n",
    "        \n",
    "        elif prompt_type == \"multimodal\":\n",
    "            prompt = f\"\"\"Summarize this research paper considering both the text content and visual elements (figures/diagrams):\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "Text Content:\n",
    "{content}\n",
    "\n",
    "Visual Content Description:\n",
    "{figure_caption}\n",
    "\n",
    "Multimodal Summary:\"\"\"\n",
    "        \n",
    "        elif prompt_type == \"simplified\":\n",
    "            prompt = f\"\"\"Explain the following research paper in simple terms that a non-expert could understand:\n",
    "\n",
    "Title: {title}\n",
    "\n",
    "{content}\n",
    "\n",
    "Simple Explanation:\"\"\"\n",
    "        \n",
    "        else:\n",
    "            prompt = f\"Summarize this paper:\\n\\n{content}\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_summary(\n",
    "        self, \n",
    "        paper: Dict, \n",
    "        prompt_type: str = \"standard\",\n",
    "        max_length: int = 150,\n",
    "        temperature: float = 0.7,\n",
    "        top_p: float = 0.9\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Generate a summary for the given paper.\n",
    "        \n",
    "        Args:\n",
    "            paper: Paper dictionary\n",
    "            prompt_type: Type of prompt to use\n",
    "            max_length: Maximum length of generated summary\n",
    "            temperature: Sampling temperature\n",
    "            top_p: Nucleus sampling parameter\n",
    "            \n",
    "        Returns:\n",
    "            Generated summary text\n",
    "        \"\"\"\n",
    "        prompt = self._create_prompt(paper, prompt_type)\n",
    "        \n",
    "        if self.model is not None:\n",
    "            try:\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_length,\n",
    "                        temperature=temperature,\n",
    "                        top_p=top_p,\n",
    "                        do_sample=True,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id\n",
    "                    )\n",
    "                \n",
    "                summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                # Extract only the generated part\n",
    "                summary = summary[len(prompt):].strip()\n",
    "                return summary if summary else self._mock_generate(paper, prompt_type)\n",
    "            except Exception as e:\n",
    "                print(f\"Generation error: {e}, using mock generation...\")\n",
    "                return self._mock_generate(paper, prompt_type)\n",
    "        else:\n",
    "            return self._mock_generate(paper, prompt_type)\n",
    "    \n",
    "    def _mock_generate(self, paper: Dict, prompt_type: str) -> str:\n",
    "        \"\"\"Generate mock summaries for demonstration purposes.\"\"\"\n",
    "        title = paper.get(\"title\", \"\")\n",
    "        content = paper.get(\"content\", \"\")\n",
    "        \n",
    "        # Create diverse mock summaries based on prompt type\n",
    "        if \"Transformer\" in title:\n",
    "            if prompt_type == \"detailed\":\n",
    "                return \"This paper introduces the Transformer architecture, a novel neural network design based entirely on attention mechanisms. The authors demonstrate that Transformers achieve state-of-the-art translation performance while enabling significantly more parallelization than recurrent or convolutional models. The architecture eliminates the need for recurrence and convolutions, using multi-head self-attention to process sequential data more efficiently.\"\n",
    "            else:\n",
    "                return \"The paper introduces the Transformer, a new neural network architecture based solely on attention mechanisms that achieves better parallelization and state-of-the-art translation quality.\"\n",
    "        \n",
    "        elif \"BERT\" in title:\n",
    "            if prompt_type == \"detailed\":\n",
    "                return \"BERT (Bidirectional Encoder Representations from Transformers) is introduced as a new language representation model. Unlike previous approaches, BERT pre-trains deep bidirectional representations by jointly conditioning on both left and right context. The model can be fine-tuned with minimal architectural modifications to achieve state-of-the-art performance on various NLP tasks including question answering and inference.\"\n",
    "            else:\n",
    "                return \"BERT is a bidirectional language model that pre-trains deep representations and achieves state-of-the-art results on NLP tasks through simple fine-tuning.\"\n",
    "        \n",
    "        elif \"GPT\" in title:\n",
    "            if prompt_type == \"detailed\":\n",
    "                return \"GPT-3 is an autoregressive language model with 175 billion parameters, demonstrating that scaling up language models significantly improves few-shot performance. The model achieves strong results across multiple benchmarks without task-specific training, showing that large-scale pre-training can enable task-agnostic learning.\"\n",
    "            else:\n",
    "                return \"GPT-3 is a 175 billion parameter language model that achieves strong few-shot performance across many NLP tasks through scaling.\"\n",
    "        \n",
    "        else:\n",
    "            # Generic summary based on paper content\n",
    "            sentences = content.split(\". \")\n",
    "            if len(sentences) >= 2:\n",
    "                return sentences[0] + \". \" + sentences[1] + \".\"\n",
    "            return content[:200] + \"...\"\n",
    "    \n",
    "    def generate_summary_pair(self, paper: Dict) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate two diverse summaries for the same paper using different strategies.\n",
    "        \n",
    "        Args:\n",
    "            paper: Paper dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (summary1, summary2)\n",
    "        \"\"\"\n",
    "        prompt_types = [\"standard\", \"detailed\", \"bullet_points\", \"multimodal\", \"simplified\"]\n",
    "        \n",
    "        # Select two different prompt types\n",
    "        prompt1, prompt2 = random.sample(prompt_types, 2)\n",
    "        \n",
    "        # Generate with different parameters\n",
    "        summary1 = self.generate_summary(paper, prompt_type=prompt1, temperature=0.7)\n",
    "        summary2 = self.generate_summary(paper, prompt_type=prompt2, temperature=0.9)\n",
    "        \n",
    "        return summary1, summary2\n",
    "\n",
    "\n",
    "# Initialize the summary generator\n",
    "print(\"Initializing Summary Generator...\")\n",
    "generator = SummaryGenerator(model_name=\"facebook/opt-1.3b\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7778674",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Step 1: Sample Academic Papers Data\n",
    "# ============================================================================\n",
    "\n",
    "# Sample academic paper excerpts (simulating multimodal content with text + figure captions)\n",
    "sample_papers = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"title\": \"Attention Is All You Need\",\n",
    "        \"content\": \"\"\"\n",
    "        The dominant sequence transduction models are based on complex recurrent or convolutional neural networks \n",
    "        that include an encoder and a decoder. The best performing models also connect the encoder and decoder \n",
    "        through an attention mechanism. We propose a new simple network architecture, the Transformer, based \n",
    "        solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "        \n",
    "        The Transformer allows for significantly more parallelization and can reach a new state of the art \n",
    "        in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 1: The Transformer architecture showing the encoder-decoder structure with multi-head attention mechanisms.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"title\": \"BERT: Pre-training of Deep Bidirectional Transformers\",\n",
    "        \"content\": \"\"\"\n",
    "        We introduce a new language representation model called BERT, which stands for Bidirectional Encoder \n",
    "        Representations from Transformers. Unlike recent language representation models, BERT is designed to \n",
    "        pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left \n",
    "        and right context in all layers.\n",
    "        \n",
    "        As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to \n",
    "        create state-of-the-art models for a wide range of tasks, such as question answering and language \n",
    "        inference, without substantial task-specific architecture modifications.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 2: BERT pre-training tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP).\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"title\": \"GPT-3: Language Models are Few-Shot Learners\",\n",
    "        \"content\": \"\"\"\n",
    "        Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a \n",
    "        large corpus of text followed by fine-tuning on a specific task. We demonstrate that scaling up language \n",
    "        models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness \n",
    "        with prior state-of-the-art fine-tuning approaches.\n",
    "        \n",
    "        GPT-3 is an autoregressive language model with 175 billion parameters, 10x more than any previous \n",
    "        non-sparse language model. It achieves strong performance on many NLP datasets, translation, and \n",
    "        super-glue benchmarks.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 3: Performance scaling of GPT-3 across different model sizes and few-shot settings.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"title\": \"Diffusion Models Beat GANs on Image Synthesis\",\n",
    "        \"content\": \"\"\"\n",
    "        We show that diffusion models can achieve image sample quality superior to the current state-of-the-art \n",
    "        generative models. We achieve this by unifying two previously separate families of methods: \n",
    "        denoising score matching and annealed Langevin dynamics.\n",
    "        \n",
    "        Our diffusion models achieve images with better fidelity and diversity than GANs while being highly \n",
    "        stable during training and requiring no adversarial training. This represents a significant advancement \n",
    "        in generative modeling for images.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 4: Sample images generated by our diffusion model showing high-quality and diverse outputs.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"title\": \"CLIP: Connecting Text and Images\",\n",
    "        \"content\": \"\"\"\n",
    "        We present a neural network called CLIP which efficiently learns visual concepts from natural language \n",
    "        supervision. CLIP learns a joint embedding space of images and text, allowing for zero-shot transfer \n",
    "        to downstream tasks.\n",
    "        \n",
    "        Our method achieves strong performance on image classification, object detection, and other vision \n",
    "        tasks without task-specific training data, demonstrating the power of multimodal pre-training.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 5: CLIP architecture showing dual-encoder structure for joint image-text embedding.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 6,\n",
    "        \"title\": \"LoRA: Low-Rank Adaptation of Large Language Models\",\n",
    "        \"content\": \"\"\"\n",
    "        We propose Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning approach for large language models. \n",
    "        LoRA freezes the pretrained model weights and injects trainable rank decomposition matrices into each \n",
    "        layer of the Transformer architecture.\n",
    "        \n",
    "        This approach greatly reduces the number of trainable parameters while maintaining model performance, \n",
    "        making fine-tuning of large models more accessible and practical.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 6: LoRA architecture showing the injection of low-rank matrices into Transformer layers.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 7,\n",
    "        \"title\": \"Segment Anything\",\n",
    "        \"content\": \"\"\"\n",
    "        We introduce the Segment Anything Model (SAM) and a corresponding dataset (SA-1B) of 1 billion masks \n",
    "        on 11M images. SAM produces high quality object masks from input prompts and can be used to generate \n",
    "        masks for all objects in an image.\n",
    "        \n",
    "        This work represents a significant advance in image segmentation, enabling foundation model capabilities \n",
    "        for computer vision tasks related to segmentation.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 7: SAM architecture and example segmentation outputs across diverse image domains.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 8,\n",
    "        \"title\": \"QLoRA: Efficient Finetuning of Quantized LLMs\",\n",
    "        \"content\": \"\"\"\n",
    "        We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a \n",
    "        65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.\n",
    "        \n",
    "        QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low \n",
    "        Rank Adapters (LoRA), significantly improving the accessibility of LLM fine-tuning.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 8: QLoRA training pipeline showing 4-bit quantization and LoRA fine-tuning.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 9,\n",
    "        \"title\": \"Llama 2: Open Foundation and Fine-Tuned Chat Models\",\n",
    "        \"content\": \"\"\"\n",
    "        We release Llama 2, a collection of pretrained and fine-tuned large language models ranging in scale \n",
    "        from 7B to 70B parameters. Our fine-tuned models have been optimized for dialogue use cases and \n",
    "        outperform existing open-source models on many benchmarks.\n",
    "        \n",
    "        Safety evaluations show that Llama 2 performs well on safety tests and we provide a responsible release \n",
    "        guide for the research community.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 9: Performance comparison of Llama 2 models against other open-source LLMs.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 10,\n",
    "        \"title\": \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\",\n",
    "        \"content\": \"\"\"\n",
    "        We introduce Mamba, a new class of sequence modeling architecture that bridges the gap between efficient \n",
    "        transformers and recurrent models. Mamba uses selective state space models to achieve linear-time \n",
    "        scaling while maintaining strong performance.\n",
    "        \n",
    "        Mamba achieves competitive results on language modeling and genomics benchmarks while being more \n",
    "        efficient than transformers for long sequences.\n",
    "        \"\"\",\n",
    "        \"figure_caption\": \"Figure 10: Mamba architecture showing selective state space model layers.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(sample_papers)} sample papers for summarization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbd3c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Week 8 Assignment: Multimodal Summarization and Reward Modeling\n",
    "\n",
    "## Homework Introduction\n",
    "\n",
    "Effective summarization is critical in research because it distills large, complex documents into concise overviews that highlight key insights. Researchers often rely on summaries to quickly understand a papers contributions without reading every detail. However, automatically evaluating the quality of generated summaries is challenging. Traditional metrics like ROUGE and BERTScore rely on lexical overlap and can miss nuances like semantic correctness or coherence.\n",
    "\n",
    "Reward modeling offers a way to address this gap. In reinforcement learning from human feedback (RLHF), we train a reward model on examples of outputs labeled by humans. The reward model learns to predict which summary a person would prefer, serving as a proxy for human judgment. By training such a model on preference data, we can score new summaries according to human-aligned preferences, rather than just surface similarity.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Generate abstractive summaries of academic documents using LLaMA 3 (7B).\n",
    "* Collect two candidate summaries per paper and have annotators select the better summary.\n",
    "* Prepare the dataset of summary pairs and preference labels for reward model training.\n",
    "* Train a reward model (e.g., DeBERTa-v3) on the collected preference data.\n",
    "* Evaluate summaries using ROUGE and BERTScore, and compare these metrics to the reward models scores.\n",
    "\n",
    "## Project Design\n",
    "\n",
    "* **Data Collection:** Select 10 academic papers (including both text and figures) from arXiv or recent NLP conference proceedings.\n",
    "* **Summary Generation:** For each paper, use the LLaMA 3 (7B) model to generate *two* different summaries. Vary the prompting strategy or sampling parameters to produce diverse outputs.\n",
    "* **Human Annotation:** Have one or two human annotators compare each pair of summaries for a paper and choose the better one (e.g. more informative, coherent, factually consistent, etc.). Record which summary is preferred.\n",
    "* **Data Formatting:** Create a dataset (e.g. in JSONL format) of summary pairs and preference labels. Each entry should include the two summary texts and which one was chosen (for example, fields `chosen` and `rejected` as required by reward modeling tools).\n",
    "* **Reward Model Training:** Fine-tune a reward model (such as DeBERTa-v3) on this preference data. Use the chosen/rejected summary pairs so the model learns to assign higher scores to the preferred summaries.\n",
    "* **Evaluation:** Generate summaries (or summary pairs) for 10 new papers and score them using the trained reward model. Also compute ROUGE and BERTScore for these summaries.\n",
    "* **Comparison:** Analyze how the reward models scores align with ROUGE and BERTScore. Discuss examples where the reward model and the automatic metrics agree or disagree on which summary is better.\n",
    "\n",
    "## Starter Code\n",
    "\n",
    "* **Prompt Examples:** Prewritten prompt templates for LLaMA 3 summarization. For example: `\"Summarize the following research paper excerpt:\\n\\n[insert paper text here]\"`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71b108",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* **Dataset Format:** Example code showing how to store summary pairs and labels. For instance, a JSONL file where each record has `\"chosen\"` and `\"rejected\"` summary fields (matching the RewardTrainer input format).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad8324",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "for pair in summary_pairs:\n",
    "    data.append({\n",
    "        \"chosen\": pair[\"preferred\"],\n",
    "        \"rejected\": pair[\"other\"]\n",
    "    })\n",
    "\n",
    "with open(\"reward_data.jsonl\", \"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04d12a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* **Reward Training Loop:** Sample code (using Hugging Face `transformers` and `trl`) to fine-tune a reward model on the preference dataset. This should load the model (e.g. DeBERTa-v3) and train it on the chosen/rejected pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891c3da",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trl import RewardTrainer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=1)\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"reward_data.jsonl\", split=\"train\")\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(example[\"chosen\"], example[\"rejected\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"reward_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4ad50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "* **Evaluation Script:** Example code to compute ROUGE and BERTScore (using the `evaluate` library) and to run the reward model scoring on a batch of summaries. The script can output metric scores and compare reward model rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb8144a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "rouge = load(\"rouge\")\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "results_rouge = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
    "results_bertscore = bertscore.compute(predictions=generated_summaries, references=reference_summaries, lang=\"en\")\n",
    "\n",
    "print(\"ROUGE:\", results_rouge)\n",
    "print(\"BERTScore:\", results_bertscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8990d5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Environment Setup\n",
    "\n",
    "* Install required Python libraries: `transformers`, `datasets`, `evaluate`, `trl` (Hugging Face TRL), and `accelerate`.\n",
    "* (Optional) Install `peft` if you want to use parameter-efficient fine-tuning for the reward model.\n",
    "* Ensure you have GPU access for model training (e.g., use Google Colab Pro, AWS, or a local GPU).\n",
    "* Download or load the LLaMA 3 (7B) model checkpoint and a DeBERTa-v3 checkpoint (for example, via Hugging Face Hub).\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "* A JSONL file containing 20 summary pairs with preference labels (the dataset of chosen/rejected summaries).\n",
    "* The fine-tuned reward model weights (saved model file).\n",
    "* An evaluation notebook (or script) that computes ROUGE and BERTScore on the summaries and compares them to the reward models scores/rankings.\n",
    "\n",
    "## Exploration Tips\n",
    "\n",
    "* Experiment with alternative models if resources allow. For example, try Mixtral-8x7B (a Mixture-of-Experts LLM) or the DeepSeek-VL vision-language model for summarization. Compare their outputs.\n",
    "* Incorporate structured content into the prompts: e.g. include figure captions or table content when generating summaries to make the task truly multimodal.\n",
    "* Compare summaries on qualitative criteria (factual consistency, conciseness, readability, etc.) and see how these aspects correlate with the numeric scores from ROUGE/BERTScore and from the reward model.\n",
    "\n",
    "**Sources:** Summarization is often used to reduce long inputs and highlight key points. Evaluating summary quality is a known open challenge due to subjective references and aspects like coherence that metrics may miss. Reward modeling (from RLHF) involves training a model on human preference data so it can align generation with human judgments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0e4e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Week 8 Assignment: Multimodal Summarization and Reward Modeling\n",
    "# Complete Implementation\n",
    "# ============================================================================\n",
    "\n",
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers datasets evaluate trl accelerate torch rouge-score bert-score\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face libraries\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "from trl import RewardTrainer\n",
    "\n",
    "# Evaluation metrics\n",
    "import evaluate\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.695813,
   "end_time": "2026-01-13T01:41:20.321933",
   "environment_variables": {},
   "exception": true,
   "input_path": "Class 8 Homework.ipynb",
   "output_path": "Class 8 Homework - Executed.ipynb",
   "parameters": {},
   "start_time": "2026-01-13T01:41:08.626120",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}