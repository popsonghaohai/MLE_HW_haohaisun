{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multimodal Summarization and Reward Modeling System\n",
        "\n",
        "**Multimodal Summarization and Reward Modeling System - Week 8 Assignment**\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a complete academic paper summarization and reward modeling pipeline:\n",
        "\n",
        "1. Summary generation using local Ollama Qwen3:8b model\n",
        "2. Summary comparison and human/auto annotation\n",
        "3. Reward model training based on DeBERTa-v3\n",
        "4. Multi-dimensional evaluation (ROUGE, BERTScore, reward scores)\n",
        "\n",
        "---\n",
        "\n",
        "## Feature Modules\n",
        "\n",
        "| Module | Description |\n",
        "|--------|-------------|\n",
        "| **SummaryGenerator** | Generate summaries via Ollama API using local Qwen3:8b |\n",
        "| **AnnotationInterface** | Interactive summary comparison annotation interface |\n",
        "| **RewardModelTrainer** | Reward model training based on DeBERTa-v3-base |\n",
        "| **SummaryEvaluator** | ROUGE, BERTScore, reward model scoring |\n",
        "| **Pipeline** | Complete end-to-end pipeline management |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup & Imports\n",
        "\n",
        "First, import required libraries and setup environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic libraries\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Transformers libraries\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# Data and evaluation libraries\n",
        "from datasets import load_dataset, Dataset\n",
        "from evaluate import load\n",
        "\n",
        "# HTTP requests\n",
        "import requests\n",
        "\n",
        "print(\"\\nEnvironment setup complete!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Custom Reward Model Trainer\n",
        "\n",
        "Create a reward model trainer compatible with the new transformers version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomRewardTrainer(Trainer):\n",
        "    \"\"\"\n",
        "    Custom reward model trainer, compatible with new transformers version\n",
        "    \n",
        "    This trainer implements reward model loss calculation for comparing\n",
        "    chosen and rejected responses.\n",
        "    \"\"\"\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"\n",
        "        Calculate reward loss\n",
        "        \n",
        "        Args:\n",
        "            model: Reward model\n",
        "            inputs: Input data\n",
        "            return_outputs: Whether to return output\n",
        "            num_items_in_batch: Number of items in batch\n",
        "        \"\"\"\n",
        "        if \"input_ids\" in inputs and \"attention_mask\" in inputs:\n",
        "            labels = inputs.pop(\"labels\", None)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Create contrastive loss\n",
        "            if labels is not None:\n",
        "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.float().view(-1))\n",
        "            else:\n",
        "                # If no labels, use MSE loss to make output close to 1\n",
        "                loss = torch.nn.functional.mse_loss(logits, torch.ones_like(logits))\n",
        "\n",
        "            return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "        return super().compute_loss(model, inputs, return_outputs)\n",
        "\n",
        "print(\"CustomRewardTrainer class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary Generator (SummaryGenerator)\n",
        "\n",
        "Generate paper summaries using local Ollama Qwen3:8b model.\n",
        "\n",
        "### Features\n",
        "- Communicate with Ollama via HTTP API\n",
        "- Support adjustable temperature parameters\n",
        "- Automatically handle chain-of-thought model output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SummaryGenerator:\n",
        "    \"\"\"\n",
        "    Generate summaries using local Ollama Qwen3:8b\n",
        "    \n",
        "    Attributes:\n",
        "        model_name: Model name in Ollama\n",
        "        api_url: URL of Ollama API\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"qwen3:8b\"):\n",
        "        \"\"\"\n",
        "        Initialize summary generator\n",
        "        \n",
        "        Args:\n",
        "            model_name: Model name in Ollama\n",
        "        \"\"\"\n",
        "        print(f\"Using local Ollama model: {model_name}\")\n",
        "        self.model_name = model_name\n",
        "        self.api_url = \"http://localhost:11434/api/generate\"\n",
        "\n",
        "    def generate_summary(\n",
        "        self,\n",
        "        paper_text: str,\n",
        "        prompt_template: str = None,\n",
        "        max_length: int = 512,\n",
        "        temperature: float = 0.7,\n",
        "        top_p: float = 0.9\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate summary via Ollama API\n",
        "\n",
        "        Args:\n",
        "            paper_text: Paper text\n",
        "            prompt_template: Prompt template\n",
        "            max_length: Maximum generation length\n",
        "            temperature: Sampling temperature (0.0-1.0)\n",
        "            top_p: Nucleus sampling parameter\n",
        "\n",
        "        Returns:\n",
        "            Generated summary text\n",
        "        \"\"\"\n",
        "        if prompt_template is None:\n",
        "            prompt_template = \"\"\"\"Please provide a concise summary of the following research paper:\\n",
        "\n",
        "{paper_text}\\n",
        "\n",
        "Provide a 2-3 sentence summary:\"\"\"\n",
        "\n",
        "        prompt = prompt_template.format(paper_text=paper_text[:3000])\n",
        "\n",
        "        payload = {\n",
        "            \"model\": self.model_name,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"num_predict\": max_length,\n",
        "                \"temperature\": temperature,\n",
        "                \"top_p\": top_p,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.api_url, json=payload)\n",
        "            response.raise_for_status()\n",
        "            result = response.json()\n",
        "\n",
        "            # Get response content\n",
        "            summary = result.get(\"response\", \"\").strip()\n",
        "\n",
        "            # If response is empty, extract from thinking field\n",
        "            if not summary and \"thinking\" in result:\n",
        "                thinking = result.get(\"thinking\", \"\")\n",
        "                lines = thinking.split('\\n')\n",
        "                summary_lines = [line.strip() for line in lines if line.strip()]\n",
        "                if summary_lines:\n",
        "                    summary = ' '.join(summary_lines[-3:])  # Take last 3 sentences\n",
        "\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling Ollama: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def generate_summary_pair(self, paper_text: str) -> Tuple[str, str]:\n",
        "        \"\"\"\n",
        "        Generate two different summaries for comparison\n",
        "\n",
        "        Args:\n",
        "            paper_text: Paper text\n",
        "\n",
        "        Returns:\n",
        "            Summary pair (summary_a, summary_b)\n",
        "            - summary_a: Low temperature (0.3) - more deterministic\n",
        "            - summary_b: High temperature (0.9) - more diverse\n",
        "        \"\"\"\n",
        "        # Low temperature - more deterministic\n",
        "        summary_a = self.generate_summary(\n",
        "            paper_text,\n",
        "            temperature=0.3,\n",
        "            top_p=0.8\n",
        "        )\n",
        "\n",
        "        # High temperature - more diverse\n",
        "        summary_b = self.generate_summary(\n",
        "            paper_text,\n",
        "            temperature=0.9,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "        return summary_a, summary_b\n",
        "\n",
        "# Test class definition\n",
        "print(\"SummaryGenerator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Annotation Interface (AnnotationInterface)\n",
        "\n",
        "Interactive summary comparison annotation interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AnnotationInterface:\n",
        "    \"\"\"\n",
        "    Human annotation interface\n",
        "    \n",
        "    Used for manually comparing two summaries and selecting the better one.\n",
        "    \"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def annotate_summary_pair(\n",
        "        paper_id: str,\n",
        "        summary_a: str,\n",
        "        summary_b: str\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Interactively annotate summary pair\n",
        "\n",
        "        Args:\n",
        "            paper_id: Paper ID\n",
        "            summary_a: Summary A (temperature=0.3)\n",
        "            summary_b: Summary B (temperature=0.9)\n",
        "\n",
        "        Returns:\n",
        "            Annotation result dictionary\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"Paper ID: {paper_id}\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"\\nSummary A (temperature=0.3):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(summary_a)\n",
        "        print(\"\\nSummary B (temperature=0.9):\")\n",
        "        print(\"-\" * 80)\n",
        "        print(summary_b)\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "        while True:\n",
        "            choice = input(\"\\nPlease select the better summary (A/B) or skip (S): \").strip().upper()\n",
        "            if choice in ['A', 'B', 'S']:\n",
        "                break\n",
        "            print(\"Invalid input, please enter A, B, or S\")\n",
        "\n",
        "        if choice == 'S':\n",
        "            return None\n",
        "\n",
        "        chosen = summary_a if choice == 'A' else summary_b\n",
        "        rejected = summary_b if choice == 'A' else summary_a\n",
        "\n",
        "        return {\n",
        "            \"paper_id\": paper_id,\n",
        "            \"summary_a\": summary_a,\n",
        "            \"summary_b\": summary_b,\n",
        "            \"chosen\": chosen,\n",
        "            \"rejected\": rejected,\n",
        "            \"annotator_choice\": choice\n",
        "        }\n",
        "\n",
        "print(\"AnnotationInterface class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reward Model Trainer (RewardModelTrainer)\n",
        "\n",
        "Train reward model based on DeBERTa-v3-base.\n",
        "\n",
        "### Model Architecture\n",
        "```\n",
        "Input: (chosen_text, rejected_text)\n",
        "       |\\n",
        "   [DeBERTa Encoder]\n",
        "       |\\n",
        "   [Pooler + Classifier]\n",
        "       |\\n",
        "   Score: Reward Score\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RewardModelTrainer:\n",
        "    \"\"\"\n",
        "    Reward model trainer\n",
        "    \n",
        "    Used to train a model that maps input text to reward scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"microsoft/deberta-v3-base\"):\n",
        "        \"\"\"\n",
        "        Initialize reward model trainer\n",
        "\n",
        "        Args:\n",
        "            model_name: Base model name (default DeBERTa-v3-base)\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "\n",
        "    def prepare_dataset(self, jsonl_path: str) -> Dataset:\n",
        "        \"\"\"\n",
        "        Prepare training dataset\n",
        "\n",
        "        Args:\n",
        "            jsonl_path: JSONL file path\n",
        "\n",
        "        Returns:\n",
        "            Hugging Face Dataset object\n",
        "        \"\"\"\n",
        "        dataset = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "        def preprocess(examples):\n",
        "            \"\"\"Preprocessing function\"\"\"\n",
        "            return self.tokenizer(\n",
        "                examples[\"chosen\"],\n",
        "                examples[\"rejected\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=512\n",
        "            )\n",
        "\n",
        "        dataset = dataset.map(preprocess, batched=True)\n",
        "        return dataset\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_dataset: Dataset,\n",
        "        output_dir: str = \"reward_model\",\n",
        "        num_epochs: int = 3,\n",
        "        batch_size: int = 8,\n",
        "        learning_rate: float = 2e-5\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Train reward model\n",
        "\n",
        "        Args:\n",
        "            train_dataset: Training dataset\n",
        "            output_dir: Output directory\n",
        "            num_epochs: Training epochs\n",
        "            batch_size: Batch size\n",
        "            learning_rate: Learning rate\n",
        "        \"\"\"\n",
        "        # Initialize model\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.model_name,\n",
        "            num_labels=1\n",
        "        )\n",
        "\n",
        "        # Set tokenizer pad_token\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        # Training parameters\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=batch_size,\n",
        "            num_train_epochs=num_epochs,\n",
        "            learning_rate=learning_rate,\n",
        "            eval_strategy=\"no\",\n",
        "            save_strategy=\"epoch\",\n",
        "            logging_steps=10,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "\n",
        "        # Create trainer\n",
        "        trainer = CustomRewardTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            processing_class=self.tokenizer\n",
        "        )\n",
        "\n",
        "        # Start training\n",
        "        print(\"Starting reward model training...\")\n",
        "        trainer.train()\n",
        "\n",
        "        # Save model\n",
        "        trainer.save_model(output_dir)\n",
        "        self.tokenizer.save_pretrained(output_dir)\n",
        "        print(f\"Model saved to: {output_dir}\")\n",
        "\n",
        "    def score_summary(self, summary: str) -> float:\n",
        "        \"\"\"\n",
        "        Score a summary using the trained reward model\n",
        "\n",
        "        Args:\n",
        "            summary: Summary text\n",
        "\n",
        "        Returns:\n",
        "            Reward score\n",
        "        \"\"\"\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            raise ValueError(\"Model not loaded, please train or load model first\")\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            summary,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            score = outputs.logits[0][0].item()\n",
        "\n",
        "        return score\n",
        "\n",
        "print(\"RewardModelTrainer class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Summary Evaluator (SummaryEvaluator)\n",
        "\n",
        "Calculate evaluation metrics like ROUGE, BERTScore, etc.\n",
        "\n",
        "### Evaluation Metrics\n",
        "| Metric | Description |\n",
        "|--------|-------------|\n",
        "| **ROUGE-1** | Word-level overlap |\n",
        "| **ROUGE-2** | Bigram-level overlap |\n",
        "| **ROUGE-L** | Longest common subsequence |\n",
        "| **BERTScore** | BERT-based semantic similarity |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SummaryEvaluator:\n",
        "    \"\"\"\n",
        "    Summary evaluator\n",
        "    \n",
        "    Supports multiple evaluation metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize evaluator\n",
        "        \"\"\"\n",
        "        print(\"Loading evaluation metrics...\")\n",
        "        self.rouge = load(\"rouge\")\n",
        "        self.bertscore = load(\"bertscore\")\n",
        "\n",
        "    def evaluate_rouge(\n",
        "        self,\n",
        "        predictions: List[str],\n",
        "        references: List[str]\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate ROUGE scores\n",
        "\n",
        "        Args:\n",
        "            predictions: Generated summary list\n",
        "            references: Reference summary list\n",
        "\n",
        "        Returns:\n",
        "            ROUGE score dictionary\n",
        "        \"\"\"\n",
        "        results = self.rouge.compute(\n",
        "            predictions=predictions,\n",
        "            references=references\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def evaluate_bertscore(\n",
        "        self,\n",
        "        predictions: List[str],\n",
        "        references: List[str]\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate BERTScore\n",
        "\n",
        "        Args:\n",
        "            predictions: Generated summary list\n",
        "            references: Reference summary list\n",
        "\n",
        "        Returns:\n",
        "            BERTScore dictionary\n",
        "        \"\"\"\n",
        "        results = self.bertscore.compute(\n",
        "            predictions=predictions,\n",
        "            references=references,\n",
        "            lang=\"en\",\n",
        "            model_type=\"microsoft/deberta-xlarge-mnli\"\n",
        "        )\n",
        "\n",
        "        # Calculate average scores\n",
        "        avg_results = {\n",
        "            \"precision\": np.mean(results[\"precision\"]),\n",
        "            \"recall\": np.mean(results[\"recall\"]),\n",
        "            \"f1\": np.mean(results[\"f1\"])\n",
        "        }\n",
        "\n",
        "        return avg_results\n",
        "\n",
        "    def comprehensive_evaluation(\n",
        "        self,\n",
        "        predictions: List[str],\n",
        "        references: List[str],\n",
        "        reward_model: RewardModelTrainer = None\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation\n",
        "\n",
        "        Args:\n",
        "            predictions: Generated summary list\n",
        "            references: Reference summary list\n",
        "            reward_model: Reward model (optional)\n",
        "\n",
        "        Returns:\n",
        "            Comprehensive evaluation results\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        # ROUGE evaluation\n",
        "        print(\"Calculating ROUGE scores...\")\n",
        "        results[\"rouge\"] = self.evaluate_rouge(predictions, references)\n",
        "\n",
        "        # BERTScore evaluation\n",
        "        print(\"Calculating BERTScore...\")\n",
        "        results[\"bertscore\"] = self.evaluate_bertscore(predictions, references)\n",
        "\n",
        "        # Reward model evaluation\n",
        "        if reward_model:\n",
        "            print(\"Calculating reward model scores...\")\n",
        "            reward_scores = [reward_model.score_summary(pred) for pred in predictions]\n",
        "            results[\"reward_scores\"] = {\n",
        "                \"scores\": reward_scores,\n",
        "                \"mean\": np.mean(reward_scores),\n",
        "                \"std\": np.std(reward_scores)\n",
        "            }\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"SummaryEvaluator class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Complete Pipeline (Pipeline)\n",
        "\n",
        "Manage the entire experiment workflow.\n",
        "\n",
        "### Flow Diagram\n",
        "```\n",
        "Paper Input → Ollama Generate Summary → Annotate → Train Reward Model → Evaluate\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Pipeline:\n",
        "    \"\"\"\n",
        "    Complete experiment workflow\n",
        "    \n",
        "    Encapsulates the entire summary generation, annotation, training, and evaluation workflow.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize pipeline\n",
        "        \"\"\"\n",
        "        self.generator = None\n",
        "        self.reward_trainer = None\n",
        "        self.evaluator = SummaryEvaluator()\n",
        "\n",
        "    def step1_generate_summaries(\n",
        "        self,\n",
        "        papers: List[Dict[str, str]],\n",
        "        output_path: str = \"summary_pairs.json\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Step 1: Generate summary pairs\n",
        "\n",
        "        Args:\n",
        "            papers: Paper list, each element contains {'id': ..., 'text': ...}\n",
        "            output_path: Output file path\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Step 1: Generate Summary Pairs ===\")\n",
        "        self.generator = SummaryGenerator()\n",
        "\n",
        "        summary_pairs = []\n",
        "        for paper in papers:\n",
        "            print(f\"\\nProcessing paper: {paper['id']}\")\n",
        "            summary_a, summary_b = self.generator.generate_summary_pair(paper['text'])\n",
        "\n",
        "            summary_pairs.append({\n",
        "                \"paper_id\": paper['id'],\n",
        "                \"summary_a\": summary_a,\n",
        "                \"summary_b\": summary_b\n",
        "            })\n",
        "\n",
        "        # Save results\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(summary_pairs, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\nSummary pairs saved to: {output_path}\")\n",
        "        return summary_pairs\n",
        "\n",
        "    def step2_annotate(\n",
        "        self,\n",
        "        summary_pairs_path: str = \"summary_pairs.json\",\n",
        "        output_path: str = \"reward_data.jsonl\",\n",
        "        auto_mode: bool = True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Step 2: Annotation\n",
        "\n",
        "        Args:\n",
        "            summary_pairs_path: Summary pairs file path\n",
        "            output_path: Output JSONL path\n",
        "            auto_mode: Auto mode (default select summary_a as chosen)\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Step 2: Annotation ===\")\n",
        "\n",
        "        with open(summary_pairs_path, 'r', encoding='utf-8') as f:\n",
        "            summary_pairs = json.load(f)\n",
        "\n",
        "        annotated_data = []\n",
        "        for pair in summary_pairs:\n",
        "            if auto_mode:\n",
        "                # Auto mode\n",
        "                result = {\n",
        "                    \"paper_id\": pair['paper_id'],\n",
        "                    \"summary_a\": pair['summary_a'],\n",
        "                    \"summary_b\": pair['summary_b'],\n",
        "                    \"chosen\": pair['summary_a'],\n",
        "                    \"rejected\": pair['summary_b'],\n",
        "                    \"annotator_choice\": \"A\"\n",
        "                }\n",
        "                annotated_data.append({\n",
        "                    \"chosen\": result[\"chosen\"],\n",
        "                    \"rejected\": result[\"rejected\"]\n",
        "                })\n",
        "                print(f\"Paper {pair['paper_id']}: Auto-select A (temperature=0.3)\")\n",
        "            else:\n",
        "                # Interactive mode\n",
        "                result = AnnotationInterface.annotate_summary_pair(\n",
        "                    pair['paper_id'],\n",
        "                    pair['summary_a'],\n",
        "                    pair['summary_b']\n",
        "                )\n",
        "                if result:\n",
        "                    annotated_data.append({\n",
        "                        \"chosen\": result[\"chosen\"],\n",
        "                        \"rejected\": result[\"rejected\"]\n",
        "                    })\n",
        "\n",
        "        # Save as JSONL\n",
        "        with open(output_path, 'w', encoding='utf-8') as f:\n",
        "            for item in annotated_data:\n",
        "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "        print(f\"\\nAnnotation data saved to: {output_path}\")\n",
        "        print(f\"Total annotated samples: {len(annotated_data)}\")\n",
        "\n",
        "    def step3_train_reward_model(\n",
        "        self,\n",
        "        train_data_path: str = \"reward_data.jsonl\",\n",
        "        output_dir: str = \"reward_model\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Step 3: Train reward model\n",
        "\n",
        "        Args:\n",
        "            train_data_path: Training data path\n",
        "            output_dir: Model output directory\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Step 3: Train Reward Model ===\")\n",
        "\n",
        "        self.reward_trainer = RewardModelTrainer()\n",
        "        dataset = self.reward_trainer.prepare_dataset(train_data_path)\n",
        "\n",
        "        self.reward_trainer.train(\n",
        "            train_dataset=dataset,\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "    def step4_evaluate(\n",
        "        self,\n",
        "        test_papers: List[Dict[str, str]],\n",
        "        reward_model_path: str = \"reward_model\"\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Step 4: Evaluation\n",
        "\n",
        "        Args:\n",
        "            test_papers: Test paper list\n",
        "            reward_model_path: Reward model path\n",
        "        \"\"\"\n",
        "        print(\"\\n=== Step 4: Evaluation ===\")\n",
        "\n",
        "        # Generate test summaries\n",
        "        predictions = []\n",
        "        references = []\n",
        "\n",
        "        for paper in test_papers:\n",
        "            summary, _ = self.generator.generate_summary_pair(paper['text'])\n",
        "            predictions.append(summary)\n",
        "            references.append(paper.get('reference_summary', summary))\n",
        "\n",
        "        # Load reward model\n",
        "        if self.reward_trainer is None:\n",
        "            self.reward_trainer = RewardModelTrainer()\n",
        "            self.reward_trainer.tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
        "            self.reward_trainer.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                reward_model_path\n",
        "            )\n",
        "\n",
        "        # Comprehensive evaluation\n",
        "        results = self.evaluator.comprehensive_evaluation(\n",
        "            predictions,\n",
        "            references,\n",
        "            self.reward_trainer\n",
        "        )\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nEvaluation Results:\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"\\nROUGE Scores:\")\n",
        "        for key, value in results[\"rouge\"].items():\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nBERTScore:\")\n",
        "        for key, value in results[\"bertscore\"].items():\n",
        "            print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "        if \"reward_scores\" in results:\n",
        "            print(\"\\nReward Model Scores:\")\n",
        "            print(f\"  Mean: {results['reward_scores']['mean']:.4f}\")\n",
        "            print(f\"  Std: {results['reward_scores']['std']:.4f}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "print(\"Pipeline class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Live Demo\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "Run the following cell to execute the complete workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare example paper data\n",
        "example_papers = [\n",
        "    {\n",
        "        \"id\": \"paper_001\",\n",
        "        \"text\": \"\"\"\n",
        "        Title: Attention Is All You Need\n",
        "\n",
        "        Abstract: The dominant sequence transduction models are based on complex\n",
        "        recurrent or convolutional neural networks. We propose a new architecture\n",
        "        called the Transformer that relies entirely on an attention mechanism to\n",
        "        draw global dependencies between input and output. The Transformer allows\n",
        "        for significantly more parallelization and can reach a new state of the\n",
        "        art in translation quality after being trained for as little as twelve\n",
        "        hours on eight P100 GPUs.\n",
        "        \"\"\",\n",
        "        \"reference_summary\": \"The Transformer is a new neural network architecture for sequence transduction that relies entirely on attention mechanisms.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"paper_002\",\n",
        "        \"text\": \"\"\"\n",
        "        Title: BERT: Pre-training of Deep Bidirectional Transformers\n",
        "\n",
        "        Abstract: We introduce a new language representation model called BERT, which\n",
        "        stands for Bidirectional Encoder Representations from Transformers. Unlike\n",
        "        recent language representation models, BERT is designed to pre-train deep\n",
        "        bidirectional representations from unlabeled text by jointly conditioning on\n",
        "        both left and right context in all layers.\n",
        "        \"\"\",\n",
        "        \"reference_summary\": \"BERT is a bidirectional Transformer language model that can be pre-trained on unlabeled text.\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"paper_003\",\n",
        "        \"text\": \"\"\"\n",
        "        Title: GPT-3: Language Models are Few-Shot Learners\n",
        "\n",
        "        Abstract: Recent work has demonstrated substantial gains on many NLP tasks\n",
        "        by pre-training on a large corpus of text. We demonstrate that scaling up language\n",
        "        models greatly improves few-shot performance.\n",
        "        \"\"\",\n",
        "        \"reference_summary\": \"GPT-3 is a large-scale language model that achieves strong few-shot learning performance.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"Prepared {len(example_papers)} papers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Generate Summary Pairs\n",
        "\n",
        "Use Ollama Qwen3:8b to generate two different summaries for each paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create pipeline instance\n",
        "pipeline = Pipeline()\n",
        "\n",
        "# Execute step 1\n",
        "summary_pairs = pipeline.step1_generate_summaries(example_papers, \"summary_pairs.json\")\n",
        "\n",
        "print(f\"\\nGenerated {len(summary_pairs)} summary pairs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Annotation\n",
        "\n",
        "Annotate the generated summary pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto mode annotation\n",
        "pipeline.step2_annotate(\"summary_pairs.json\", \"reward_data.jsonl\", auto_mode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train Reward Model\n",
        "\n",
        "Train DeBERTa-v3 reward model based on annotation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train reward model (may take a few minutes)\n",
        "pipeline.step3_train_reward_model(\"reward_data.jsonl\", \"reward_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Evaluation\n",
        "\n",
        "Evaluate model performance using multiple metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model\n",
        "results = pipeline.step4_evaluate(example_papers[:2], \"reward_model\")\n",
        "\n",
        "print(\"\\nEvaluation completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Using Components Individually\n",
        "\n",
        "## Summary Generator Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary generator\n",
        "generator = SummaryGenerator(\"qwen3:8b\")\n",
        "\n",
        "# Single summary generation\n",
        "paper_text = \"\"\"\n",
        "Title: My Research Paper\n",
        "\n",
        "This paper presents a novel approach to solving complex problems...\n",
        "\"\"\"\n",
        "\n",
        "summary = generator.generate_summary(paper_text, temperature=0.7)\n",
        "print(f\"Summary: {summary}\")\n",
        "\n",
        "# Generate summary pair\n",
        "summary_a, summary_b = generator.generate_summary_pair(paper_text)\n",
        "print(f\"\\nSummary A: {summary_a}\")\n",
        "print(f\"\\nSummary B: {summary_b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Trained Reward Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create reward model trainer\n",
        "trainer = RewardModelTrainer()\n",
        "\n",
        "# Load trained model\n",
        "trainer.tokenizer = AutoTokenizer.from_pretrained(\"reward_model\")\n",
        "trainer.model = AutoModelForSequenceClassification.from_pretrained(\"reward_model\")\n",
        "\n",
        "# Score summary\n",
        "test_summary = \"This paper presents a novel approach to machine learning.\"\n",
        "score = trainer.score_summary(test_summary)\n",
        "print(f\"Reward score: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Notes\n",
        "\n",
        "1. **Ollama Service**: Ensure Ollama is running (`ollama serve`)\n",
        "2. **Model Download**: First run will download DeBERTa model\n",
        "3. **GPU Acceleration**: GPU available for faster training\n",
        "4. **Windows Encoding**: Script handles UTF-8 encoding\n",
        "\n",
        "---\n",
        "\n",
        "**Week 8 Assignment - Multimodal Summarization and Reward Modeling**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
