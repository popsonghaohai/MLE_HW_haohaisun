{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Assignment: Voice Agent Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we start to hands on buiding Research Voice Agent, truly useful AI Research Assistants must listen, understand, and respond with voice. **we will give you some simple introduction code as a starter, feel free to write your own code or do optimization.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Learning Objectives this week\n",
    "to build a simple Voice Agent, we need these following knowledge.\n",
    "\n",
    "* **1. Speech Recognition (ASR):** Convert audio to text using models like Whisper or Google Speech-to-Text.\n",
    "* **2. Dialogue Generation with LLMs:** Feed transcribed user input into LLM (e.g. LLaMA 3) and generate natural language responses.\n",
    "* **3. Text-to-Speech (TTS):** Use a TTS engine (CozyVoice) to convert generated responses into spoken audio.\n",
    "* **4. FastAPI for API Serving:** Create a web server with FastAPI to handle audio file uploads and return voice responses.\n",
    "* **5. Conversation State Management:** Track conversation history to enable multi-turn interaction.\n",
    "* **6. Low-Latency Real-Time Processing:** Use asynchronous functions to reduce inference time and improve response experience.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ‚úÖ You do NOT need Docker. Just ensure your local Python environment works.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Project: Build an Local Voice Assistant\n",
    "\n",
    "### üéØ Goal:\n",
    "\n",
    "Develop a real-time voice chatbot that can:\n",
    "\n",
    "1. Take audio input via HTTP,\n",
    "2. Transcribe audio to text (ASR),\n",
    "3. Generate a response using LLM,\n",
    "4. Convert the response back to speech (TTS),\n",
    "5. Support 5-turn conversational memory.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: FastAPI Skeleton\n",
    "\n",
    "Create a simple FastAPI server that accepts an audio file via POST and returns an audio file in response:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the official guidance of FastAPI [fastapi](https://fastapi.tiangolo.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import FileResponse\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    # TODO: ASR ‚Üí LLM ‚Üí TTS\n",
    "    return FileResponse(\"response.wav\", media_type=\"audio/wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your server:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --reload\n",
    "```\n",
    "\n",
    "Test it with `curl`, Postman, or a custom frontend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: ASR (Speech Recognition)\n",
    "\n",
    "Use OpenAI Whisper to transcribe the uploaded audio to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "asr_model = whisper.load_model(\"small\")\n",
    "\n",
    "def transcribe_audio(audio_bytes):\n",
    "    with open(\"temp.wav\", \"wb\") as f:\n",
    "        f.write(audio_bytes)\n",
    "    result = asr_model.transcribe(\"temp.wav\")\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to the `/chat/` route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_text = transcribe_audio(audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print `user_text` for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: Response Generation (LLM)\n",
    "\n",
    "Generate context-aware responses using Llama 3. Use HuggingFace `pipeline` to call LLaMA 3 or similar models:\n",
    "\n",
    "*Note* The llama-3.1-8b might not run in the 16G Tensorflow GPU due to out of the memory usage. try use some smaller model for that such as `llama-3.2-1b` or `llama-3.2-3b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.1-8B\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "def generate_response(user_text):\n",
    "    conversation_history.append({\"role\": \"user\", \"text\": user_text})\n",
    "    # Construct prompt from history\n",
    "    prompt = \"\"\n",
    "    for turn in conversation_history[-5:]:\n",
    "        prompt += f\"{turn['role']}: {turn['text']}\\n\"\n",
    "    outputs = llm(prompt, max_new_tokens=100)\n",
    "    bot_response = outputs[0][\"generated_text\"]\n",
    "    conversation_history.append({\"role\": \"assistant\", \"text\": bot_response})\n",
    "    return bot_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call in route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_text = generate_response(user_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: TTS (Text to Speech)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert LLM text responses to natural-sounding speech. \\\n",
    "try to use cozyvoice to complete Text to Speech, here is the original project.\n",
    "[Cosyvoice](https://github.com/FunAudioLLM/CosyVoice)\n",
    "\n",
    "other suggestion:\n",
    "You can also try to use [BentoTTX](https://github.com/bentoml/BentoXTTS?tab=readme-ov-file) or [Pyttsx3](https://pypi.org/project/pyttsx3/) if you have trouble to set up CosyVoice (kinda out dated here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('third_party/Matcha-TTS')\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice, CosyVoice2\n",
    "from cosyvoice.utils.file_utils import load_wav\n",
    "import torchaudio\n",
    "\n",
    "cosyvoice = CosyVoice2('pretrained_models/CosyVoice2-0.5B', load_jit=False, load_trt=False, load_vllm=False, fp16=False)\n",
    "\n",
    "# NOTE if you want to reproduce the results on https://funaudiollm.github.io/cosyvoice2, please add text_frontend=False during inference\n",
    "# zero_shot usage\n",
    "prompt_speech_16k = load_wav('./asset/zero_shot_prompt.wav', 16000)\n",
    "for i, j in enumerate(cosyvoice.inference_zero_shot('Êî∂Âà∞Â•ΩÂèã‰ªéËøúÊñπÂØÑÊù•ÁöÑÁîüÊó•Á§ºÁâ©ÔºåÈÇ£‰ªΩÊÑèÂ§ñÁöÑÊÉäÂñú‰∏éÊ∑±Ê∑±ÁöÑÁ•ùÁ¶èËÆ©ÊàëÂøÉ‰∏≠ÂÖÖÊª°‰∫ÜÁîúËúúÁöÑÂø´‰πêÔºåÁ¨ëÂÆπÂ¶ÇËä±ÂÑøËà¨ÁªΩÊîæ„ÄÇ', 'Â∏åÊúõ‰Ω†‰ª•ÂêéËÉΩÂ§üÂÅöÁöÑÊØîÊàëËøòÂ•ΩÂë¶„ÄÇ', prompt_speech_16k, stream=False)):\n",
    "    torchaudio.save('zero_shot_{}.wav'.format(i), j['tts_speech'], cosyvoice.sample_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you set up the BentoTTX, you can use it to do the TTS.\n",
    "# Make sure you run the server before you use it.\n",
    "# if the python didn't give you voice response, try to set up using cURL to test the server.\n",
    "import bentoml\n",
    "\n",
    "with bentoml.SyncHTTPClient(\"http://localhost:3000\") as client:\n",
    "        result = client.synthesize(\n",
    "            text=\"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n",
    "            lang=\"en\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use it in the route:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythonaudio_path = synthesize_speech(bot_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 5: Full Integration\n",
    "\n",
    "Your final `/chat/` endpoint might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.post(\"/chat/\")\n",
    "async def chat_endpoint(file: UploadFile = File(...)):\n",
    "    audio_bytes = await file.read()\n",
    "    user_text = transcribe_audio(audio_bytes)\n",
    "    bot_text = generate_response(user_text)\n",
    "    audio_path = synthesize_speech(bot_text)\n",
    "    return FileResponse(audio_path, media_type=\"audio/wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Deliverables\n",
    "\n",
    "* [ ] A runnable FastAPI project with `/chat/` endpoint\n",
    "* [ ] A working voice assistant that handles **5-turn** multi-round conversations\n",
    "* [ ] Code with clear structure and modular components (ASR, LLM, TTS)\n",
    "* [ ] A **2-minute screen recording** demo: record 5 turns of real-time interaction\n",
    "* [ ] Optional: Add conversation memory display, prompt formatting logic, async optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Extension Ideas (Optional)\n",
    "\n",
    "* Use `async` processing for parallel ASR/LLM/TTS.\n",
    "* Integrate a microphone frontend UI for live recording.\n",
    "* Add speaker identification or personalized voice response.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
